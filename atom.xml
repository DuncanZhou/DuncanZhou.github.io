<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Duncan&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/DuncanZhou/"/>
  <updated>2018-08-22T09:08:31.507Z</updated>
  <id>https://github.com/DuncanZhou/</id>
  
  <author>
    <name>duncan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>同步到腾讯云</title>
    <link href="https://github.com/DuncanZhou/2018/08/22/tencentcloud/"/>
    <id>https://github.com/DuncanZhou/2018/08/22/tencentcloud/</id>
    <published>2018-08-22T09:08:31.301Z</published>
    <updated>2018-08-22T09:08:31.507Z</updated>
    
    <content type="html"><![CDATA[<p>我的博客即将搬运同步至腾讯云+社区，邀请大家一同入驻：<a href="https://cloud.tencent.com/developer/support-plan?invite_code=cibtnefnj6my" target="_blank" rel="external">https://cloud.tencent.com/developer/support-plan?invite_code=cibtnefnj6my</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我的博客即将搬运同步至腾讯云+社区，邀请大家一同入驻：&lt;a href=&quot;https://cloud.tencent.com/developer/support-plan?invite_code=cibtnefnj6my&quot; target=&quot;_blank&quot; rel=&quot;exter
      
    
    </summary>
    
      <category term="Life" scheme="https://github.com/DuncanZhou//categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>推荐算法</title>
    <link href="https://github.com/DuncanZhou/2018/08/20/RecommendationNotes/"/>
    <id>https://github.com/DuncanZhou/2018/08/20/RecommendationNotes/</id>
    <published>2018-08-20T01:59:02.116Z</published>
    <updated>2018-08-20T02:55:15.905Z</updated>
    
    <content type="html"><![CDATA[<h3 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h3><h3 id="1-基于内容-用户的推荐"><a href="#1-基于内容-用户的推荐" class="headerlink" title="1.基于内容 / 用户的推荐"></a>1.基于内容 / 用户的推荐</h3><p>更多依赖相似性计算然后推荐</p><ul><li>基于<strong>用户信息</strong>进行推荐</li><li>基于<strong>内容 、物品的信息</strong>进行推荐</li></ul><h3 id="2-协同过滤"><a href="#2-协同过滤" class="headerlink" title="2.协同过滤"></a>2.协同过滤</h3><p>需要通过用户行为来计算用户或物品见的相关性</p><ul><li><p>基于<strong>用户的协同推荐</strong>: 以人为本</p><p>| 小张 | 产品经理、Google、增长   |<br>| —— | ———————————— |<br>| 小明 | 产品经理、Google、比特币 |<br>| 小吴 | 比特币、区块链、以太币   |</p><p><strong>这是一个用户关注内容的列表，显然在这个列表中，小张和小明关注的内容更为相似，那么可以给小张推荐比特币。</strong></p></li><li><p>基于<strong>物品的系统推荐</strong></p><p>以物为本建立各商品的相似度矩阵</p><p>| 产品经理 | 小张、小明 |<br>| ———— | ————— |<br>| Google   | 小张、小明 |<br>| 比特币   | 小明、小吴 |</p><p>小张和小明都不约而同地看了产品经理和Google，这可以说明产品经理和Google有相似，那么之后<strong>有看了Google相关内容的用户就可以给推荐产品经理</strong>的相关内容。     </p></li></ul><h3 id="3-基于知识的推荐"><a href="#3-基于知识的推荐" class="headerlink" title="3.基于知识的推荐"></a>3.基于知识的推荐</h3><p>某一领域的一整套规则和路线进行推荐。参照可汗学院知识树。</p><p>补充：（图片来源知乎shawn1943，感谢）</p><p><img src="https://pic1.zhimg.com/80/v2-9f88f829b59ddb4f1e0571c46c158d1c_hd.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;算法分类&quot;&gt;&lt;a href=&quot;#算法分类&quot; class=&quot;headerlink&quot; title=&quot;算法分类&quot;&gt;&lt;/a&gt;算法分类&lt;/h3&gt;&lt;h3 id=&quot;1-基于内容-用户的推荐&quot;&gt;&lt;a href=&quot;#1-基于内容-用户的推荐&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="Recommendation" scheme="https://github.com/DuncanZhou//categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="https://github.com/DuncanZhou//tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>超参的搜索方法整理</title>
    <link href="https://github.com/DuncanZhou/2018/08/17/SuperParas/"/>
    <id>https://github.com/DuncanZhou/2018/08/17/SuperParas/</id>
    <published>2018-08-17T02:17:27.027Z</published>
    <updated>2018-08-20T03:03:04.919Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-网格搜索"><a href="#1-网格搜索" class="headerlink" title="1.网格搜索"></a>1.网格搜索</h3><p>网格搜索通过查找搜索范围内的所有的点，来确定最优值。它返回目标函数的最大值或损失函数的最小值。给出较大的搜索范围，以及较小的步长，网格搜索是一定可以找到全局最大值或最小值的。 </p><p>当人们实际使用网格搜索来找到最佳超参数集的时候，一般会先使用较广的搜索范围，以及较大的步长，来找到全局最大值或者最小值可能的位置。然后，人们会缩小搜索范围和步长，来达到更精确的最值。 </p><h3 id="2-随机搜索"><a href="#2-随机搜索" class="headerlink" title="2.随机搜索"></a>2.随机搜索</h3><p>随机搜索的思想和网格搜索比较相似，只是不再测试上界和下界之间的所有值，只是在搜索范围中随机取样本点。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。</p><p>通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。但是和网格搜索的快速版（非自动版）相似，结果也是没法保证的。 </p><h3 id="3-基于梯度的优化"><a href="#3-基于梯度的优化" class="headerlink" title="3.基于梯度的优化"></a>3.基于梯度的优化</h3><h3 id="4-贝叶斯优化"><a href="#4-贝叶斯优化" class="headerlink" title="4.贝叶斯优化"></a>4.贝叶斯优化</h3><p>贝叶斯优化寻找使全局达到最值的参数时，使用了和网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新的点时，会忽略前一个点的信息。而贝叶斯优化充分利用了这个信息。贝叶斯优化的工作方式是通过对目标函数形状的学习，找到使结果向全局最大值提升的参数。它学习目标函数形状的方法是，根据先验分布，假设一个搜集函数。在每一次使用新的采样点来测试目标函数时，它使用这个信息来更新目标函数的先验分布。然后，算法测试由后验分布给出的，全局最值最可能出现的位置的点。 </p><p>补充:</p><p><img src="https://raw.githubusercontent.com/DuncanZhou/images/master/alipsipng.png" alt="PSI"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-网格搜索&quot;&gt;&lt;a href=&quot;#1-网格搜索&quot; class=&quot;headerlink&quot; title=&quot;1.网格搜索&quot;&gt;&lt;/a&gt;1.网格搜索&lt;/h3&gt;&lt;p&gt;网格搜索通过查找搜索范围内的所有的点，来确定最优值。它返回目标函数的最大值或损失函数的最小值。给出较大的搜索
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//categories/MachineLearning/"/>
    
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Hive SQL 学习</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/SQL_Learning/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/SQL_Learning/</id>
    <published>2018-08-10T09:52:09.175Z</published>
    <updated>2018-08-17T07:20:11.298Z</updated>
    
    <content type="html"><![CDATA[<h3 id="partition-by"><a href="#partition-by" class="headerlink" title="partition by"></a>partition by</h3><blockquote><p>partition by关键字是分析性函数的一部分，它和聚合函数不同的地方在于它能返回一个分组中的多条记录，而聚合函数一般只有一条反映统计值的记录，partition by用于给结果集分组，如果没有指定那么它把整个结果集作为一个分组</p></blockquote><p>example: 一个班有学生id，成绩，班级，现在将学生根据班级按照成绩排名。(partition by)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select *,row_number() over(partition by Grade order by Score desc) as Sequence from Student</div></pre></td></tr></table></figure><h3 id="lateral-view"><a href="#lateral-view" class="headerlink" title="lateral view"></a>lateral view</h3><h3 id="explode-posexplode"><a href="#explode-posexplode" class="headerlink" title="explode / posexplode"></a>explode / posexplode</h3><blockquote><p>explode 拆分一行称多行，而posexplode是根据多行匹配行号进行拆分多行。</p></blockquote><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><h4 id="a-first-value"><a href="#a-first-value" class="headerlink" title="a. first_value"></a>a. first_value</h4><p>​    取分组内排序后，截止到当前行，第一个值</p><h4 id="b-last-value"><a href="#b-last-value" class="headerlink" title="b.last_value"></a>b.last_value</h4><p>​    取分组内排序后，截止到当前行，最后一个值  </p><h4 id="c-lead-col-n-default"><a href="#c-lead-col-n-default" class="headerlink" title="c.lead(col,n,default)"></a>c.lead(col,n,default)</h4><p>​    用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） </p><h4 id="d-lag-col-n-default"><a href="#d-lag-col-n-default" class="headerlink" title="d.lag(col,n,default)"></a>d.lag(col,n,default)</h4><p>​    与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL） </p><h4 id="c-聚集函数-over-partition-by-col1-order-by-col-rows-range-between-UNBOUNDED-num-preceding-and-num-FOLLOWING-current-ROW"><a href="#c-聚集函数-over-partition-by-col1-order-by-col-rows-range-between-UNBOUNDED-num-preceding-and-num-FOLLOWING-current-ROW" class="headerlink" title="c.聚集函数 + over + (partition by col1 [order by col (rows | range) between (UNBOUNDED | [num]) preceding and (num FOLLOWING | current ROW))"></a>c.聚集函数 + over + (partition by col1 [order by col (rows | range) between (UNBOUNDED | [num]) preceding and (num FOLLOWING | current ROW))</h4><h4 id="d-ROW-NUMBER"><a href="#d-ROW-NUMBER" class="headerlink" title="d.ROW_NUMBER()"></a>d.ROW_NUMBER()</h4><p>​    从1开始，按照顺序，生成分组内记录的序列 </p><h4 id="e-RANK"><a href="#e-RANK" class="headerlink" title="e.RANK()"></a>e.RANK()</h4><p>​    生成数据项在分组中的排名，排名相等会在名次中留下空位 </p><h4 id="f-DENSE-RANK"><a href="#f-DENSE-RANK" class="headerlink" title="f.DENSE_RANK()"></a>f.DENSE_RANK()</h4><p>​    生成数据项在分组中的排名，排名相等会在名次中不会留下空位  </p><h4 id="g-CUME-DIST"><a href="#g-CUME-DIST" class="headerlink" title="g.CUME_DIST()"></a>g.CUME_DIST()</h4><p>​    小于等于当前值的行数/分组内总行数 </p><h4 id="h-PERCENT-RANK"><a href="#h-PERCENT-RANK" class="headerlink" title="h.PERCENT_RANK ()"></a>h.PERCENT_RANK ()</h4><p>​    分组内当前行的RANK值-1/分组内总行数-1  </p><h4 id="i-NTILE-n"><a href="#i-NTILE-n" class="headerlink" title="i.NTILE(n)"></a>i.NTILE(n)</h4><p>​    用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布 </p><p>Note:</p><ul><li>From子句：执行顺序自上而下，从左到右，从后往前，所以<strong>数据量少的表尽量放后</strong></li><li>where子句：执行顺序自下而上，从右到左，<strong>可以过滤掉大量记录的条件写在where子句的末尾</strong></li><li>group by子句：通过将不需要的记录在group by之前过滤掉，<strong>避免使用having来过滤</strong></li><li>having子句：尽量少用</li><li>select子句：尽量少用*，取字段名称</li><li>order by子句：执行顺序为从左到右排序</li><li>join：<strong>尽量把数据量大的表放在最右边来进行关联</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;partition-by&quot;&gt;&lt;a href=&quot;#partition-by&quot; class=&quot;headerlink&quot; title=&quot;partition by&quot;&gt;&lt;/a&gt;partition by&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;partition by关键字是分
      
    
    </summary>
    
      <category term="SQL" scheme="https://github.com/DuncanZhou//categories/SQL/"/>
    
    
      <category term="SQL" scheme="https://github.com/DuncanZhou//tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>pyspark记录</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/SparkDataFrameLearning/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/SparkDataFrameLearning/</id>
    <published>2018-08-10T09:52:09.172Z</published>
    <updated>2018-08-10T09:58:27.704Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark-DataFrame学习"><a href="#Spark-DataFrame学习" class="headerlink" title="Spark DataFrame学习"></a>Spark DataFrame学习</h2><h3 id="1-文件的读取"><a href="#1-文件的读取" class="headerlink" title="1. 文件的读取"></a>1. 文件的读取</h3><p>1.1 spark.read.json() / spark.read.parquet() 或者 spark.read.load(path,format=”parquet/json”)</p><p>1.2 和数据库的交互 spark.sql(“”)</p><h3 id="2-函数使用"><a href="#2-函数使用" class="headerlink" title="2.函数使用"></a>2.函数使用</h3><ul><li><p>2.1 printSchema() - 显示表结构</p></li><li><p>2.2 df.select(col) - 查找某一列的值</p></li><li><p>2.3 df.show([int n])  - 显示[某几行的]的值</p></li><li><p>2.4 df.filter(condition) - 过滤出符合条件的行</p></li><li><p>2.5 df.groupby(col).count() </p><p>df.groupby(col).agg(col,func.min(),func.max(),func.sum()) - 聚合函数</p></li><li><p>2.6 spark.createDataFrame([(),(),(),()…,()],(col1,col2,col3,…,coln))</p></li><li><p>2.7 自定义udf函数</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@pandas_udf("col1 type,col2 type,...,coln type",PandasUDFType.GROUPD_MAP)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(pdf)</span>:</span></div><div class="line"><span class="keyword">pass</span></div></pre></td></tr></table></figure><p>df.groupby(col).apply(f).show()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Spark-DataFrame学习&quot;&gt;&lt;a href=&quot;#Spark-DataFrame学习&quot; class=&quot;headerlink&quot; title=&quot;Spark DataFrame学习&quot;&gt;&lt;/a&gt;Spark DataFrame学习&lt;/h2&gt;&lt;h3 id=&quot;1-文件的
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
      <category term="Spark" scheme="https://github.com/DuncanZhou//tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模型记录</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/SomeModels/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/SomeModels/</id>
    <published>2018-08-10T09:52:09.160Z</published>
    <updated>2018-08-10T09:58:14.028Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实战模型记录"><a href="#实战模型记录" class="headerlink" title="实战模型记录"></a>实战模型记录</h2><h3 id="1-GBDT（Gradient-Boosting-Decision-Tree）"><a href="#1-GBDT（Gradient-Boosting-Decision-Tree）" class="headerlink" title="1.GBDT（Gradient Boosting Decision Tree）"></a>1.GBDT（Gradient Boosting Decision Tree）</h3><ul><li>GBDT中的树是<strong>回归树（不是分类树）</strong>，GBDT用来做回归预测，调整后也可以用来分类。</li><li>回归树：回归树总体流程类似于分类树，<strong>区别在于，回归树的每一个节点都会得到一个预测值</strong>，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每个feature的每个阈值找最好的分割点，<strong>但衡量标准不再是最大熵，而是最小平方误差</strong>。<strong>分枝终止条件为属性值唯一或者预设的终止条件（叶子个数上限）</strong></li><li>提升树算法：提升树是迭代多棵回归树来共同决策。<strong>当采用平方误差损失函数时</strong>，<strong>每一个棵回归树学习的是之前所有树的结论和残差</strong>，拟合得到一个当前的残差回归树。</li><li><strong>梯度提升决策树：</strong>当损失函数是平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。但<strong>对于一般的损失函数，往往每一步优化没那么容易</strong>（如绝对值损失函数和Huber损失函数），所以有梯度下降方法。</li></ul><h3 id="2-XGBoost（eXtreme-Gradient-Boosting）"><a href="#2-XGBoost（eXtreme-Gradient-Boosting）" class="headerlink" title="2.XGBoost（eXtreme Gradient Boosting）"></a>2.XGBoost（eXtreme Gradient Boosting）</h3><p>和gbdt对比：</p><ul><li>1.GBDT以CART作为基分类器，xgboost还<strong>支持线性分类器</strong>。</li><li>2.GBDT在优化函数中只用到一阶导数信息，<strong>xgboost则对代价函数进行了二阶泰勒展开</strong>，同时用到了一阶和二阶导数。</li><li>3.xgboost在代价函数中<strong>加入了正则项</strong>，控制了模型的复杂度。正则项包含两部分：叶子节点数和叶子结点输出分数。</li><li>4.划分点的查找:<strong>贪心算法和近似算法</strong></li><li>5.支持并行，<strong>在特征粒度上并行</strong>，预先对数据进行排序，保存为block结构，在节点分裂时计算每个特征的信息增益，<strong>各个特征的信息增益就是多个线程进行</strong>。</li></ul><h3 id="3-LightGBM"><a href="#3-LightGBM" class="headerlink" title="3.LightGBM"></a>3.LightGBM</h3><p>优化点</p><ul><li>1.Histogram算法：先把连续的浮点特征值离散化成k个整数，同事构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累计统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</li><li>2.带深度限制的Leaf-wise的叶子生长策略：每次从当前所有叶子中，<strong>找到分裂增益最大的一个叶子，然后分裂，如此循环</strong>。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。 </li></ul><h3 id="4-RandomForest"><a href="#4-RandomForest" class="headerlink" title="4.RandomForest"></a>4.RandomForest</h3><p>用<strong>bootstrap自助法生成m个训练集</strong>，<strong>对每个训练集构造一颗决策树</strong>，在节点找特征进行分裂的时候，并不是对所有特征找到使得指标（如信息增益）最大的，而是<strong>在特征中随机抽取一部分特征</strong>，在抽取到的特征中找到最优解，进行分裂。模型预测阶段就是bagging策略，分类投票，回归取均值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;实战模型记录&quot;&gt;&lt;a href=&quot;#实战模型记录&quot; class=&quot;headerlink&quot; title=&quot;实战模型记录&quot;&gt;&lt;/a&gt;实战模型记录&lt;/h2&gt;&lt;h3 id=&quot;1-GBDT（Gradient-Boosting-Decision-Tree）&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Data Mining" scheme="https://github.com/DuncanZhou//categories/Data-Mining/"/>
    
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>IV值和WOE值记录</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/IVandWOE/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/IVandWOE/</id>
    <published>2018-08-10T09:52:09.149Z</published>
    <updated>2018-08-10T09:55:49.539Z</updated>
    
    <content type="html"><![CDATA[<h2 id="IV和WOE记录"><a href="#IV和WOE记录" class="headerlink" title="IV和WOE记录"></a>IV和WOE记录</h2><h3 id="IV-（Information-Value）"><a href="#IV-（Information-Value）" class="headerlink" title="IV （Information Value）"></a>IV （Information Value）</h3><p>1）用途：评价特征或变量的预测能力。类似的指标还有信息增益    、增益率和基尼系数等</p><p>2）IV的计算依赖于WOE</p><h3 id="WOE（Weight-of-Evidence）"><a href="#WOE（Weight-of-Evidence）" class="headerlink" title="WOE（Weight of Evidence）"></a>WOE（Weight of Evidence）</h3><p>1）要对一个变量进行WOE编码，需要把这个变量进行分组处理（离散化 / 分箱），分组后对于第i组，WOE的计算公式如下：</p><script type="math/tex; mode=display">WOE_i=ln(\frac{py_i}{pn_i})=ln(\frac{\frac{\#y_i}{\#y_T}}{\frac{\#n_i}{\#n_T}})</script><p>其中，$py_i$是<strong>这个组中响应客户</strong>占所有<strong>样本中响应客户</strong>的比例，$pn_i$是<strong>这个组中未响应客户</strong>占<strong>样本中未响应客户</strong>的比例。</p><blockquote><p> 所以，WOE表示的实际上是<strong>“当前分组中响应客户占所有响应客户的比例”和”当前分组中没有响应的客户占所有没响应的客户的比例“的差异</strong></p></blockquote><h3 id="IV的计算"><a href="#IV的计算" class="headerlink" title="IV的计算"></a>IV的计算</h3><script type="math/tex; mode=display">IV_i=(py_i-pn_i)*WOE_i</script><script type="math/tex; mode=display">IV = \sum_{i}^{n}IV_i</script><p>其中，n为变量分组的个数。</p><h3 id="为什么使用IV而不是直接用WOE"><a href="#为什么使用IV而不是直接用WOE" class="headerlink" title="为什么使用IV而不是直接用WOE"></a>为什么使用IV而不是直接用WOE</h3><ul><li>1.IV和WOE的差别在于IV在WOE基础上乘以（$py_i-pn_i$）- $pyn$ ,乘以了这个$pyn$变量保证了每个分组的结果都是<strong>非负数</strong>。</li><li>2.乘以$pyn$后，体现出了变量当前分组中个体的数量占整体个体数量的比例，对变量预测能力的影响。</li></ul><h3 id="IV的极端情况处理"><a href="#IV的极端情况处理" class="headerlink" title="IV的极端情况处理"></a>IV的极端情况处理</h3><ul><li>1.合理分组</li><li>2.0 —&gt; 1</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;IV和WOE记录&quot;&gt;&lt;a href=&quot;#IV和WOE记录&quot; class=&quot;headerlink&quot; title=&quot;IV和WOE记录&quot;&gt;&lt;/a&gt;IV和WOE记录&lt;/h2&gt;&lt;h3 id=&quot;IV-（Information-Value）&quot;&gt;&lt;a href=&quot;#IV-（Inf
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
      <category term="DataMing" scheme="https://github.com/DuncanZhou//tags/DataMing/"/>
    
  </entry>
  
  <entry>
    <title>数据挖掘整理</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/DataMiningNotes/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/DataMiningNotes/</id>
    <published>2018-08-10T09:52:09.146Z</published>
    <updated>2018-08-10T09:57:55.638Z</updated>
    
    <content type="html"><![CDATA[<h2 id="数据挖掘整理"><a href="#数据挖掘整理" class="headerlink" title="数据挖掘整理"></a>数据挖掘整理</h2><h3 id="1-数据的基本描述"><a href="#1-数据的基本描述" class="headerlink" title="1.数据的基本描述"></a>1.数据的基本描述</h3><h4 id="1-1-中心趋势度量"><a href="#1-1-中心趋势度量" class="headerlink" title="1.1 中心趋势度量"></a>1.1 中心趋势度量</h4><ul><li><strong>均值</strong></li><li><strong>截尾均值</strong>：丢弃高低端极端值后的均值</li><li><strong>中位数</strong>：有序数据值得中间值</li><li><strong>众数</strong>：集合中出现最频繁的值</li><li><strong>中列数</strong>：最大值和最小值的平均值</li></ul><h4 id="1-2-数据散布"><a href="#1-2-数据散布" class="headerlink" title="1.2 数据散布"></a>1.2 数据散布</h4><ul><li><p><strong>极差</strong>：最大值与最小值之差</p></li><li><p><strong>分位数</strong>：取自数据分布的每隔一定间隔上的点，把数据划分成基本上大小相等的连贯集合</p></li><li><p><strong>四分位数</strong>：3个数据点，把数据分布划分成4个相等的部分，使得每部分表示数据分布的四分之一。（<strong>中位数、四分位数、百分位数</strong>是使用广泛的分位数）</p></li><li><p><strong>方差</strong></p></li><li><p><strong>标准差</strong></p></li><li><p><strong>四分位数极差（IQR）</strong>：第1个和第3个四分位数之间的距离，IQR = Q3 - Q1</p><blockquote><p>识别可疑的<strong>离群点</strong>的通畅规则是，挑选落在<strong>第3个四分位数之上</strong>或<strong>第一个四分位数之下至少1.5*IQR</strong>处的值。</p></blockquote><hr><p><strong><em>图形的表示</em></strong></p><hr></li></ul><ul><li><p><strong>a)盒图</strong>：盒的端点一般在四分位数上，使得盒的长度是四分位数极差IQR。中位数用盒内的线标记。盒外的两条线延伸到最小和最大观测值。</p></li><li><p><strong>b)分位数图</strong>：一种观察单变量数据分布的简单有效方法</p></li><li><p><strong>c)直方图：</strong></p></li><li><p><strong>d)散点图：</strong>确定两个数值变量之间看上去是否存在联系、模式或趋势的最有效的图形方法之一</p></li></ul><h4 id="1-3-相似性的度量"><a href="#1-3-相似性的度量" class="headerlink" title="1.3 相似性的度量"></a>1.3 相似性的度量</h4><ul><li>Jaccard相似性</li><li>余弦相似性</li><li>欧式距离、曼哈顿距离、闵可夫斯基距离</li></ul><h3 id="2-数据预处理"><a href="#2-数据预处理" class="headerlink" title="2.数据预处理"></a>2.数据预处理</h3><p><strong>2.1数据清洗</strong>：填写缺失值、光滑噪声数据，识别或删除离群点，并解决不一致性来“清理”数据</p><ul><li>缺失值的处理：忽略该行、人工填写缺失值、使用一个全局常量填充、使用属性的中心度量（均值或中位数）、使用与给定元组属同一类的所有样本的均值或中位数、使用最可能的值填充缺失值（使用回归、使用贝叶斯形式方法的基于推理的工具或决策树归纳确定）</li></ul><p><strong>2.2数据集成</strong>：分析中的数据来自多个数据源</p><ul><li>冗余和相关性分析：标称数据的卡方相关检验、Pearson相关系数、协方差</li></ul><p><strong>2.3数据归约</strong>：维归约和数值归约</p><p><strong>2.4数据变换</strong>：</p><ul><li>光滑：去掉噪声</li><li>属性构造：可以由给定的属性构造新的属性并添加到属性集中</li><li>聚集：对数据进行汇总或聚集</li><li>规范化：把属性数据按比例缩放</li><li>离散化：label encoder 、onehot</li><li>由标称数据产生概念分层：属性层级划分</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;数据挖掘整理&quot;&gt;&lt;a href=&quot;#数据挖掘整理&quot; class=&quot;headerlink&quot; title=&quot;数据挖掘整理&quot;&gt;&lt;/a&gt;数据挖掘整理&lt;/h2&gt;&lt;h3 id=&quot;1-数据的基本描述&quot;&gt;&lt;a href=&quot;#1-数据的基本描述&quot; class=&quot;headerlink
      
    
    </summary>
    
      <category term="Data Mining" scheme="https://github.com/DuncanZhou//categories/Data-Mining/"/>
    
    
      <category term="DataMing" scheme="https://github.com/DuncanZhou//tags/DataMing/"/>
    
  </entry>
  
  <entry>
    <title>OpenMPI学习</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/OpenMPILearning/OpenMPILearning/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/OpenMPILearning/OpenMPILearning/</id>
    <published>2018-08-10T09:24:49.523Z</published>
    <updated>2017-05-22T04:07:54.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>由于学校服务器上不允许随便装其他环境，所以用自己pc跑了程序.<br>学校服务器配置:10台物理机 每台机子2个核<br>PC:4核</p><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>1.OpenMPI<br>2.Python环境<br>3.nltk包安装<br>4.numpy环境<br>5.mpi4py</p><h3 id="代码程序"><a href="#代码程序" class="headerlink" title="代码程序"></a>代码程序</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/python</div><div class="line">#-*-coding:utf-8-*-</div><div class="line">&apos;&apos;&apos;@author:duncan&apos;&apos;&apos;</div><div class="line">import re</div><div class="line">import os</div><div class="line">import math</div><div class="line">import time</div><div class="line">import sys</div><div class="line">reload(sys)</div><div class="line">sys.setdefaultencoding(&apos;utf-8&apos;)</div><div class="line">import nltk</div><div class="line">from nltk.tokenize import word_tokenize</div><div class="line">from nltk.corpus import stopwords</div><div class="line">from nltk.stem import WordNetLemmatizer</div><div class="line">from mpi4py import MPI</div><div class="line">tweets_path = &apos;TestTweets/&apos;</div><div class="line"></div><div class="line">tags = 10</div><div class="line"></div><div class="line"></div><div class="line"># 公共通信变量</div><div class="line">comm = MPI.COMM_WORLD</div><div class="line"># 当前进程获取当前进程的id</div><div class="line">comm_rank = comm.Get_rank()</div><div class="line"># 获取整个通信结点的数量</div><div class="line">comm_size = comm.Get_size()</div><div class="line"></div><div class="line"># 计算一段文本中的tags,返回tag列表</div><div class="line">def GetTags(text):</div><div class="line">    twitter_stop_words = [&quot;from&quot;,&quot;TO&quot;,&quot;to&quot;,&quot;https&quot;,&quot;RT&quot;,&quot;URL&quot;,&quot;in&quot;,&quot;re&quot;,&quot;thank&quot;,&quot;thanks&quot;,&quot;today&quot;,&quot;yesterday&quot;,&quot;tomorrow&quot;,&quot;night&quot;,&quot;tonight&quot;,&quot;day&quot;,&quot;year&quot;,&quot;last&quot;,&quot;oh&quot;,&quot;yeah&quot;,&quot;amp&quot;]</div><div class="line">    &apos;&apos;&apos;</div><div class="line"></div><div class="line">    :param text: 文本</div><div class="line">    :return: 返回tag列表</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    # 首先针对推文中去除其他符号</div><div class="line">    text = re.sub(r&apos;[@|#][\d|\w|_]+|http[\w|:|.|/|\d]+&apos;,&quot;&quot;,text)</div><div class="line">    wordslist = []</div><div class="line">    if text == &quot;&quot; or text == None:</div><div class="line">        return []</div><div class="line"></div><div class="line">    # 利用nltk分词</div><div class="line">    words = word_tokenize(text)</div><div class="line">    for word in words:</div><div class="line">        # 去除停用词</div><div class="line">        if word not in (stopwords.words(&quot;english&quot;) and twitter_stop_words):</div><div class="line">            if(len(word) &gt; 2 and word.isalpha()):</div><div class="line">                wordslist.append(word.lower())</div><div class="line">    # 继续对词性进行标注</div><div class="line">    try:</div><div class="line">        pos = nltk.pos_tag(wordslist)</div><div class="line">    except Exception as e:</div><div class="line">        pos = []</div><div class="line">    if(len(pos) &lt; 1):</div><div class="line">        return []</div><div class="line">    tags = []</div><div class="line">    lemmatizer = WordNetLemmatizer()</div><div class="line">    for w in pos:</div><div class="line">        word = &quot;&quot;</div><div class="line">        # 是动词,做词性还原</div><div class="line">        if(w[1][0] == &apos;V&apos;):</div><div class="line">            word = lemmatizer.lemmatize(w[0],&apos;v&apos;)</div><div class="line">            tags.append(word)</div><div class="line">        # 是名词做词性还原</div><div class="line">        elif(w[1][0] == &quot;N&quot;):</div><div class="line">            word = lemmatizer.lemmatize(w[0])</div><div class="line">            tags.append(word)</div><div class="line">    i = 0</div><div class="line">    multicandidates = []</div><div class="line">    while(i &lt; len(pos) - 2):</div><div class="line">        phase = &quot;&quot;</div><div class="line">        # 动名词 | 动词 + 形容词 +名词</div><div class="line">        if (pos[i])[1][0] == &apos;V&apos; and (pos[i + 1][1][0] == &apos;N&apos; or (pos[i + 1][1][0] == &quot;J&quot; and pos[i + 2][1][0] == &quot;N&quot;)):</div><div class="line">            if pos[i + 1][1][0] == &apos;N&apos;:</div><div class="line">                suffix = lemmatizer.lemmatize(pos[i + 1][0],&apos;a&apos;)</div><div class="line">            else:</div><div class="line">                suffix = lemmatizer.lemmatize(pos[i + 1][0],&apos;n&apos;)</div><div class="line">            phase += lemmatizer.lemmatize((pos[i])[0],&apos;v&apos;) + &quot; &quot; + suffix</div><div class="line">            i = i + 2</div><div class="line">            while(i &lt; len(pos) and (pos[i])[1][0] == &apos;N&apos;):</div><div class="line">                phase += &quot; &quot; + lemmatizer.lemmatize((pos[i])[0])</div><div class="line">                i += 1</div><div class="line"></div><div class="line">            multicandidates.append(phase)</div><div class="line">        # 形容词　+ 名词</div><div class="line">        elif(pos[i][1][0] == &quot;J&quot; and pos[i + 1][1][0] == &quot;N&quot;):</div><div class="line">            if((i !=0 and pos[i - 1 ][1][0] != &quot;V&quot;) or i == 0):</div><div class="line">                phase +=lemmatizer.lemmatize((pos[i])[0],&quot;a&quot;) + &quot; &quot; + lemmatizer.lemmatize((pos[i + 1])[0])</div><div class="line">                i += 2</div><div class="line">                while(i &lt; len(pos) and (pos[i])[1][0] == &apos;N&apos;):</div><div class="line">                    phase += &quot; &quot; + lemmatizer.lemmatize((pos[i])[0],&quot;n&quot;)</div><div class="line">                    i += 1</div><div class="line">            multicandidates.append(phase)</div><div class="line">        else:</div><div class="line">            i += 1</div><div class="line">    if len(multicandidates) != 0:</div><div class="line">        tags += multicandidates</div><div class="line">    return tags</div><div class="line"></div><div class="line"></div><div class="line"># 计算words列表中词频</div><div class="line">def CalcTF(words,number):</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    :param number: 限制返回的tag数量</div><div class="line">    :param words: 传入words的list列表</div><div class="line">    :return: 返回对应字典形式 &#123;tag:TF&#125;,最终返回列表</div><div class="line">    &apos;&apos;&apos;</div><div class="line">    worddic = &#123;&#125;</div><div class="line">    # 先转换成集合</div><div class="line">    dic = set(words)</div><div class="line">    for word in dic:</div><div class="line">        TF = words.count(word)</div><div class="line">        worddic[word] = TF</div><div class="line">    # 将字典按照词频排序,取前10个</div><div class="line">    worddic = sorted(worddic.items(),key = lambda val:val[1],reverse=True)</div><div class="line">    return worddic[:number]</div><div class="line"></div><div class="line"></div><div class="line">if __name__ == &apos;__main__&apos;:</div><div class="line">    # text = &quot;how do i love you? Beautiful girls like eating cakes. I like dancing and swimming. How about you?he loves me? all we eat cakes.&quot;</div><div class="line">    # print CalcTF(GetTags(text),10)</div><div class="line"></div><div class="line">    flag = False</div><div class="line">    # 获取文件夹中用户总数及名称</div><div class="line">    names_list = os.listdir(tweets_path)</div><div class="line">    users_number = len(names_list)</div><div class="line">    if(comm_rank == 0):</div><div class="line">        print &quot;共%d个用户&quot; % users_number</div><div class="line">    block_size = math.ceil(users_number * 1.0 / comm_size)</div><div class="line">    # print &quot;block_size大小%d&quot; % block_size</div><div class="line">    start = i = int(comm_rank * block_size)</div><div class="line">    users_tags = []</div><div class="line">    start_time = time.time()</div><div class="line">    while(i &lt; block_size + start and i &lt; users_number):</div><div class="line">        with open(tweets_path + names_list[i],&apos;r&apos;) as f:</div><div class="line">            text = f.read()</div><div class="line">        # 得到某一用户的前n个tags,并有词频</div><div class="line">        user_tags = CalcTF(GetTags(text.decode(&quot;utf-8&quot;)),tags)</div><div class="line">        # print user_tags</div><div class="line">        users_tags += user_tags</div><div class="line">        i += 1</div><div class="line">        print &quot;已处理%d个用户&quot; % i</div><div class="line"></div><div class="line">    if(comm_rank == 0):</div><div class="line">        i = 1</div><div class="line">        while(i &lt; comm_size):</div><div class="line">            rev_tags = comm.recv(source=i)</div><div class="line">            # 合并起来</div><div class="line">            users_tags += rev_tags</div><div class="line">            i += 1</div><div class="line">            # print users_tags</div><div class="line">        # 写入文件</div><div class="line">        with open(&quot;/home/duncan/tags_tf&quot;,&quot;w&quot;) as f:</div><div class="line">            # 使得从第一行开始写</div><div class="line">            for tag in users_tags:</div><div class="line">                f.write(tag[0])</div><div class="line">                f.write(&quot;\t&quot;)</div><div class="line">                f.write(str(tag[1]))</div><div class="line">                f.write(&quot;\n&quot;)</div><div class="line">        print &quot;TF值写入完成&quot;</div><div class="line">        # 当对文件写入完成时发送广播</div><div class="line">        flag = True</div><div class="line">        comm.bcast(flag,root=0)</div><div class="line">    else:</div><div class="line">        # 除了0号进程外,其他进程发送数据,先发送自己的进程id</div><div class="line">        comm.send(users_tags,dest=0)</div><div class="line"></div><div class="line">    if(comm_rank != 0):</div><div class="line">        flag = comm.bcast(None,root=0)</div><div class="line"></div><div class="line">    # 当flag为True时继续向下计算</div><div class="line">    if(flag == True):</div><div class="line">        print &quot;process %d 开始计算idf&quot; % comm_rank</div><div class="line">        # 对保存好的文件中的TF词频tags继续计算IDF</div><div class="line">        with open(&quot;/home/duncan/tags_tf&quot;,&quot;r&quot;) as f:</div><div class="line">            lines = f.readlines()</div><div class="line">        # 每个进程结点需要处理的tags数</div><div class="line">        block_size = (int)(math.ceil(users_number * 1.0 / comm_size)) * tags</div><div class="line">        start = lineid = comm_rank * block_size</div><div class="line">        tags_tfidf = []</div><div class="line">        while(lineid &lt; block_size + start and lineid &lt; users_number * tags):</div><div class="line"></div><div class="line">            # 对lines[i]去计算它的idf,在其他用户tags中搜寻</div><div class="line">            user_id = int(math.ceil((lineid + 1) * 1.0 / tags))</div><div class="line">            # 所以该用户tags行标在[(user_id - 1) * tags,user_id * tags - 1]</div><div class="line">            search_rowid = 0</div><div class="line">            count = 1</div><div class="line">            while(search_rowid &lt; users_number * tags):</div><div class="line">                # 当在其中一个用户的tags中搜寻到后直接跳至下一个用户的tags</div><div class="line">                if(search_rowid &lt; (user_id - 1) * tags or search_rowid &gt; user_id * tags - 1):</div><div class="line">                    if((lines[search_rowid].split(&quot;\t&quot;))[0] == (lines[lineid].split(&quot;\t&quot;))[0]):</div><div class="line">                        count += 1</div><div class="line">                        # 跳到下一个用户tags开始处</div><div class="line">                        current_user_id = int(math.ceil((search_rowid + 1) * 1.0 / tags))</div><div class="line">                        search_rowid = current_user_id * tags</div><div class="line">                    else:</div><div class="line">                        search_rowid += 1</div><div class="line">                else:</div><div class="line">                    # 在所需要判断的用户的tags范围内,则跳出</div><div class="line">                    search_rowid = user_id * tags</div><div class="line">            idf = math.log(users_number * 1.0 / count,2)</div><div class="line">            # print count,idf</div><div class="line">            tags_tfidf.append(((lines[lineid].split(&quot;\t&quot;))[0],idf * int((lines[lineid].split(&quot;\t&quot;))[1])))</div><div class="line">            lineid += 1</div><div class="line"></div><div class="line">        end_time = time.time()</div><div class="line">        # 每个结点都处理完自己的用户推文</div><div class="line">        print &quot;process %d cost %f&quot; % (comm_rank,end_time - start_time)</div><div class="line"></div><div class="line">        # 该进程计算完其下所有用户的tags的tfidf</div><div class="line">        if(comm_rank == 0):</div><div class="line">            # 收集tags_tfidf</div><div class="line">            i = 1</div><div class="line">            while(i &lt; comm_size):</div><div class="line">                recv_tags = comm.recv(source = i)</div><div class="line">                tags_tfidf += recv_tags</div><div class="line">                i += 1</div><div class="line">            # 最终结果写入文件</div><div class="line">            with open(&quot;/home/duncan/tags_tfidf&quot;,&quot;w&quot;) as f:</div><div class="line">                for tag in tags_tfidf:</div><div class="line">                    f.write(tag[0])</div><div class="line">                    f.write(&quot;\t&quot;)</div><div class="line">                    f.write(str(tag[1]))</div><div class="line">                    f.write(&quot;\n&quot;)</div><div class="line">        else:</div><div class="line">            comm.send(tags_tfidf,dest=0)</div></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>1.48个用户,每个用户10个标签  加速比1.6左右(单核:365s;4核:226s)<br>3.100个用户,每个用户10个标签,加速比1.7左右(单核:781s;4核:457s)<br>3.1000个用户,每个用户10个标签,加速比(1.8左右)(单核:5706s;4核:3108s)<br>4.10000个用户,每个用户10个标签,加速比(2左右)(单核:69299s;4核:34643s)<br><img src="http://ww4.sinaimg.cn/large/006HJ39wgy1fftzigg1rxj30m80go0t5.jpg" alt="实验结果图"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;配置&quot;&gt;&lt;a href=&quot;#配置&quot; class=&quot;headerlink&quot; title=&quot;配置&quot;&gt;&lt;/a&gt;配置&lt;/h3&gt;&lt;p&gt;由于学校服务器上不允许随便装其他环境，所以用自己pc跑了程序.&lt;br&gt;学校服务器配置:10台物理机 每台机子2个核&lt;br&gt;PC:4核&lt;/p&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Martin Luther King-美国黑人民权运动领袖</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/MartinLutherKing/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/MartinLutherKing/</id>
    <published>2018-08-10T09:24:49.406Z</published>
    <updated>2018-01-20T02:53:02.000Z</updated>
    
    <content type="html"><![CDATA[<p>I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.<br>Five score years ago, a great American, in whose symbolic shadow we stand today, signed the Emancipation Proclamation. This momentous decree came as a great beacon light of hope to millions of Negro slaves who had been seared in the flames of withering injustice. It came as a joyous daybreak to end the long night of bad captivity.<br>But one hundred years later, the Negro still is not free. One hundred years later, the life of the Negro is still sadly crippled by the manacles of segregation and the chains of discrimination. One hundred years later, the Negro lives on a lonely island of poverty in the midst of a vast ocean of material prosperity. One hundred years later, the Negro is still languished in the corners of American society and finds himself an exile in his own land. And so we’ve come here today to dramatize a shameful condition.<br>In a sense we’ve come to our nation’s capital to cash a check. When the architects of our republic wrote the magnificent words of the Constitution and the Declaration of Independence, they were signing a promissory note to which every American was to fall heir. This note was a promise that all men, yes, black men as well as white men, would be guaranteed the “unalienable Rights” of “Life, Liberty and the pursuit of Happiness.” It is obvious today that America has defaulted on this promissory note, insofar as her citizens of color are concerned. Instead of honoring this sacred obligation, America has given the Negro people a bad check, a check which has come back marked “insufficient funds.”<br>But we refuse to believe that the bank of justice is bankrupt. We refuse to believe that there are insufficient funds in the great vaults of opportunity of this nation. And so, we’ve come to cash this check, a check that will give us upon demand the riches of freedom and the security of justice.<br>We have also come to this hallowed spot to remind America of the fierce urgency of Now. This is no time to engage in the luxury of cooling off or to take the tranquilizing drug of gradualism. Now is the time to make real the promises of democracy. Now is the time to rise from the dark and desolate valley of segregation to the sunlit path of racial justice. Now is the time to lift our nation from the quicksands of racial injustice to the solid rock of brotherhood. Now is the time to make justice a reality for all of God’s children.<br>It would be fatal for the nation to overlook the urgency of the moment. This sweltering summer of the Negro’s legitimate discontent will not pass until there is an invigorating autumn of freedom and equality. Nineteen sixty-three is not an end, but a beginning. And those who hope that the Negro needed to blow off steam and will now be content will have a rude awakening if the nation returns to business as usual. And there will be neither rest nor tranquility in America until the Negro is granted his citizenship rights. The whirlwinds of revolt will continue to shake the foundations of our nation until the bright day of justice emerges.<br>But there is something that I must say to my people, who stand on the warm threshold which leads into the palace of justice: In the process of gaining our rightful place, we must not be guilty of wrongful deeds. Let us not seek to satisfy our thirst for freedom by drinking from the cup of bitterness and hatred. We must forever conduct our struggle on the high plane of dignity and discipline. We must not allow our creative protest to degenerate into physical violence. Again and again, we must rise to the majestic heights of meeting physical force with soul force.<br>The marvelous new militancy which has engulfed the Negro community must not lead us to a distrust of all white people, for many of our white brothers, as evidenced by their presence here today, have come to realize that their destiny is tied up with our destiny. And they have come to realize that their freedom is inextricably bound to our freedom.<br>We cannot walk alone.<br>And as we walk, we must make the pledge that we shall always march ahead.<br>We cannot turn back.<br>There are those who are asking the devotees of civil rights, “When will you be satisfied?” We can never be satisfied as long as the Negro is the victim of the unspeakable horrors of police brutality. We can never be satisfied as long as our bodies, heavy with the fatigue of travel, cannot gain lodging in the motels of the highways and the hotels of the cities. We cannot be satisfied as long as the Negro’s basic mobility is from a smaller ghetto to a larger one. We can never be satisfied as long as our children are stripped of their selfhood and robbed of their dignity by signs stating “for whites only.” We cannot be satisfied as long as a Negro in Mississippi cannot vote and a Negro in New York believes he has nothing for which to vote. No, no, we are not satisfied, and we will not be satisfied until “justice rolls down like waters, and righteousness like a mighty stream.”<br>I am not unmindful that some of you have come here out of great trials and tribulations. Some of you have come fresh from narrow jail cells. And some of you have come from areas where your quest — quest for freedom left you battered by the storms of persecution and staggered by the winds of police brutality. You have been the veterans of creative suffering. Continue to work with the faith that unearned suffering is redemptive. Go back to Mississippi, go back to Alabama, go back to South Carolina, go back to Georgia, go back to Louisiana, go back to the slums and ghettos of our northern cities, knowing that somehow this situation can and will be changed.<br>Let us not wallow in the valley of despair, I say to you today, my friends.<br>And so even though we face the difficulties of today and tomorrow, I still have a dream. It is a dream deeply rooted in the American dream.<br>I have a dream that one day this nation will rise up and live out the true meaning of its creed: “We hold these truths to be self-evident, that all men are created equal.”<font color="red"><br>I have a dream that one day on the red hills of Georgia, the sons of former slaves and the sons of former slave owners will be able to sit down together at the table of brotherhood.<br>I have a dream that one day even the state of Mississippi, a state sweltering with the heat of injustice, sweltering with the heat of oppression, will be transformed into an oasis of freedom and justice.<br>I have a dream that my four little children will one day live in a nation where they will not be judged by the color of their skin but by the content of their character.<br>I have a dream today!<br>I have a dream that one day, down in Alabama, with its vicious racists, with its governor having his lips dripping with the words of “interposition” and “nullification” — one day right there in Alabama little black boys and black girls will be able to join hands with little white boys and white girls as sisters and brothers.<br>I have a dream today!<br>I have a dream that one day every valley shall be exalted, and every hill and mountain shall be made low, the rough places will be made plain, and the crooked places will be made straight; “and the glory of the Lord shall be revealed and all flesh shall see it together.”?<br>This is our hope, and this is the faith that I go back to the South with.<br>With this faith, we will be able to hew out of the mountain of despair a stone of hope. With this faith, we will be able to transform the jarring discords of our nation into a beautiful symphony of brotherhood. With this faith, we will be able to work together, to pray together, to struggle together, to go to jail together, to stand up for freedom together, knowing that we will be free one day.</font><br>And this will be the day — this will be the day when all of God’s children will be able to sing with new meaning:<br>My country ‘tis of thee, sweet land of liberty, of thee I sing.<br>Land where my fathers died, land of the Pilgrim’s pride,<br>From every mountainside, let freedom ring!<br>And if America is to be a great nation, this must become true.<br>And so let freedom ring from the prodigious hilltops of New Hampshire.<br>Let freedom ring from the mighty mountains of New York.<br>Let freedom ring from the heightening Alleghenies of<br>Pennsylvania.<br>Let freedom ring from the snow-capped Rockies of Colorado.<br>Let freedom ring from the curvaceous slopes of California.<br>But not only that.<br>Let freedom ring from Stone Mountain of Georgia.<br>Let freedom ring from Lookout Mountain of Tennessee.<br>Let freedom ring from every hill and molehill of Mississippi.<br>From every mountainside, let freedom ring.<br>And when this happens, when we allow freedom ring, when we let it ring from every village and every hamlet, from every state and every city, we will be able to speed up that day when all of God’s children, black men and white men, Jews and Gentiles, Protestants and Catholics, will be able to join hands and sing in the words of the old Negro spiritual:<br>Free at last! Free at last!<br>Thank God Almighty, we are free at last!</p><p>今天，我高兴地同大家一起参加这次将成为我国历史上为争取自由而举行的最伟大的示威集会。<br>100年前，一位伟大的美国人—今天我们就站在他的雕像前—签署了《解放黑奴宣言》。这项重要法令的颁布，对于千百万灼烤于非正义残焰中的黑奴，犹如带来希望之光的硕大灯塔，恰似结束漫漫长夜禁锢的欢畅黎明。<br>然而100年后的今天，我们必须正视黑人还没有得到自由这一悲惨的事实。100年后的今天，在种族隔离的镣铐和种族歧视的枷锁下，黑人的生活备受压榨。100年后的今天，黑人仍生活在物质充裕的海洋中一个穷困的孤岛上。100年后的今天，黑人仍然蜷缩在美国社会的角落里，并且意识到自己是故土家园中的流亡者。今天我们在这里集会，就是要把这种骇人听闻的情况公诸世人。<br>就某种意义而言，今天我们是为了要求兑现诺言而汇集到我们国家的首都来的。我们共和国的缔造者草拟宪法和独立宣言的气壮山河的词句时，曾向每一个美国人许下了诺言，他们承诺所有人—不论白人还是黑人—都享有不可让渡的生存权、自由权和追求幸福权。<br>就有色公民而论，美国显然没有实践她的诺言。美国没有履行这项神圣的义务，只是给黑人开了一张空头支票，支票上盖着“资金不足”的戳子后便退了回来。但是我们不相信正义的银行已经破产，我们不相信，在这个国家巨大的机会之库里已没有足够的储备。因此今天我们要求将支票兑现——这张支票将给予我们宝贵的自由和正义保障。<br>我们来到这个圣地也是为了提醒美国，现在是非常急迫的时刻。现在绝非奢谈冷静下来或服用渐进主义的镇静剂的时候。现在是实现民主的诺言时候。现在是从种族隔离的荒凉阴暗的深谷攀登种族平等的光明大道的时候，现在是向上帝所有的儿女开放机会之门的时候，现在是把我们的国家从种族不平等的流沙中拯救出来，置于兄弟情谊的磐石上的时候。<br>如果美国忽视时间的迫切性和低估黑人的决心，那么，这对美国来说，将是致命伤。自由和平等的爽朗秋天如不到来，黑人义愤填膺的酷暑就不会过去。1963年并不意味着斗争的结束，而是开始。有人希望，黑人只要撒撒气就会满足；如果国家安之若素，毫无反应，这些人必会大失所望的。黑人得不到公民的基本权利，美国就不可能有安宁或平静，正义的光明的一天不到来，叛乱的旋风就将继续动摇这个国家的基础。<br>但是对于等候在正义之宫门口的心急如焚的人们，有些话我是必须说的。在争取合法地位的过程中，我们不要采取错误的做法。我们不要为了满足对自由的渴望而抱着敌对和仇恨之杯痛饮。我们斗争时必须永远举止得体，纪律严明。我们不能容许我们的具有崭新内容的抗议蜕变为暴力行动。我们要不断地升华到以精神力量对付物质力量的崇高境界中去。<br>现在黑人社会充满着了不起的新的战斗精神，但是不能因此而不信任所有的白人。因为我们的许多白人兄弟已经认识到，他们的命运与我们的命运是紧密相连的，他们今天参加游行集会就是明证。他们的自由与我们的自由是息息相关的。我们不能单独行动。<br>当我们行动时，我们必须保证向前进。我们不能倒退。现在有人问热心民权运动的人，“你们什么时候才能满足？”<br>只要黑人仍然遭受警察难以形容的野蛮迫害，我们就绝不会满足。<br>只要我们在外奔波而疲乏的身躯不能在公路旁的汽车旅馆和城里的旅馆找到住宿之所，我们就绝不会满足。<br>只要黑人的基本活动范围只是从少数民族聚居的小贫民区转移到大贫民区，我们就绝不会满足。<br>只要我们的孩子被“仅限白人”的标语剥夺自我和尊严，我们就绝不会满足。<br>只要密西西比州仍然有一个黑人不能参加选举，只要纽约有一个黑人认为他投票无济于事，我们就绝不会满足。<br>不！我们现在并不满足，我们将来也不满足，除非正义和公正犹如江海之波涛，汹涌澎湃，滚滚而来。<br>我并非没有注意到，参加今天集会的人中，有些受尽苦难和折磨，有些刚刚走出窄小的牢房，有些由于寻求自由，曾在居住地惨遭疯狂迫害的打击，并在警察暴行的旋风中摇摇欲坠。你们是人为痛苦的长期受难者。坚持下去吧，要坚决相信，忍受不应得的痛苦是一种赎罪。<br>让我们回到密西西比去，回到亚拉巴马去，回到南卡罗来纳去，回到佐治亚去，回到路易斯安那去，回到我们北方城市中的贫民区和少数民族居住区去，要心中有数，这种状况是能够也必将改变的。<br>我们不要陷入绝望而不可自拔。朋友们，今天我对你们说，在此时此刻，我们虽然遭受种种困难和挫折，我仍然有一个梦想，这个梦想深深扎根于美国的梦想之中。<br>我梦想有一天，这个国家会站立起来，真正实现其信条的真谛：“我们认为真理是不言而喻，人人生而平等。”<br>我梦想有一天，在佐治亚的红山上，昔日奴隶的儿子将能够和昔日奴隶主的儿子坐在一起，共叙兄弟情谊。<br>我梦想有一天，甚至连密西西比州这个正义匿迹，压迫成风，如同沙漠般的地方，也将变成自由和正义的绿洲。<br>我梦想有一天，我的四个孩子将在一个不是以他们的肤色，而是以他们的品格优劣来评价他们的国度里生活。<br>今天，我有一个梦想。我梦想有一天，亚拉巴马州能够有所转变，尽管该州州长现在仍然满口异议，反对联邦法令，但有朝一日，那里的黑人男孩和女孩将能与白人男孩和女孩情同骨肉，携手并进。<br>今天，我有一个梦想。<br>我梦想有一天，幽谷上升，高山下降；坎坷曲折之路成坦途，圣光披露，满照人间。<br>这就是我们的希望。我怀着这种信念回到南方。有了这个信念，我们将能从绝望之岭劈出一块希望之石。有了这个信念，我们将能把这个国家刺耳的争吵声，改变成为一支洋溢手足之情的优美交响曲。<br>有了这个信念，我们将能一起工作，一起祈祷，一起斗争，一起坐牢，一起维护自由；因为我们知道，终有一天，我们是会自由的。<br>在自由到来的那一天，上帝的所有儿女们将以新的含义高唱这支歌：“我的祖国，美丽的自由之乡，我为您歌唱。您是父辈逝去的地方，您是最初移民的骄傲，让自由之声响彻每个山岗。”<br>如果美国要成为一个伟大的国家，这个梦想必须实现！</p><p>让自由之声从新罕布什尔州的巍峨的崇山峻岭响起来！<br>让自由之声从纽约州的崇山峻岭响起来！<br>让自由之声从宾夕法尼亚州的阿勒格尼山响起来！<br>让自由之声从科罗拉多州冰雪覆盖的落基山响起来！<br>让自由之声从加利福尼亚州蜿蜒的群峰响起来！<br>不仅如此，还要让自由之声从佐治亚州的石岭响起来！<br>让自由之声从田纳西州的瞭望山响起来！<br>让自由之声从密西西比的每一座丘陵响起来！<br>让自由之声从每一片山坡响起来！<br>当我们让自由之声响起，让自由之声从每一个大小村庄、每一个州和每一个城市响起来时，我们将能够加速这一天的到来，那时，上帝的所有儿女，黑人和白人，犹太教徒和非犹太教徒，耶稣教徒和天主教徒，都将手携手，合唱一首古老的黑人灵歌：<br>“自由啦！自由啦！感谢全能上帝，我们终于自由啦!”</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I am happy to join with you today in what will go down in history as the greatest demonstration for freedom in the history of our nation.
      
    
    </summary>
    
      <category term="Life" scheme="https://github.com/DuncanZhou//categories/Life/"/>
    
    
      <category term="English" scheme="https://github.com/DuncanZhou//tags/English/"/>
    
  </entry>
  
  <entry>
    <title>求最优解算法学习</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/Alogorithms_Learning/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/Alogorithms_Learning/</id>
    <published>2018-08-10T09:24:49.394Z</published>
    <updated>2018-01-19T03:39:32.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简要"><a href="#简要" class="headerlink" title="简要"></a>简要</h3><p>本篇主要记录三种求最优解的算法:<strong>动态规划(dynamic programming)</strong>,<strong>贪心算法</strong>和<strong>平摊分析</strong>.</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>1.动态规划是通过组合子问题的解而解决整个问题的.分治法算法是指将问题划分成一些<font color="red">独立</font>的子问题, <font color="red">递归地求解</font>各个子问题,然后<font color="red">合并子问题的解</font>而得到原问题的解.与此不同,动态规划适用于<font color="red">子问题不是独立</font>的情况,也就是各个子问题包含公共的子子问题.在这种情况下,若用分治法则会做许多不必要的工作,即重复地求解公共的子子问题.</p><p>动态规划算法的设计可以分为以下四个步骤:</p><blockquote><p>1.描述最优解的结构<br>2.递归定义最优解的值<br>3.按自底向上的方式计算最优解的值<br>4.由计算出的结果构造一个最优解</p></blockquote><p><font color="red"><strong>能否运用动态规划方法的标志之一:</strong></font>一个问题的最优解包含了子问题的一个最优解.这个性质为最优子结构.</p><p>适合采用动态规划的最优化问题的两个要素:<font color="red"><strong>最优子结构</strong></font>和<font color="red"><strong>重叠子问题</strong></font></p><h3 id="贪心算法"><a href="#贪心算法" class="headerlink" title="贪心算法"></a>贪心算法</h3><p>1.贪心算法是使所做的选择看起来都是当前最佳的,期望通过所做的局部最优选择来产生出一个全局最优解.</p><p>2.贪心算法的每一次操作都对结果产生直接影响,而动态规划不是.贪心算法对每个子问题的解决方案做出选择,不能回退;动态规划则会根据之前的选择结果对当前进行选择,有回退功能.动态规划主要运用于二维或三维问题,而贪心一般是一维问题.</p><p>3.贪心算法要经过证明才能运用到算法中.</p><h3 id="平摊分析"><a href="#平摊分析" class="headerlink" title="平摊分析"></a>平摊分析</h3><p>(未完-待续)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;简要&quot;&gt;&lt;a href=&quot;#简要&quot; class=&quot;headerlink&quot; title=&quot;简要&quot;&gt;&lt;/a&gt;简要&lt;/h3&gt;&lt;p&gt;本篇主要记录三种求最优解的算法:&lt;strong&gt;动态规划(dynamic programming)&lt;/strong&gt;,&lt;strong&gt;贪心算
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>天池-半导体质量预测</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/Semiconduction/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/Semiconduction/</id>
    <published>2018-08-10T09:24:49.387Z</published>
    <updated>2018-01-19T03:43:07.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="天池-半导体质量预测"><a href="#天池-半导体质量预测" class="headerlink" title="天池-半导体质量预测"></a>天池-半导体质量预测</h2><p>最近跟着做天池的比赛,将比赛过程中遇到的问题记录如下:</p><h3 id="1-特征的选择"><a href="#1-特征的选择" class="headerlink" title="1.特征的选择?"></a>1.特征的选择?</h3><blockquote><p><strong>特征选择的方法</strong>: 1) 嵌入式 2) 过滤式 3) 封装式</p></blockquote><p><strong>1)数据清洗:</strong></p><ul><li>1.筛选掉重复的列</li><li>2.对于类别类型特征,利用sklearn编码(one-hot, Label Encoder等)</li><li>3.使用平均值填充完后再去除冗余列(方差为0,列重复)</li></ul><p>清洗过后,特征从原来的8000多维降到了3400多维.</p><ul><li>4.特征中存在全为NaN值的,也去掉这些列</li></ul><blockquote><p>总结:数据清洗过后,总的特征维数维3342维;随机森林MSE为:0.03612</p></blockquote><p><strong>2)特征选择:</strong></p><ul><li>嵌入式: 根据模型来分析特征的重要性,最常见的方式为<strong>正则化</strong>来做特征选择.</li><li>过滤式: 评估单个特征和结果之间的相关程度,排序留下Top相关的特征部分.(缺点:没有考虑到特征之间的关联作用,可能把有用的关联特征误踢掉)</li><li>封装式: 把特征选择看作一个特征子集搜索问题,筛选各种特征子集,用模型评估效果.</li></ul><blockquote><p>处理方法:</p><ul><li>过滤式:使用单个随机森林得到的feature<em>importances</em>排序后保留了44个特征,为<br>[‘310X207’, ‘210X158’, ‘311X7’, ‘330X1132’, ‘220X13’, ‘310X149’, ‘750X883’, ‘210X207’, ‘312X144’, ‘210X192’, ‘312X61’, ‘312X66’, ‘440AX77’, ‘220X197’, ‘310X153’, ‘330X1190’, ‘344X252’, ‘310X33’, ‘210X174’, ‘440AX95’, ‘312X777’, ‘330X102’, ‘440AX187’, ‘340X161’, ‘312X55’, ‘330X590’, ‘210X89’, ‘330X1129’, ‘210X164’, ‘210X188’, ‘330X1146’, ‘310X119’, ‘360X1049’, ‘440AX182’, ‘750X640’, ‘440AX65’, ‘312X789’, ‘311X154’, ‘310X43’, ‘312X782’, ‘312X555’, ‘420X4’, ‘312X785’, ‘210X229’]</li></ul></blockquote><ul><li>包裹式:利用随机森林的性能作为评价指标筛选出200个特征<br>set([‘440AX98’, ‘310X207’, ‘210X158’, ‘261X641’, ‘261X269’, ‘312X61’, ‘312X66’, ‘220X197’, ‘520X317’, ‘400X151’, ‘400X150’, ‘400X153’, ‘330X594’, ‘330X590’, ‘210X164’, ‘210X8’, ‘420X4’, ‘330X1223’, ‘310X117’, ‘261X524’, ‘310X119’, ‘261X607’, ‘750X640’, ‘210X126’, ‘311X154’, ‘312X555’, ‘261X689’, ‘520X245’, ‘261X477’, ‘750X883’, ‘330X589’, ‘261X590’, ‘300X8’, ‘261X591’, ‘261X468’, ‘440AX77’, ‘300X3’, ‘220X179’, ‘330X1190’, ‘220X177’, ‘220X176’, ‘220X175’, ‘220X174’, ‘220X173’, ‘220X172’, ‘220X171’, ‘220X170’, ‘261X464’, ‘TOOL (#2)’, ‘330X351’, ‘330X102’, ‘330X355’, ‘330X354’, ‘330X1049’, ‘330X1042’, ‘312X57’, ‘312X55’, ‘210X89’, ‘330X1040’, ‘330X1043’, ‘330X1129’, ‘261X460’, ‘330X1044’, ‘261X462’, ‘330X1046’, ‘210X188’, ‘330X353’, ‘360X1049’, ‘440AX66’, ‘440AX67’, ‘440AX64’, ‘440AX65’, ‘330X135’, ‘330X134’, ‘312X144’, ‘330X133’, ‘330X132’, ‘330X139’, ‘312X782’, ‘210X174’, ‘312X785’, ‘312X789’, ‘261X608’, ‘261X609’, ‘520X312’, ‘520X313’, ‘520X314’, ‘330X1228’, ‘420X33’, ‘330X1132’, ‘261X600’, ‘261X601’, ‘330X641’, ‘330X1226’, ‘330X1221’, ‘330X1220’, ‘210X206’, ‘210X207’, ‘261X598’, ‘261X599’, ‘340X105’, ‘340X107’, ‘210X190’, ‘210X191’, ‘210X192’, ‘261X593’, ‘261X594’, ‘261X596’, ‘261X597’, ‘220X557’, ‘220X551’, ‘310X37’, ‘310X36’, ‘310X34’, ‘310X33’, ‘261X268’, ‘310X31’, ‘310X30’, ‘440AX95’, ‘210X3’, ‘210X4’, ‘210X5’, ‘210X6’, ‘210X7’, ‘312X777’, ‘210X9’, ‘261X260’, ‘261X261’, ‘261X266’, ‘261X267’, ‘261X264’, ‘261X265’, ‘520X246’, ‘520X247’, ‘261X736’, ‘261X737’, ‘520X242’, ‘520X243’, ‘310X153’, ‘344X252’, ‘440AX90’, ‘261X262’, ‘330X1146’, ‘440AX182’, ‘440AX187’, ‘261X687’, ‘261X688’, ‘310X43’, ‘330X157’, ‘330X404’, ‘261X512’, ‘261X513’, ‘330X401’, ‘520X55’, ‘330X403’, ‘261X517’, ‘261X518’, ‘261X519’, ‘311X7’, ‘330X409’, ‘330X159’, ‘330X158’, ‘330X461’, ‘520X333’, ‘220X13’, ‘310X149’, ‘520X244’, ‘261X338’, ‘330X1249’, ‘330X1248’, ‘300X7’, ‘261X330’, ‘261X331’, ‘340X161’, ‘261X333’, ‘330X1247’, ‘344X121’, ‘261X336’, ‘330X1244’, ‘520X240’, ‘330X1230’, ‘520X241’, ‘330X1241’, ‘261X335’, ‘220X535’, ‘210X129’, ‘210X128’, ‘220X531’, ‘220X530’, ‘210X125’, ‘210X124’, ‘210X127’, ‘261X526’, ‘210X121’, ‘210X120’, ‘210X123’, ‘261X230’, ‘261X592’, ‘440AX123’, ‘261X742’, ‘440AX99’, ‘311X83’, ‘220X178’, ‘330X535’, ‘210X229’]</li></ul><p>1) 提取特征后,xgboost的mse为0.0325341683406<br>2) 单个随机森林的5折交叉验证的平均mse为0.0288353227614<br>(max_depth=None,n_estimators=160,min_samples_leaf=2,max_features=n_features)</p><p>使用模型的features<em>importances</em>选择的特征和rfe做交集得到的特征为:<br>[‘210X158’, ‘330X1228’, ‘330X1132’, ‘220X13’, ‘310X149’, ‘750X883’, ‘330X589’, ‘210X207’, ‘440AX77’, ‘312X66’, ‘210X192’, ‘330X1190’, ‘310X33’, ‘312X555’, ‘310X31’, ‘310X30’, ‘440AX95’, ‘210X6’, ‘210X8’, ‘330X102’, ‘340X161’, ‘312X57’, ‘310X153’, ‘330X590’, ‘210X89’, ‘330X1129’, ‘210X164’, ‘312X777’, ‘210X188’, ‘330X1146’, ‘310X119’, ‘750X640’, ‘311X7’, ‘312X144’, ‘310X43’, ‘312X782’, ‘210X174’, ‘420X4’, ‘210X229’, ‘312X785’, ‘312X789’]</p><h3 id="2-缺失值的处理"><a href="#2-缺失值的处理" class="headerlink" title="2.缺失值的处理?"></a>2.缺失值的处理?</h3><ul><li>使用任意数值填充</li><li>使用平均值填充</li></ul><h3 id="3-维数降维"><a href="#3-维数降维" class="headerlink" title="3.维数降维?"></a>3.维数降维?</h3><h3 id="4-模型的选择"><a href="#4-模型的选择" class="headerlink" title="4.模型的选择?"></a>4.模型的选择?</h3><ol><li>Random Forest</li><li>GBDT(Gradient Boosting Decision Tree)<blockquote><p>这里记录下GBDT的发展过程: Regression Decision Tree -&gt; Boosting Decision Tree -&gt; Gradient Boosting Decision Tree,GBDT利用加法模型和前向分步法实现学习的优化过程.GBDT是一个基于迭代累加的决策树算法，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。 缺点:1) 计算复杂度高 2) 不适合高维稀疏特征</p></blockquote></li></ol><p>3.Xgboost</p><blockquote><p>xgboost是boosting Tree的一个很牛的实现:</p><ul><li>显示地把树模型复杂度作为正则项加到优化目标中</li><li>公式推导中用到了二阶导数,用了二阶泰勒展开</li><li>实现了分裂点寻找近似算法</li><li>利用了特征的稀疏性</li><li>并行计算</li></ul></blockquote><p>xgboost的训练速度远远快于传统的GBDT,10倍量级.</p><blockquote><p>总结:重新选用xgboost模型,参数如下,mse为0.0320532717482<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">params=&#123;&apos;booster&apos;:&apos;gbtree&apos;,</div><div class="line">    &apos;objective&apos;: &apos;reg:linear&apos;,</div><div class="line">    &apos;eval_metric&apos;: &apos;rmse&apos;,</div><div class="line">    &apos;max_depth&apos;:4,</div><div class="line">    &apos;lambda&apos;:6,</div><div class="line">    &apos;subsample&apos;:0.75,</div><div class="line">    &apos;colsample_bytree&apos;:1,</div><div class="line">    &apos;min_child_weight&apos;:1,</div><div class="line">    &apos;eta&apos;: 0.04,</div><div class="line">    &apos;seed&apos;:0,</div><div class="line">    &apos;nthread&apos;:8,</div><div class="line">     &apos;silent&apos;:0&#125;</div></pre></td></tr></table></figure></p></blockquote><h3 id="5-实践过程"><a href="#5-实践过程" class="headerlink" title="5.实践过程"></a>5.实践过程</h3><p>1.特征选择过程:去除全为Nan的列,去除Nan值个数大于200的列,去除object列,去除重复的列,选择Pearson相关系数&gt;0.2的列,最后共得到5600多维特征.<br>这一步很粗糙,改进: </p><ul><li>1) 加入object的列</li><li>2)特征维数继续筛减:可以试一下PCA降维</li><li>3)时间列属性的加入</li></ul><p>2.模型的选择:单模型线性回归线下mse:0.0388左右,而线上为0.0446.之前用随机森林回归预测,线下0.0297,而线上0.0493.从这个现象结合线下数据只有500条是否可以得出线下和线上数据并不是分布相同,或者说差异较大,而且线下训练可能存在过拟合.2017.12.24将三种回归模型加权平均融合提交结果.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;天池-半导体质量预测&quot;&gt;&lt;a href=&quot;#天池-半导体质量预测&quot; class=&quot;headerlink&quot; title=&quot;天池-半导体质量预测&quot;&gt;&lt;/a&gt;天池-半导体质量预测&lt;/h2&gt;&lt;p&gt;最近跟着做天池的比赛,将比赛过程中遇到的问题记录如下:&lt;/p&gt;
&lt;h3 id
      
    
    </summary>
    
      <category term="Competition" scheme="https://github.com/DuncanZhou//categories/Competition/"/>
    
    
      <category term="Competition" scheme="https://github.com/DuncanZhou//tags/Competition/"/>
    
  </entry>
  
  <entry>
    <title>NP-Hard问题(重点关注k-median问题)</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/k-median_problem/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/k-median_problem/</id>
    <published>2018-08-10T09:24:49.383Z</published>
    <updated>2018-01-19T03:40:33.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h3><p>例子:</p><blockquote><p>k-median问题:在备选工厂集里面选定k个工厂,使得需求点到离它最近工厂的加权距离总和最小.</p></blockquote><h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2 方法"></a>2 方法</h3><blockquote><p>近似方法分为两种:近似算法(Approximate Algorithms)和启发式算法(Heuristic Algorithms).近似算法通常有质量保证的解.然而启发式算法通常可找到在传统解决问题的经验中找到寻求一种面向问题的策略,之后用这种策略来在可行时间内寻找一个相对比较好的解,但对解的质量没有保证.</p></blockquote><p>工厂选址问题已经形成了多种求解方法,大致可以分为定性和定量两类方法.</p><ul><li>定性的方法主要是结合层次分析法和模糊综合法对各方案进行指标评价,找出最优选址.</li><li>定量的常用方法包括<strong>松弛算法和启发式式算法</strong>以及两者的结合.</li></ul><p>启发式搜索在状态空间中对每一个要搜索的位置按照某种方式进行评估,得到最优的位置,再从这个位置进行搜索直到达到目标.常用的<strong>启发式算法包括:禁忌搜索/遗传算法/进化算法/模拟退火算法/蚁群算法/人工神经网络</strong>等等.</p><p>Note:<br>Metric问题:指距离函数上是对称的且满足三角形不等式.</p><h2 id="3-求解NP-Hard问题常用方法"><a href="#3-求解NP-Hard问题常用方法" class="headerlink" title="3 求解NP-Hard问题常用方法"></a>3 求解NP-Hard问题常用方法</h2><h3 id="3-1-近似算法-Approximate-Algorithms-含近似比"><a href="#3-1-近似算法-Approximate-Algorithms-含近似比" class="headerlink" title="3.1 近似算法(Approximate Algorithms,含近似比)"></a>3.1 近似算法(Approximate Algorithms,含近似比)</h3><h3 id="3-1-1-贪心算法-Greedy-Algorithm"><a href="#3-1-1-贪心算法-Greedy-Algorithm" class="headerlink" title="3.1.1 贪心算法(Greedy Algorithm)"></a>3.1.1 贪心算法(Greedy Algorithm)</h3><h3 id="3-1-2-局部搜索-Local-Search"><a href="#3-1-2-局部搜索-Local-Search" class="headerlink" title="3.1.2 局部搜索(Local Search)"></a>3.1.2 局部搜索(Local Search)</h3><h3 id="3-2-启发式算法-Heuristic-Algorithms-不能保证解的质量"><a href="#3-2-启发式算法-Heuristic-Algorithms-不能保证解的质量" class="headerlink" title="3.2 启发式算法(Heuristic Algorithms,不能保证解的质量)"></a>3.2 启发式算法(Heuristic Algorithms,不能保证解的质量)</h3><h3 id="3-2-1-遗传算法-Genetic-Algorithm"><a href="#3-2-1-遗传算法-Genetic-Algorithm" class="headerlink" title="3.2.1 遗传算法(Genetic Algorithm)"></a>3.2.1 遗传算法(Genetic Algorithm)</h3><ul><li>交叉:对非公共部分进行单点交叉,若交叉过后,子代对于父代有改进,则用子代替换父代.</li><li>变异:基于种群中所有个体的概率,通过轮盘赌选择一个个体i进行变异:对个体i进行局部搜索,若有改进则取代种群中的最差个体.</li></ul><h3 id="3-2-2-模拟退火算法-SA算法"><a href="#3-2-2-模拟退火算法-SA算法" class="headerlink" title="3.2.2 模拟退火算法(SA算法)"></a>3.2.2 模拟退火算法(SA算法)</h3><ul><li>step1:贪心或随机构造初始解</li><li>step2:设置一个退火温度参数T,假设当前解X,他的邻居解X’与其目标函数差值为f(X)-f(X’),如果邻居解比其差,则需要以概率T来接受邻居解</li></ul><h3 id="3-2-3-蚁群算法-Ant-Colony-Optimization-ACO"><a href="#3-2-3-蚁群算法-Ant-Colony-Optimization-ACO" class="headerlink" title="3.2.3 蚁群算法(Ant Colony Optimization,ACO)"></a>3.2.3 蚁群算法(Ant Colony Optimization,ACO)</h3><h3 id="3-2-4-禁忌搜索-Tabu-Search"><a href="#3-2-4-禁忌搜索-Tabu-Search" class="headerlink" title="3.2.4 禁忌搜索(Tabu Search)"></a>3.2.4 禁忌搜索(Tabu Search)</h3><p>禁忌搜索在局部搜索(基于领域搜索)算法中加入了禁忌周期,使得搜索领域里的解分成了两种类型:禁忌的解和非禁忌的解,这样帮助了搜索过程”跳坑”,扩大了搜索领域,避免了局部最优,增强了解的疏散性.</p><p>思路:<br>（1）给定一个禁忌表（Tabu List）H=null,并选定一个初始解X_now.<br>（2）如果满足停止规则，则停止计算，输出结果；否则，在X_now的领域中选出满足不受禁忌的候选集N(X_now).在N(X_now)中选择一个评价值最佳的解X_next，X_now:=X_next;更新历史记录H, 重复步骤（2）.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-介绍&quot;&gt;&lt;a href=&quot;#1-介绍&quot; class=&quot;headerlink&quot; title=&quot;1 介绍&quot;&gt;&lt;/a&gt;1 介绍&lt;/h3&gt;&lt;p&gt;例子:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;k-median问题:在备选工厂集里面选定k个工厂,使得需求点到离它最近工厂
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>JAVA虚拟机了解</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/JVM/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/JVM/</id>
    <published>2018-08-10T09:24:49.380Z</published>
    <updated>2018-01-19T03:40:20.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-走进JAVA"><a href="#1-走进JAVA" class="headerlink" title="1.走进JAVA"></a>1.走进JAVA</h3><p>1.JDK(Java Developmen Kit):将Java程序设计语言,Java虚拟机和Java API类库这三部分统称为JDK.</p><p>2.JRE(Java Runtime Environment):把Java API类库中的Java SE API子集和Java虚拟机这两部分统称为JRE.</p><h3 id="2-JAVA内存区域与内存溢出异常"><a href="#2-JAVA内存区域与内存溢出异常" class="headerlink" title="2.JAVA内存区域与内存溢出异常"></a>2.JAVA内存区域与内存溢出异常</h3><p><strong>先放一张jvm运行时数据区分配情况图</strong><br><img src="https://raw.githubusercontent.com/DuncanZhou/images/master/jvm-01.png" alt="jvm运行时数据区"></p><ol><li><p>Java堆中分配内存方式:如果Java堆中的内存是绝对规整的,所有用过的内存都放在一边,空闲的内存放在另一边,中间放着一个指针作为分界点的指示器,那所分配内存就是仅仅把那个指针向空闲空间那边挪动一段与对象大小相等的距离,这种分配方式称为<strong>指针碰撞(Bump the pointer)</strong>;如果Java堆中的内存并不是规整的,已使用内存和空闲的内存相互交错,虚拟机就必须维护一个列表,记录上哪些内存块是可用的,在分配的时候从列表中找到一块足够大的空间划分给对象,并更新列表上的记录,这种分配方式称为<strong>空闲列表(Free List)</strong>;</p></li><li><p>划分内存过程需要考虑同步问题,两种方式:1)对分配内存空间的动作进行同步处理(CAS配上失败重试的方式保证更新操作的原子性);2)每个线程在Java堆中预先分配一小块内存,为<strong>本地线程分配缓冲(Thread Local Allocation Buffer)</strong>,当TLAB分配完了之后再做同步操作.</p></li><li><p>对象的访问定位:<strong>使用句柄和直接指针</strong></p></li></ol><h3 id="3-垃圾收集算法"><a href="#3-垃圾收集算法" class="headerlink" title="3.垃圾收集算法"></a>3.垃圾收集算法</h3><ul><li>标记-清除算法:首先标记出所有需要回收的对象,在标记完成后一次性回收所有被标记的对象.</li><li>复制算法:将可用内存分为两块,每次只使用其中一块,当着一块内存用完了,就将还存活的对象复制到另一块上,然后把已使用的内存空间一次清理掉.</li><li>标记-整理算法:首先标记出所有需要回收的对象,让所有存活的对象都向一端移动,然后直接清理掉端界外的内存.\</li><li>分代收集法:将堆分为新生代和老年代,根据每个代的特点选择合适的垃圾回收算法.</li></ul><h3 id="4-垃圾收集器"><a href="#4-垃圾收集器" class="headerlink" title="4.垃圾收集器"></a>4.垃圾收集器</h3><p><img src="https://raw.githubusercontent.com/DuncanZhou/images/master/jvm-02.png" alt="jvm垃圾收集器"></p><ol><li><p>Serial收集器<br>这个收集器是一个单线程的收集器,但它的”单线程”的意义并不仅仅说明它只会使用一个CPU或一条收集线程去完成垃圾收集工作,更重要的是在它进行垃圾收集时,必须暂停其他所有的工作线程,直到它收集结束.(Client模式下默认的垃圾收集器)</p></li><li><p>ParNew收集器<br>ParNew收集器就是Serial收集器的多线程版本,除了使用多条线程进行垃圾收集之外,其余行为包括Serial收集器可用的所有控制参数,收集算法,Stop the world,对象分配规则,回收策略等都与Serial收集器完全一样.(Server模式下的垃圾收集器)</p></li><li><p>Parallel Scavenge收集器<br>Parallel Scavenge收集器更多关注于控制吞吐量(吞吐量=运行用户代码时间/(运行用户代码时间+垃圾收集时间)).</p></li><li><p>Serial Old收集器</p></li><li><p>Parallel Old收集器</p></li><li><p>CMS收集器<br>CMS收集器是一种以<strong>获取最短回收停顿时间为目标</strong>的收集器.目前很大一部分的Java应用集中在互联网站或者B/S系统的服务端上.<br>CMS收集器是基于”标记-清除”算法实现的,运作过程分为4个步骤:</p></li></ol><ul><li>初始标记</li><li>并发标记</li><li>重新标记</li><li>并发清除</li></ul><ol><li>G1收集器<br>运作大致分为:</li></ol><ul><li>初始标记(标记GC Roots直接关联到的对象)</li><li>并发标记(GC Roots可达性分析)</li><li>最终标记(修正在并发标记阶段标记发生改变的那部分标记记录)</li><li>筛选回收(对各个Region的回收价值和成本排序)</li></ul><h3 id="5-内存对象收集器"><a href="#5-内存对象收集器" class="headerlink" title="5.内存对象收集器"></a>5.内存对象收集器</h3><p>1.新生代GC(Minor GC):指发生在新生代的垃圾收集动作,Minor GC非常频繁,一般回收速度也比较快.</p><p>2.老年代GC(Major GC/Full GC):指发生在老年代的GC,出现了Major GC,经常会伴随至少一次的Minor GC,Major GC的速度一般会比Minor GC慢10倍以上.</p><h3 id="6-虚拟机性能监控与故障处理工具"><a href="#6-虚拟机性能监控与故障处理工具" class="headerlink" title="6.虚拟机性能监控与故障处理工具"></a>6.虚拟机性能监控与故障处理工具</h3><ul><li>jps:虚拟机进程状况工具</li><li>jstat:虚拟机统计信息监视工具</li><li>jinfo:Java配置信息工具(实时地查看和调整虚拟机各项参数)</li><li>jmap:Java内存映像工具</li><li>jhat:虚拟机堆转储快照分析工具</li><li>jstack:Java堆栈跟踪工具</li><li>JConsole和VisualVM(可视化工具)</li></ul><h3 id="7-虚拟机执行子系统"><a href="#7-虚拟机执行子系统" class="headerlink" title="7.虚拟机执行子系统"></a>7.虚拟机执行子系统</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-走进JAVA&quot;&gt;&lt;a href=&quot;#1-走进JAVA&quot; class=&quot;headerlink&quot; title=&quot;1.走进JAVA&quot;&gt;&lt;/a&gt;1.走进JAVA&lt;/h3&gt;&lt;p&gt;1.JDK(Java Developmen Kit):将Java程序设计语言,Java虚拟机
      
    
    </summary>
    
      <category term="Java" scheme="https://github.com/DuncanZhou//categories/Java/"/>
    
    
  </entry>
  
  <entry>
    <title>四则运算表达式求值</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/Expression/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/Expression/</id>
    <published>2018-08-10T09:24:49.377Z</published>
    <updated>2018-01-19T03:40:06.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="表达式求值"><a href="#表达式求值" class="headerlink" title="表达式求值"></a>表达式求值</h3><blockquote><p>对于表达式的求值,一般使用中缀表达式转后缀表达式后,对后缀表达式求值,因为对于后缀或者前缀表达式计算,计算的顺序都是唯一的.</p></blockquote><h4 id="中缀表达式转后缀表达式的方法："><a href="#中缀表达式转后缀表达式的方法：" class="headerlink" title="中缀表达式转后缀表达式的方法："></a>中缀表达式转后缀表达式的方法：</h4><ul><li>1.遇到操作数：直接输出（添加到后缀表达式中）</li><li>2.栈为空时，遇到运算符，直接入栈</li><li>3.遇到左括号：将其入栈</li><li>4.遇到右括号：执行出栈操作，并将出栈的元素输出，直到弹出栈的是左括号，左括号不输出。</li><li>5.遇到其他运算符：加减乘除：弹出所有优先级大于或者等于该运算符的栈顶元素，然后将该运算符入栈</li><li>6.最终将栈中的元素依次出栈，输出。<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">public ArrayList&lt;String&gt; SuffixExpression(ArrayList&lt;String&gt; string)&#123;</div><div class="line">        ArrayList&lt;String&gt; suffix = new ArrayList&lt;&gt;();</div><div class="line">        Stack&lt;String&gt; stack = new Stack&lt;&gt;();</div><div class="line">        for(int i = 0; i &lt; string.size(); i++)&#123;</div><div class="line">            if(string.get(i).equals(&quot;*&quot;) || string.get(i).equals(&quot;/&quot;) || string.get(i).equals(&quot;(&quot;))&#123;</div><div class="line">                //push</div><div class="line">                stack.push(string.get(i));</div><div class="line">            &#125;</div><div class="line">            else if(string.get(i).equals(&quot;+&quot;) || string.get(i).equals(&quot;-&quot;))&#123;</div><div class="line">                while(!stack.isEmpty() &amp;&amp; (stack.peek().equals(&quot;*&quot;) || stack.peek().equals(&quot;/&quot;)))</div><div class="line">                &#123;</div><div class="line">                        suffix.add(stack.pop());</div><div class="line">                &#125;</div><div class="line">                //push</div><div class="line">                stack.push(string.get(i));</div><div class="line">            &#125;</div><div class="line">            else if(string.get(i).equals(&quot;)&quot;))</div><div class="line">            &#123;</div><div class="line">                while(!stack.isEmpty() &amp;&amp; !stack.peek().equals(&quot;(&quot;))</div><div class="line">                    suffix.add(stack.pop());</div><div class="line">                //pop (</div><div class="line">                stack.pop();</div><div class="line">            &#125;else&#123;</div><div class="line">                suffix.add(string.get(i));</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        //push all elements in stack</div><div class="line">        while(!stack.isEmpty())</div><div class="line">            suffix.add(stack.pop());</div><div class="line">        return suffix;</div><div class="line">    &#125;</div></pre></td></tr></table></figure></li></ul><h3 id="计算后缀表达式"><a href="#计算后缀表达式" class="headerlink" title="计算后缀表达式"></a>计算后缀表达式</h3><p>遇到操作数压入栈,否则弹出两个操作数进行操作后再压入栈中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">public int CalaculateSuffix(ArrayList&lt;String&gt; suffix)&#123;</div><div class="line">        //when meet operator, pop two elements to calaculate</div><div class="line">        Stack&lt;String&gt; stack = new Stack&lt;&gt;();</div><div class="line">        for(int i = 0; i &lt; suffix.size(); i++)&#123;</div><div class="line">            if(!(suffix.get(i).equals(&quot;+&quot;) || suffix.get(i).equals(&quot;-&quot;) || suffix.get(i).equals(&quot;*&quot;) || suffix.get(i).equals(&quot;/&quot;)))</div><div class="line">                //push into stack</div><div class="line">                stack.push(suffix.get(i));</div><div class="line">            else&#123;</div><div class="line">                //calucate</div><div class="line">                String operator = suffix.get(i);</div><div class="line">                String b = stack.pop();</div><div class="line">                String a = stack.pop();</div><div class="line">                String temp = String.valueOf(operate(a,b,operator));</div><div class="line">                stack.push(temp);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        return Integer.valueOf(stack.pop());</div><div class="line">    &#125;</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;表达式求值&quot;&gt;&lt;a href=&quot;#表达式求值&quot; class=&quot;headerlink&quot; title=&quot;表达式求值&quot;&gt;&lt;/a&gt;表达式求值&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;对于表达式的求值,一般使用中缀表达式转后缀表达式后,对后缀表达式求值,因为对于后缀或者前缀
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
      <category term="Stack" scheme="https://github.com/DuncanZhou//tags/Stack/"/>
    
  </entry>
  
  <entry>
    <title>P问题/NP问题/NP-Hard问题/NP-Complete问题</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/NP_Problem/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/NP_Problem/</id>
    <published>2018-08-10T09:24:49.375Z</published>
    <updated>2018-01-19T03:41:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>近日,论文中涉及到NP-Hard问题,写下笔记对以上问题进行区分.</p><blockquote><p>P问题:在多项式时间内可以求解的问题.</p><p>NP问题:在多项时间内不能求解,在多项式时间内可以验证的问题.</p><p>NP-Hard问题:所有的NP问题在多项式时间内可以归约到该问题,该问题为NP-Hard问题.</p><p>NP-Complete问题:一个问题即是NP-Hard问题,同时又是NP问题.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日,论文中涉及到NP-Hard问题,写下笔记对以上问题进行区分.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;P问题:在多项式时间内可以求解的问题.&lt;/p&gt;
&lt;p&gt;NP问题:在多项时间内不能求解,在多项式时间内可以验证的问题.&lt;/p&gt;
&lt;p&gt;NP-Hard问题:所有的NP问题
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>python-MPI安装命令</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/Python-MPI/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/Python-MPI/</id>
    <published>2018-08-10T09:24:49.372Z</published>
    <updated>2018-01-19T03:42:23.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="在Ubuntu下安装MPI环境-python环境"><a href="#在Ubuntu下安装MPI环境-python环境" class="headerlink" title="在Ubuntu下安装MPI环境(python环境)"></a>在Ubuntu下安装MPI环境(python环境)</h3><p><font color="red">Step1:</font>安装python环境&lt;/br&gt;</p><p><font color="red">Step2:</font>sudo apt-get install openmpi-bin&lt;/br&gt;</p><p><font color="red">Step3:</font>sudo apt-get install libopenmpi-dev&lt;/br&gt;</p><p><font color="red">Step4:</font>sudo apt-get install python-mpi4py&lt;/br&gt;</p><p><strong>(第三步不要忽略)</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;在Ubuntu下安装MPI环境-python环境&quot;&gt;&lt;a href=&quot;#在Ubuntu下安装MPI环境-python环境&quot; class=&quot;headerlink&quot; title=&quot;在Ubuntu下安装MPI环境(python环境)&quot;&gt;&lt;/a&gt;在Ubuntu下安装MPI
      
    
    </summary>
    
      <category term="Note" scheme="https://github.com/DuncanZhou//categories/Note/"/>
    
    
  </entry>
  
  <entry>
    <title>社交网络中抽取有代表性的用户</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/Representatives/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/Representatives/</id>
    <published>2018-08-10T09:24:49.368Z</published>
    <updated>2018-01-19T03:42:40.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-为什么要做这个问题"><a href="#1-为什么要做这个问题" class="headerlink" title="1.为什么要做这个问题"></a>1.为什么要做这个问题</h3><h4 id="1-1-从社会应用角度"><a href="#1-1-从社会应用角度" class="headerlink" title="1.1 从社会应用角度"></a>1.1 从社会应用角度</h4><ul><li>在HCI(人机交互)中,实施调查和去获得用户的反馈都是主要针对有代表性的用户.</li><li>代表性人物的行为习惯和关注点可以折射出整体用户的兴趣偏向和关注点,对于广告投放,物品推荐是有助的. </li><li>对于目前日益增长的社交网络用户,从大量的社交网络用户中抽取一个具有代表性的子集才是Human-readable的,有益于数据分析,相当于一个数据摘要.</li></ul><h4 id="1-2-从科研方法的角度"><a href="#1-2-从科研方法的角度" class="headerlink" title="1.2 从科研方法的角度"></a>1.2 从科研方法的角度</h4><ul><li>从大量模型或数据点中抽取一个保留了原数据集的特征是机器学习/计算机视觉领域数据分析和推荐系统领域都是一个重要的问题.</li><li>机器学习领域,找原型子集来辅助分类算法.</li></ul><h3 id="2-怎样定义代表性"><a href="#2-怎样定义代表性" class="headerlink" title="2.怎样定义代表性"></a>2.怎样定义代表性</h3><blockquote><p>Note:和在社交网络中寻找影响力最大化的问题不同,找出具有代表性的用户的目的是抽取一些”平均”的用户,他们能够在统计上代表原来所有用户的特征.</p></blockquote><h4 id="2-1-代表性用户具备的条件"><a href="#2-1-代表性用户具备的条件" class="headerlink" title="2.1 代表性用户具备的条件:"></a>2.1 代表性用户具备的条件:</h4><p><font color="blue">版本一.</font></p><ul><li>1.从<font color="green">属性特征角度</font>上,他们很好的代表了原数据集用户的属性特征(行为习惯/性格特征/领域情况等等),即,与原数据集用户具有<strong><font color="red">较少的特征损耗</font></strong></li><li>2.从<font color="green">分布特征角度</font>,代表性子集应尽可能拟合原数据集的样本分布,即,与原数据集具有<strong><font color="red">较少的分布损耗</font></strong>(类似于原数据集中每个领域的人物分布,代表性子集能够拟合原数据集每个领域的人物分布)</li><li>3.从<font color="green">差异性角度</font>上,代表性子集需要能够作为每个领域的<font color="red"><strong>典型</strong></font>人物,所以代表性子集内部各领域之间的人物需要保持一定的差异性,即,代表性子集内部需要<strong><font color="red">较大的差异性或较小的相似性</font></strong></li></ul><p><font color="blue">版本二.</font></p><ul><li>1.从<font color="green">特征角度</font>上,他们很好的代表了原数据集用户的属性特征(行为习惯/性格特征/领域情况等等),即,与原数据集用户具有<strong><font color="red">较少的特征损耗</font></strong></li><li>2.从<font color="green">分布角度</font>,代表性子集在满足(1)条件下应尽可能的分散或稀疏,使得子集可以尽可能地还原原数据集的分布,即,P具有<strong><font color="red">具有稀疏性</font></strong>;<br>-note:如果仅仅要求<strong>特征损耗最小</strong>,可能会导致代表性子集都聚集在人数较多较相似的团体中,以致于原数据集的分布丢失.</li></ul><p>目前倾向于版本一.</p><h4 id="2-2-问题定义"><a href="#2-2-问题定义" class="headerlink" title="2.2 问题定义:"></a>2.2 问题定义:</h4><p>在原数据集人物集合中寻找这样的代表性子集P</p><ul><li>a)P能够满足以上代表性的定义</li><li>b)P是数量最小的那个代表性集合</li></ul><h4 id="2-3-Novel之处或者contibution"><a href="#2-3-Novel之处或者contibution" class="headerlink" title="2.3 Novel之处或者contibution:"></a>2.3 Novel之处或者contibution:</h4><ul><li>1.代表性人物包含了两种情况的综合考虑,之前论文中大多考虑单一方面</li><li>2.代表性人物的大小不需要先验设定.</li></ul><p>将用户以各个属性构建向量,以向量之间的距离来定义人物之间的代表性.<br>以Twitter社交拓扑为例,当A用户关注了B用户,将会有A指向B的一条有向边,</p><h3 id="3-如何具体评价子集的代表性"><a href="#3-如何具体评价子集的代表性" class="headerlink" title="3.如何具体评价子集的代表性"></a>3.如何具体评价子集的代表性</h3><h3 id="4-方法"><a href="#4-方法" class="headerlink" title="4.方法"></a>4.方法</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-为什么要做这个问题&quot;&gt;&lt;a href=&quot;#1-为什么要做这个问题&quot; class=&quot;headerlink&quot; title=&quot;1.为什么要做这个问题&quot;&gt;&lt;/a&gt;1.为什么要做这个问题&lt;/h3&gt;&lt;h4 id=&quot;1-1-从社会应用角度&quot;&gt;&lt;a href=&quot;#1-1-从社
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Personalized Search泛读记录</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/Personalized%20Search%E6%B3%9B%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/Personalized Search泛读记录/</id>
    <published>2018-08-10T09:24:49.365Z</published>
    <updated>2018-01-19T03:41:52.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Backgournd"><a href="#Backgournd" class="headerlink" title="Backgournd"></a>Backgournd</h3><p>搜索在20年前就已出现在互联网，而如今搜索已经无处不在。传统的搜索像这样，用户给出Query，Query中包含1个或多个关键词，搜索引擎通过关键词去检索返回查询结果。然而，在互联网上存在的资源早已是亿万级，所以仅仅用传统的搜索方法去返回给用户查询结果势必会存在大量用户不需要的结果，根据2007年”Tag recommendations in folksonomies”一文中提出不考虑用户偏好返回的搜索结果中仅有20%-45%是用户想要的，另外,用户所想查找的内容也可能远远不在结果的前列，所以，这类问题的解决需要在传统的搜索方法上考虑context-上下文，即，<br>简要概括:1.用户搜索返回的结果大量是其所不需要的;2.不同的用户提出同一个关键词,搜索引擎返回的结果都是同样的,而不同的用户使用同一个关键词所想搜索的意图其实可能是不同的.<br>将(1)用户的行为、习惯、兴趣/历史搜索结果等等;(2)资源上下文(3)任务上下文等因素考虑进去。</p><p>网络个性化用于四类:predicting web navigation, assisting personalization information, personalizing content, and personalizing search results<br>有两类方法:协同过滤和user profiles<br>协同过滤的缺点:这种方法能根据大多数人的兴趣推测什么是流行的,但不能预测某一个用户是否对新的页面是否感兴趣.</p><p><strong>1.User Models</strong><br>1.1 可以用来构建用户特征的有<br>a)内容方面:查询的关键词、网页的内容、桌面索引等等<br>b)行为方面:浏览的网页、tag 活动/直接或间接的反馈等<br>c)上下文方面:性别、年龄、地理位置、时间等</p><p>1.2 时间上分短期和长期兴趣(将两者结合起来,按时间分配权重)<br>1.2.1长期兴趣偏好建立方式:<br>    行为:具体的查询和URLs<br>    内容:语言模型/主题模型</p><p>1.2.2短期兴趣偏好建立方式:<br>    搜索session的queries</p><p>1.3 用户分个体还是某类群体</p><p><strong><font color="red">Note:</font></strong><br><strong><font size="4">a</font></strong>.在用户model中还需考虑的有长期和短期的兴趣,仅仅根据用户长期的兴趣来推测用户现在想要的搜索结果会有偏差.e.g.一个人之前搜索的”java”都是关于编程语言,但不排除他下一次搜索”java”是要找”java咖啡或者java 岛”.用户的兴趣会改变的.<br><strong><font size="4">b</font></strong>.对用户可profile的信息有:Clickthrough Histories/Queries Histories/搜索过网页的Snippets/收藏过的书签<br><strong><font size="4">c</font></strong>.Domain Ontological:所谓的领域本体（domain-specific ontology）就是对学科概念的一种描述，包括学科中的概念、概念的属性、概念间的关系以及属性和关系的约束。由于知识具有显著的领域特性，所以领域本体能够更为合理而有效地进行知识的表示。<br><strong><font size="4">d</font></strong>.Folksonomy中的challenges:(1)用户标注的tags有很多的同样的拼写,不同意思的单词;有很多同义词;(2)怎么根据tags去对用户偏好建模(Tags聚类/VSM-空间向量模型/领域本体论(有一篇文章中将用户的tags映射到ODP(the Open Directionary Project)-Web topic ontology中))<br>—在Folksonomy中,用户标注的资源都是用户所感兴趣的资源,或者说用户所标注的资源都能代表用户的兴趣偏好;也就是说用户不再仅仅是web资源的消费者,同时用户可以通过承担web的一些任务同时方便其他web用户.(Folksnonomy这样一类系统代表性的有:Flickr.com/Delicious.com/Last.fm)<br><strong><font size="4">e</font></strong>.Clustering:聚类用于两个方面:切分和分层(分级).<br><strong><font size="4">f</font></strong>.<strong>Social Context</strong>:有一些预定义好的值,每个上下文的值都有具体的值.<strong>Verbal Context:</strong>历史查询/点击记录等</p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>1.可以入手的方面有三类:user model(long-term &amp; short-term)/query的替换或扩充/resource(web resource)/privacy/evaluation/查询效率<br>2.可以选择的对象有两个:Server端和Client-Side端</p><p>用户相关性:<br>1.直接的显示反馈<br>2.隐式地从点击熵获得</p><p>用户潜在的意图:<br>1.上下文的元数据<br>    Location/Time/Device</p><p>2.过去的行为<br>    当前的session活动/长期的活动和兴趣偏好</p><blockquote><p>基本的方法：<br>（1）对Query做扩展或替换<br>比如，用户正在浏览的关于汽车的页面，那么当他搜索“轮廓”时，会将Query的关键词添加“汽车”关键词，使返回的结果是用户想要找的汽车轮廓结果。<br>（2）对结果排序<br>根据user profile，进行相似度匹配对结果重新排序。</p></blockquote><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>1.MovieLens(notations &amp; ratings)<br>2.Delicious.com(bookmarks)<br>3.Flickr(notations)<br>4.DOMZ(ODP)—web search经常使用<br>5.BibSonomy(<a href="http://bibsonomy.org" target="_blank" rel="external">http://bibsonomy.org</a>)<br>6.CiteULike(网页书签数据)</p><h3 id="泛读论文时方法总结"><a href="#泛读论文时方法总结" class="headerlink" title="泛读论文时方法总结"></a>泛读论文时方法总结</h3><p>其他方法：<br>1）2002年CIKM中将用户的Query分类<br>2）2003年”Scaling Personalized Web Search”根据用户兴趣给页面分配权重用图来计算，类似PageRank<br>3）2005年WWW”A Personalized Search Engine Based on Web-Snippet Hierarchical Clustering”对网页的Snippet做了聚类然后对结果再根据User Profile对查询结果再排序<br><strong><font color="red">(Snippet指的是网页的标题和摘要)</font></strong><br>4）2005年WWW-“CubeSVD: A Novel Approach to Personalized Web Search”将用户的点击链接历史记录(who click which web page)作为user profile部分辅助查询.通过从点击链接记录数据中发现用户的兴趣和搜索信息的模式.<br>5)2005年CIKM”Implicit User Modeling for Personalized Search”User Model方法,完成了一个客户端查询代理.针对长期兴趣可能会改变的问题,本文中利用即时的搜索上下文和隐式的反馈来user model.<br>隐式的反馈信息有着两类:1)根据之前的查询寻找合适的term去扩展现在的query;2)利用用户已经看过的文档形成摘要来对没有看过的文档重新排序<br>6)2005年WI”Personalized Serach Based on User Search History”根据用户搜索历史对用户profile,利用User Profiles来对查询或者snippets分类,然后再对搜索结果重排序<br>7)2005年WI”Personaliezed Search Results with User Interest Hierarchies Learnt from Bookmarks”建立分层的user profile来对查询结果重新排序.<br>8)2005年DATAK期刊”Category ranking for personalzied search”在ODP分类基础上根据用户profile重新选择一个子图结构分类来personalize 查询结果<br>9)2006年WWW”Automatic Identification of User Interest For Personalized Search”利用用户历史点击记录构建user profile来对查询结果重新排序.<br><strong><font color="blue">补充:2006年SIGKDD”Mining Long-Term Search History to Improve Search Accuracy”挖掘用户长期的搜索历史提出统计的语言模型</font></strong><br>10)2007年WWW”A Large-scale Evaluation and Analysis of Personalized Search Strategies”评价Personalized Search对于传统搜索是否有提高,并且揭示了Click-Based方法优于profile-based方法<br>11)2007年CIS”Personalized Web Search Using User Profile”在Client-Side构建User Profile,然后对用户的Query根据Profile进行扩充使查询更具体,将Query提交给Search Engine.<br>12)2007WWW”Privacy-Enhancing Personalized Web Search”权衡用户隐私和个性化搜索的需要,将用户无结构的个人数据整合成有结构的User Profile<br>13)2007年SIGIR”Privacy Protection in Personalized Search”在个性化搜索时同时保护用户隐私,在Client-Side保护隐私比在Server-Client要好<br>14)2007年WI”Using Personalized Web Search for Enhancing Common Sense and Folksonomy Based Intelligent Search Systems”对于大众分类标签的这样的系统中,用户检索时对于其他用户标注的内容会检索到不相关的内容,本文利用搜索历史和兴趣分类来建立用户偏好<br>15)2008年CIKM”Matching Task Profiles and User Needs in Personalized Web Search”在Client-Side,同时结合用户之前的<strong><font color="green">历史搜索结果</font></strong>和<strong><font color="green>" 当前的session上下文<="" font=""></font></strong>对用户建立不同粒度的profile.同时结合past search results(search histories)和current session context就弥补了只根据用户长期兴趣造成的缺点.<br>16)2008年WWW”Personalized Search and Exploration with MyTag”一篇DEMO完成了这样一个系统,根据flickr,YouTube和del.icio.us多个系统构造用户profile,在用户查询时完成返回多个平台的个性化资源结果.<br>17)2008年SIGIR”The Impact of History Length on Personalized Search”一篇DEMO基于任务研究web search(Task-based即限制了查询的方向,限制了查询任务),研究了搜索历史的长度对personalized search的影响.<br>18)2009年”Cluster Based Personalized Search”着手了两方面:利用文本聚类方法来Personalized Search和新的evaluation准则<br>19)2009年WSDM”Discovering and Using Groups to Improve Personalized Search”由于利用收集的个人信息来User Profile,但是由于通常User的个人信息通常不足够来构建,所以该篇文章利用其他用户来辅助收集用户个人信息.通过Query的相似性groupize一类用户<br><strong><font color="blue">20)2009年TKDE期刊”Evaluating the Effectiveness of Personalized Web Search”测试了五种personalized search算法,提出了新的评价框架来测试是否personalized search对于不同的用户提出不同的queries在不同的搜索上下文中有没有用.(五种测试方法:2种Click-based和3种Topipcal-interest-based);并且,提出了现有的personalized search的缺点:大多数提出的算法都是运用到所有的用户和查询上(对于有些很明确的查询不需要应用personalized;personalization算法的有效性会根据不同的搜索上下文而不同;现有的论文中测试personalized search的算法是基于少量的参与者积累查询数据集,很少有在真是世界中数据集做测试)</font></strong><br><strong><font color="blue">21)2009年CIKM”Personalized Social Search Based on the User’s Social Network”利用用户的三种社会关系:家庭社会关系/相似社会关系/全部的社会关系来建立user profile</font></strong><br><strong><font color="blue">22)2009年CSE”Social Tagging in Query Expansion:a New Way for Personalized Web Search”对于社交网络和协标注系统利用协标注来对Query进行扩展</font></strong><br><strong><font color="blue">23)2009年Konwl Inf syst”Towards a graph-based user profile modeling for a session-based personalized search”提出了利用图结构来user profile</font></strong><br>24)2010年WWW”Anonymizing User Profiles for Personalized Web Search”关于user profile的隐私保护<br>25)2010年”Applying Taxonomic Knowledge and Semantic Collaborative Filtering to Personalized Search: a Bayesian Belief Network based Approach”对于利用查询关键词匹配得到结果的方法而言,有些结果与查询有关而却与查询的关键字术语没有能匹配的结果往往会漏掉.该篇文章为了找出具有权威性的文本,通过语义协同过滤,用贝叶斯信念网络来代表用户的偏好,查询和相关的文本.<br><strong><font color="green">26)2010年CIKM”CiteData:A new multi-faceted dataset for evaluating personalized search performance”现在的personalized search系统使用了用户各种各样的特征数据如:文本超链接/分类标签等,将各种分类方法和社会标注结合起来,随之有分类/PageRank/协同过滤等算法来处理personalized search,但是对于这些方法的评价一直没有合适的数据集,所以该篇文章提出新的评价方法,利用多种多方面的数据来评价personalized search方法的表现.</font></strong><br><strong><font color="blue">27)2010年AMT”Folksonomy-Based Ontological User Interest Profile Modeling and Its Application in Personalized Search”在大众分类系统中,利用用户标注的tags并运用领域本体论来构建用户兴趣偏好</font></strong><br>28)2010年<strong>ICDE</strong>“Personalized Web Search with Location Preferences”文中将用户偏好概念分为了内容概念和位置概念,本文不仅从搜索结果/点击率来构建内容上的兴趣偏好,还考虑了位置概念.<br>29)2010年WIC”Personalized Search based on a User-centered Recommender Engine”提出了将<strong>推荐系统</strong>和Personalized Search结合起来<br><strong><font color="blue">30)2011年CIKM”A Framework for Personalized and Collaborative Clustering of Search Results”根据search results利用Wiki预料来聚类和协同过滤的方法来优化个性化搜索结果.</font></strong><br>30)2011年WEBIST”A Multi-factor Tag-Based Personalized Search”提出了利用用户的tag activity(浏览过的网页和对网页分配的标签)来建立用户的偏好然后再重新排序搜索结果.<br>31)2011年IS的期刊”A personalized search using a  semantic distance measure in a graph-based ranking model”用图结构(映射到ODP上)来表示文本和user profiles,基于语义距离测量来重新对搜索结果排序.<br>32)2011年UMAP”Leveraging Collaborative Filtering to Tag-Based Personalized Search”利用协同过滤的方法通过其他相似用户计算用户的潜在兴趣偏好,通过相似物品来构建物品的潜在tags.<br><strong><font color="blue">33)2011年CSC”Modeling User’s Preference in Folksonomy for Personalized Search”在大众分类系统利用标签聚类来构建user profile</font></strong><br>34)2011年Canadian AI”Normal Distribution Re-Weighting for Personalized Web Search”根据term的频率建立向量构建profile,但是同时重新对vector建立权重.因为频率大小对profile的影响是不一样的,其中,比较注重的是Mid-frequency.<br>35)2011年WWW”Personalized Search on Flickr based on Searcher’s Preference Prediction”一篇DEMO,基于Flickr系统根据用户的朋友的兴趣偏好和聚类方法来预测该用户所要找的图片.(e.g.用户搜索”长城”,返回118147张照片结果,但是,他/她所需要或想要的是哪一张或那几张需要自己去从中挑选)<br><strong><font color="blue">36)2011年FSKD”User Profile for Personalized Web Search”利用三种机器学习方法(Rocchio/K-Nearest Neighbors/SVM)来构建user profiles</font></strong><br>37)2012年ICCCI”Construction of Semantic User Profile for Personalized Web Search”完成这样一个系统,让用户输入用户名和邮件地址后从网页抓取和邮件地址相关的信息来构建user profile(使用VSM)(依据ODP).<br>38)2012年APWeb”Context-Aware Personalized Search Based on User and Resource Profiles in Folksonomies”指出了之前运用于Folksonomy系统中建立VSM后TF-IDF和BM25方法的不合理之处.<br><strong><font color="blue">39)2012年Information Systems期刊”Folksonomy-based personalized search and ranking in social media services”同时利用面向用户的tags和面向items的tags构建模型,构建user-tag矩阵/user-item矩阵/tag-item矩阵;对于查询的term没有出现在标注中的情况也能够对结果重新排序</font></strong><br>40)2012年”Multilingual User Modeling for Personalized Reranking of Multilingual Web Search Results”用多语言来构建用户模型.<br>41)2012年ADMA”Personalized Diversity Search Based on User’s Social Relationships”针对搜索引擎由于不能领会用户潜在的意图和兴趣偏好,所以不能返回给用户精确/充足,且伴随有累赘的结果.现有的方法有返回多样性的结果来满足大部分用户,并且统一地运用到所有的用户和查询中,返回的结果通常返回的是大部分用户的需求,对于某个用户的具体需求并没有被考虑进去.本文将多样性搜索和个性化搜索结合来使搜索结果对于群体和某个用户来说更加精确.<br><strong><font color="blue">42)2012年SIGMOD”Taagle:Efficient,Personalized Search in Collaborative Tagging Networks”用户带有在社交网络中的权值,items带有用户的关键词标注,用户用某一个tags来搜索返回Top-k个结果</font></strong><br>43)2013年SIGMOD”Efficient Ad-hoc Search for Personalized PageRank”对PPR做了改进<br><strong><font color="blue">44)2013年WWW”Enhancing Personalized Search by Mining and Modeling Task Behavior”提出之前在Personalized Search中都是比较依赖和用户历史查询记录相关信息,对于新的查询可能会无所适从;本文提出了Task-based(基于URLs)的方法,通过在历史搜索日志中挖掘出提出过和当前用户任务相关的用户,利用他们的on-task行为来提升web pages的排序.并将算法和Query-based进行对比</font></strong><br><strong><font color="blue">45)2013年(LiQing)WI-IAT”Finding Dominating Set from Verbal Contextual Graph for Persoanlized Search in Folksonomy”对于去挖掘用户潜在的意图和兴趣偏好,基于上下文的信息是不可或缺的,在社会语言学中上下文中分为Verbal Context(queries历史/点击历史数据)和social context(mood/weather/time).通过对比了social context之后,作者选用了verbal context语言模型,verbal context模型用图结构构造,并将重要的节点区别出来.</font></strong><br><strong><font color="blue">46)2013年SIGKDD一篇DEMO”LAICOS:An Open Source Platform Personalized Social Web Search”1.利用了文本内容来建立social context2.和之前方法一样也用了对query进行扩展的方法.当用户提出一个query,系统会根据用户experience匹配query,同时,系统还会根据其他提出过相似查询的用户来返回相似的文档</font></strong><br><strong><font color="blue">47)2013年CIKM”Personalized Models of Search Satisfaction”这篇文章通过区分不同用户的搜索行为来建立用户的满意度,从而使个性化搜索更为准确.(依赖于点击数据)</font></strong><br>48)2013年SIGIR”Personalized Ranking Model Adaptation for Web Search”针对之前搜索引擎对所有的用户都运用单一的排序模型而提出了新的排序模型(通过一系列的线性转化,缩放或者转变)<br>49)2013年ICCCSA”Personalized Semantic Search Using ODP:A Sutdy Case in Academic Domain”将文本大致分类到相应的实体ODP来完成语义搜索<br>50)2013年SIGIR”SoPRa: A New Social Personalized Ranking Function for Improving Web Search”提出了新的搜索结果等级排序函数<br>51)201年SIGIR”Using Social Annotations to Enhance Document Representation for Personalized Search”基于用户查询过的网页,不仅仅基于该用户对其的标注,而且考虑其他用户的标注.因为如果只考虑该用户的标注存在两个问题:1)忽略了他没有标注的页面2)分配的等级分数不合理<br>52)2014年WSDM”Adapting Deep RankNet for Personalized Search”:RankNet被广泛地应用在web搜索任务中,但是很少有应用在Personalized Search中.本文利用5层深度神经网络来构造RankNet运用于Personalized Search中.<br>53)2014年KDD”Personalized Search Result Diversification via Structured Learning”利用有监督学习来解决搜索结果个性化多样性的问题,既保持结果的多样性,同时结合用户的兴趣偏好<br><strong><font color="blue">54)2015年”Adaptive and Multiple Interest-aware User Profiles for Personalized Search in Folksonomy:A Simple but Effective Graph-based Profiling Model”基于图结构利用社会标注的tags构造自适应的且融合多种用户兴趣偏好的user profiles.</font></strong><br>55)2015年WWW”An Optimization Framework for Weighting Implicit Relevance Labels for Personalized Web Search”提出了之前给web document分配权重的不合理之处,另外重新提出了personalized ranking算法.<br>56)2015年”Real Time Personalized Search on Social Networks”提出了在社交网络平台中两个特点1)频繁的内容更新2)小社区群体;而现有的搜索算法都还不能解决这样两个问题,本文提出了实时的personalized top-k查询(等级排序算法融合了时间/社会相关性/文本相似性).<strong>加入了时间因素</strong><br>57)2016年SIGKDD”How to Get Them a Dream Job”主要针对Job Personalized Search.<br>58)2016年SIGIR一篇DEMO”Learning to Rank Personalized Search Results in Professional Networks”在领英中提出新的结果等级排序算法.<br>59)2016年Neurocomputing”Personalized search for social media via dominating verbal context”<strong>Qing Li</strong>同之前篇<br>60)2016年Knowledge-Based Systems”Preference recmmendation for personalized search”综合之前的user profiles模型,本文指出使用比较广泛有一个CP-nets模型,不仅能简明地表达用户定性的兴趣偏好,而且很好地定义了用户偏好的范围.现在很多基于CP-nets的搜索系统都是假设用户之前已经定义好他们的兴趣偏好范围,但是在生活中这并不现实.本文的工作一方面利用不完全的CP-nets,另一方面利用协同过滤来弥补CP-nets的不足.还有一方面,本文提出偏好推荐模式来弥补CP-nets的不足.<br>61)2016年”Topic Model based Privacy Protection in Personalized Web Search”本文在保持搜索引擎个性化的同时,通过在用户查询日志中加入控制噪声来保护用户隐私.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Backgournd&quot;&gt;&lt;a href=&quot;#Backgournd&quot; class=&quot;headerlink&quot; title=&quot;Backgournd&quot;&gt;&lt;/a&gt;Backgournd&lt;/h3&gt;&lt;p&gt;搜索在20年前就已出现在互联网，而如今搜索已经无处不在。传统的搜索像这样，用
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Twitter用户数据Profiling</title>
    <link href="https://github.com/DuncanZhou/2018/08/10/TwitterUsersDataProfiling/"/>
    <id>https://github.com/DuncanZhou/2018/08/10/TwitterUsersDataProfiling/</id>
    <published>2018-08-10T09:24:49.361Z</published>
    <updated>2018-01-19T03:43:59.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><blockquote><p>数据摘要:One of the crucial requirements before comsuming datasets for any application is to understand the dataset at hand and its metadata.[1]<br>Data profiling is the set of activities and processes to determine the meta-data about a given dataset.[1]</p><p>总体地说,数据概要可以描述为是能够描述原样本数据的一个子集或者结果.比较简单地一种方式是计算平均值,总和或者统计频率最高的一些值等等方式.而较为有挑战性的是,在多列数据中找出其之间的相互函数或次序依赖等等关系.</p></blockquote><p>传统的数据摘要包括data exploration/data cleansing/data integration.而之后,data management和big data analytics也开始出现.</p><p>特别地,因为大数据的数据量大,多样性等特性,传统的技术对于其查询,存储及聚合都是花费高昂的.所以,data profiling在这里就显得非常重要.</p><blockquote><p>Data profiling is an important preparatory task to determine which data to mine, how to import data into various tools, and how to interpret the results.[1]</p></blockquote><p><img src="http://i1.buimg.com/1949/2fc64d8931d34670.png" alt="Data Profiling"></p><h4 id="Data-Profiling和Data-Mining的比较"><a href="#Data-Profiling和Data-Mining的比较" class="headerlink" title="Data Profiling和Data Mining的比较"></a>Data Profiling和Data Mining的比较</h4><p>1.Distinction by the object of analysis:<strong>Instance</strong> vs. <strong>schema</strong> or <strong>column</strong> vs. <strong>rows</strong><br>2.Distinction by the goal of the task:<strong>Description of existing data</strong> vs. <strong>new insights beyond existing data</strong> .</p><h3 id="2-动机或用例"><a href="#2-动机或用例" class="headerlink" title="2.动机或用例"></a>2.动机或用例</h3><blockquote><p>Data Profiling的目的:</p><ul><li>Data Exploration</li><li>Database management</li><li>Database reverse engineering</li><li>Data integration</li><li>Big data analytics</li></ul></blockquote><h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h3><p>1.依赖关系数据库,使用SQL语句查询返回结果(不能够找出所有属性列的依赖)<br>  单列和多列分析<br>2.搜索最优解:启发式算法<br>  启发式算法是一种技术,使得<strong>可接受的计算成本内</strong>去搜寻最好的解,但<strong>不一定能保证所得到的可行解和最优解</strong>,甚至在多数情况下,无法阐述所得解同最优解的近似程度.<br>3.聚类算法—&gt;筛选<br>4.按每一维动态规划找出子集</p><h3 id="4-twitter数据集人物特征选取"><a href="#4-twitter数据集人物特征选取" class="headerlink" title="4.twitter数据集人物特征选取"></a>4.twitter数据集人物特征选取</h3><ul><li>地理位置特征(反映了用户的时空分布,对于POI的推荐是有用的)</li><li>活跃度特征(可用于聚类分析)</li><li>影响力特征(可用于聚类分析)</li><li>推文特征(反映了用户的兴趣爱好,对于推荐系统是有用的)</li><li>时域特征</li></ul><h4 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h4><p>1.提取<br>2.正则化(最典型的就是数据的归一化处理,即将数据统一映射到[0,1]区间)</p><blockquote><p><strong>常见的数据归一化方法:</strong></p><ul><li>min-max,对原始数据的线性变换</li><li>log函数转换</li><li>atan函数转换</li><li>z-score标准化</li><li>Decimal scaling小数定标标准化</li><li>Logistic/Softmax变换</li><li>Softmax函数</li><li>模糊量化模式</li></ul></blockquote><p>特征选取原因:该特征代表了用户的…,对于…工作是有用的.</p><h3 id="5-twitter-data-profiling思路"><a href="#5-twitter-data-profiling思路" class="headerlink" title="5.twitter data profiling思路"></a><font color="red">5.twitter data profiling思路</font></h3><p><strong>Motivation</strong><br>聚类结果的代表性:</p><blockquote><p>Even though the construction of a cluster representation is an important step in decision making, it has not been examined closely by researchers.</p></blockquote><p><strong>度量准则:</strong><br><img src="https://ooo.0o0.ooo/2017/06/22/594bcb6c616ec.png" alt=""></p><p><strong>特征提取</strong><br>直接:location(时区),Followers/Following,category<br>间接:Activity,Influence,*InterestTags</p><p><strong>距离定义</strong><br>有序属性:闵可夫斯基距离(p=2时为欧式距离)<br>无序属性:VDM</p><p><strong>方法</strong></p><ul><li>1.聚类方法(LVQ)</li><li>2.定义图结构来搜索</li></ul><p><strong>Challenge-挑战</strong></p><ul><li>a.原集和profile子集的代表性度量准则的定义</li><li>b.ProfileSet的大小,k的确定</li><li>c.寻找ProfileSet(Representation of Clustering[2])</li><li>d.优化搜索算法</li></ul><h3 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5.参考文献"></a>5.参考文献</h3><p>1.Data Profiling-A Tutorial SIGMOD 2017<br>2.Data Clustering: A Review IEEE Computer Society</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1.概念&quot;&gt;&lt;/a&gt;1.概念&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;数据摘要:One of the crucial requirements before comsu
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
</feed>
