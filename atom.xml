<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Duncan&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://github.com/DuncanZhou/"/>
  <updated>2019-08-18T13:56:31.566Z</updated>
  <id>https://github.com/DuncanZhou/</id>
  
  <author>
    <name>duncan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FM &amp; FFM &amp; DeepFM</title>
    <link href="https://github.com/DuncanZhou/2019/08/18/FM/"/>
    <id>https://github.com/DuncanZhou/2019/08/18/FM/</id>
    <published>2019-08-18T06:17:40.416Z</published>
    <updated>2019-08-18T13:56:31.566Z</updated>
    
    <content type="html"><![CDATA[<h3 id="模型表示为：因子分解机（Factorization-Machine）"><a href="#模型表示为：因子分解机（Factorization-Machine）" class="headerlink" title="模型表示为：因子分解机（Factorization Machine）"></a>模型表示为：因子分解机（Factorization Machine）</h3><h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><p>在如广告点击预测问题中，根据用户画像、广告位以及一些其他特征来预测用户是否会点击广告。当对离散特征进行One-hot编码后，将出现特征维度爆炸，而且特征数据较稀疏。因此，FM最大的特点是对于稀疏的数据具有很好的学习能力。</p><hr><p>可以处理以下三类问题：</p><ul><li><strong>回归问题</strong>：使用最小均方误差作为优化标准</li><li><strong>二分类问题</strong>：加一个激活函数，如sigmoid或tanh函数等</li><li><strong>排序问题</strong>：按照预测分数召回</li></ul><h3 id="2-优点"><a href="#2-优点" class="headerlink" title="2.优点"></a>2.优点</h3><ul><li>可以<strong>在非常稀疏的数据</strong>中进行合理的参数估计</li><li>在FM模型的<strong>复杂度是线性</strong>的</li><li>FM是一个<strong>通用模型</strong>，可以应用于任何特征为实值的情况</li></ul><h3 id="3-为什么有效？模型细节"><a href="#3-为什么有效？模型细节" class="headerlink" title="3.为什么有效？模型细节"></a>3.为什么有效？模型细节</h3><ul><li><p>在一般的线性模型中，各个特征独立考虑的，没有考虑到特征与特征之间的相互关系。但实际上，<strong>大量的特征之间是有关联</strong>的。 </p><blockquote><p>举例：电商中，男性购买啤酒较多，女性购买化妆品较多，性别与购买类别之间存在关联。</p></blockquote></li><li><p>模型</p><ul><li><p>一般的线性模型为</p><script type="math/tex; mode=display">y=w_0+\sum_{i=1}^{n}w_ix_i</script></li><li><p>对于度为2的因子分解机模型为：</p><script type="math/tex; mode=display">y=w_0+\sum_{i=1}^{n}w_ix_i+\sum_{i=1}^{n-1}\sum_{j=i+1}^{n}<v_i,v_j>x_ix_j</script><p>，其中，$<v_i,v_j>$表示两个大小为$k$的向量之间的点积。与线性模型相比，FM的模型多了后面特征组合的部分。</v_i,v_j></p></li></ul></li><li><p>如何求解？</p><p>对每一个特征分量$x<em>i$构造一个辅助向量$v_i=(v</em>{i1},v<em>{i2},…,v</em>{i<em>k})$，利用$v_iv_j^T$对交叉项的$w</em>{ij}$进行估计。</p></li><li><p>K的选取？</p><p>k越大能够解释的特征维度越高，但是k的选取不宜太大。</p></li><li><p>为什么求解模型复杂度是线性的？</p><p><img src="/DuncanZhou/2019/08/18/FM/C:/Users\DUNCAN~1\AppData\Local\Temp\1566111191953.png" alt="1566111191953"></p></li><li><p>求解过程</p><p>使用随机梯度下降方式求解</p></li></ul><hr><h3 id="局部感知因子分解机（FFM）"><a href="#局部感知因子分解机（FFM）" class="headerlink" title="局部感知因子分解机（FFM）"></a>局部感知因子分解机（FFM）</h3><h3 id="1-基于FM改进之处"><a href="#1-基于FM改进之处" class="headerlink" title="1.基于FM改进之处"></a>1.基于FM改进之处</h3><p>特征One-hot之后过于稀疏，因此，同一个categorical特征经过One-hot编码之后生成的数值特征可以放到同一个field。</p><p>因此在FFM中，每一维特征都会针对其他特征的每个field，分别学习一个隐变量，该隐变量不仅与特征相关，也与field相关。假设样本的$n$个特征属于$f$个field，那么FFM的二次项有$nf$个隐向量。而在FM模型中每一维特征的隐向量只有一个。如果隐向量的长度为$k$，那么FFM的二次项参数有$nfk$个，远多于FM的$nk$个。</p><h3 id="2-模型"><a href="#2-模型" class="headerlink" title="2.模型"></a>2.模型</h3><script type="math/tex; mode=display">y=w_0+\sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n<v_{i,fj},v_{j,fi}>x_ix_j</script><h3 id="3-求解"><a href="#3-求解" class="headerlink" title="3.求解"></a>3.求解</h3><p>随机梯度下降，同FM</p><h3 id="4-应用"><a href="#4-应用" class="headerlink" title="4.应用"></a>4.应用</h3><p>为了使用FFM方法，所有的特征必须转换成“field_id:feat_id:value”格式，field_id代表特征所属field的编号，feat_id是特征编号，value是特征的值。 </p><hr><h3 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h3><h3 id="1-概念-1"><a href="#1-概念-1" class="headerlink" title="1.概念"></a>1.概念</h3><p>DeepFM目的是同时学习<strong>低阶和高阶</strong>的特征交叉，主要由FM和DNN两部分组成，底部共享同样的输入。模型可以表示为：</p><script type="math/tex; mode=display">y=sigmoid(y_{FM}+y_{DNN})</script><p>这里的低阶和高阶指的是特征组合的维度，虽然FM理论上可以对高阶特征组合进行建模，但实际上因为计算复杂度原因，一般都只用到了二阶特征组合。因此，FM负责二阶特征组合，DNN负责高阶特征的组合。</p><h3 id="2-优势"><a href="#2-优势" class="headerlink" title="2.优势"></a>2.优势</h3><ul><li>同时结合高阶和低阶特征组合（FM+DNN）</li><li>端到端模型，无需特征工程（DNN）</li><li>共享相同的输入和embedding参数，训练高效（借助FFM来做预训练，得到embedding后的向量）</li><li>评估模型时，用到了新的指标“Gini Normalization”</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;模型表示为：因子分解机（Factorization-Machine）&quot;&gt;&lt;a href=&quot;#模型表示为：因子分解机（Factorization-Machine）&quot; class=&quot;headerlink&quot; title=&quot;模型表示为：因子分解机（Factorizatio
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//categories/MachineLearning/"/>
    
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>ubuntu下sublime中文输入问题</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/ubuntu-sublime/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/ubuntu-sublime/</id>
    <published>2019-08-17T08:14:59.465Z</published>
    <updated>2018-01-19T03:44:06.575Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ubuntu下安装的sublime-text中文不能输入问题："><a href="#ubuntu下安装的sublime-text中文不能输入问题：" class="headerlink" title="ubuntu下安装的sublime text中文不能输入问题："></a>ubuntu下安装的sublime text中文不能输入问题：</h1><h2 id="a-保存下面的代码到文件sublime-imfix-c-位于-目录"><a href="#a-保存下面的代码到文件sublime-imfix-c-位于-目录" class="headerlink" title="a.保存下面的代码到文件sublime_imfix.c(位于~目录)"></a>a.保存下面的代码到文件sublime_imfix.c(位于~目录)</h2><p>#include”gtk/gtkimcontext.h”<br>void gtk_im_context_set_client_window (GtkIMContext <em>context, GdkWindow  </em>window)<br>{<br>GtkIMContextClass *klass;<br>g_return_if_fail (GTK_IS_IM_CONTEXT (context));<br>klass = GTK_IM_CONTEXT_GET_CLASS (context);<br>if (klass-&gt;set_client_window)<br>klass-&gt;set_client_window (context, window);<br>g_object_set_data(G_OBJECT(context),”window”,window);<br>if(!GDK_IS_WINDOW (window))<br>return;<br>int width = gdk_window_get_width(window);<br>int height = gdk_window_get_height(window);<br>if(width != 0 &amp;&amp; height !=0)<br>gtk_im_context_focus_in(context);<br>}</p><h2 id="b-将上一步的代码编译成共享库libsublime-imfix-so，命令"><a href="#b-将上一步的代码编译成共享库libsublime-imfix-so，命令" class="headerlink" title="b.将上一步的代码编译成共享库libsublime-imfix.so，命令"></a>b.将上一步的代码编译成共享库libsublime-imfix.so，命令</h2><p>gcc -shared -o libsublime-imfix.so sublime_imfix.c  <code>pkg-config --libs --cflags gtk+-2.0</code> -fPIC<br>    注意：如果提示 gtk/gtkimcontext.h：没有那个文件或目录，那就是没有相关的依赖软件，安装命令：</p><p>sudo apt-get install build-essential libgtk2.0-dev</p><h2 id="c-将libsublime-imfix-so拷贝到sublime-text所在文件夹"><a href="#c-将libsublime-imfix-so拷贝到sublime-text所在文件夹" class="headerlink" title="c.将libsublime-imfix.so拷贝到sublime_text所在文件夹"></a>c.将libsublime-imfix.so拷贝到sublime_text所在文件夹</h2><p>sudo mv libsublime-imfix.so /opt/sublime_text/</p><h2 id="d-修改文件-usr-bin-subl的内容"><a href="#d-修改文件-usr-bin-subl的内容" class="headerlink" title="d.修改文件/usr/bin/subl的内容"></a>d.修改文件/usr/bin/subl的内容</h2><p>sudo gedit /usr/bin/subl<br>将</p><p>#!/bin/sh</p><p>exec /opt/sublime_text/sublime_text “$@”</p><p>修改为</p><p>#!/bin/sh</p><p>LD_PRELOAD=/opt/sublime_text/libsublime-imfix.so exec /opt/sublime_text/sublime_text “$@”</p><p>原文链接：<a href="http://www.jianshu.com/p/1f3a3e4f4e92" target="_blank" rel="external">http://www.jianshu.com/p/1f3a3e4f4e92</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;ubuntu下安装的sublime-text中文不能输入问题：&quot;&gt;&lt;a href=&quot;#ubuntu下安装的sublime-text中文不能输入问题：&quot; class=&quot;headerlink&quot; title=&quot;ubuntu下安装的sublime text中文不能输入问题
      
    
    </summary>
    
      <category term="Note" scheme="https://github.com/DuncanZhou//categories/Note/"/>
    
    
  </entry>
  
  <entry>
    <title>Twitter用户数据Profiling</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/TwitterUsersDataProfiling/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/TwitterUsersDataProfiling/</id>
    <published>2019-08-17T08:14:59.458Z</published>
    <updated>2018-01-19T03:43:58.286Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概念"><a href="#1-概念" class="headerlink" title="1.概念"></a>1.概念</h3><blockquote><p>数据摘要:One of the crucial requirements before comsuming datasets for any application is to understand the dataset at hand and its metadata.[1]<br>Data profiling is the set of activities and processes to determine the meta-data about a given dataset.[1]</p><p>总体地说,数据概要可以描述为是能够描述原样本数据的一个子集或者结果.比较简单地一种方式是计算平均值,总和或者统计频率最高的一些值等等方式.而较为有挑战性的是,在多列数据中找出其之间的相互函数或次序依赖等等关系.</p></blockquote><p>传统的数据摘要包括data exploration/data cleansing/data integration.而之后,data management和big data analytics也开始出现.</p><p>特别地,因为大数据的数据量大,多样性等特性,传统的技术对于其查询,存储及聚合都是花费高昂的.所以,data profiling在这里就显得非常重要.</p><blockquote><p>Data profiling is an important preparatory task to determine which data to mine, how to import data into various tools, and how to interpret the results.[1]</p></blockquote><p><img src="http://i1.buimg.com/1949/2fc64d8931d34670.png" alt="Data Profiling"></p><h4 id="Data-Profiling和Data-Mining的比较"><a href="#Data-Profiling和Data-Mining的比较" class="headerlink" title="Data Profiling和Data Mining的比较"></a>Data Profiling和Data Mining的比较</h4><p>1.Distinction by the object of analysis:<strong>Instance</strong> vs. <strong>schema</strong> or <strong>column</strong> vs. <strong>rows</strong><br>2.Distinction by the goal of the task:<strong>Description of existing data</strong> vs. <strong>new insights beyond existing data</strong> .</p><h3 id="2-动机或用例"><a href="#2-动机或用例" class="headerlink" title="2.动机或用例"></a>2.动机或用例</h3><blockquote><p>Data Profiling的目的:</p><ul><li>Data Exploration</li><li>Database management</li><li>Database reverse engineering</li><li>Data integration</li><li>Big data analytics</li></ul></blockquote><h3 id="3-方法"><a href="#3-方法" class="headerlink" title="3.方法"></a>3.方法</h3><p>1.依赖关系数据库,使用SQL语句查询返回结果(不能够找出所有属性列的依赖)<br>  单列和多列分析<br>2.搜索最优解:启发式算法<br>  启发式算法是一种技术,使得<strong>可接受的计算成本内</strong>去搜寻最好的解,但<strong>不一定能保证所得到的可行解和最优解</strong>,甚至在多数情况下,无法阐述所得解同最优解的近似程度.<br>3.聚类算法—&gt;筛选<br>4.按每一维动态规划找出子集</p><h3 id="4-twitter数据集人物特征选取"><a href="#4-twitter数据集人物特征选取" class="headerlink" title="4.twitter数据集人物特征选取"></a>4.twitter数据集人物特征选取</h3><ul><li>地理位置特征(反映了用户的时空分布,对于POI的推荐是有用的)</li><li>活跃度特征(可用于聚类分析)</li><li>影响力特征(可用于聚类分析)</li><li>推文特征(反映了用户的兴趣爱好,对于推荐系统是有用的)</li><li>时域特征</li></ul><h4 id="特征处理"><a href="#特征处理" class="headerlink" title="特征处理"></a>特征处理</h4><p>1.提取<br>2.正则化(最典型的就是数据的归一化处理,即将数据统一映射到[0,1]区间)</p><blockquote><p><strong>常见的数据归一化方法:</strong></p><ul><li>min-max,对原始数据的线性变换</li><li>log函数转换</li><li>atan函数转换</li><li>z-score标准化</li><li>Decimal scaling小数定标标准化</li><li>Logistic/Softmax变换</li><li>Softmax函数</li><li>模糊量化模式</li></ul></blockquote><p>特征选取原因:该特征代表了用户的…,对于…工作是有用的.</p><h3 id="5-twitter-data-profiling思路"><a href="#5-twitter-data-profiling思路" class="headerlink" title="5.twitter data profiling思路"></a><font color="red">5.twitter data profiling思路</font></h3><p><strong>Motivation</strong><br>聚类结果的代表性:</p><blockquote><p>Even though the construction of a cluster representation is an important step in decision making, it has not been examined closely by researchers.</p></blockquote><p><strong>度量准则:</strong><br><img src="https://ooo.0o0.ooo/2017/06/22/594bcb6c616ec.png" alt=""></p><p><strong>特征提取</strong><br>直接:location(时区),Followers/Following,category<br>间接:Activity,Influence,*InterestTags</p><p><strong>距离定义</strong><br>有序属性:闵可夫斯基距离(p=2时为欧式距离)<br>无序属性:VDM</p><p><strong>方法</strong></p><ul><li>1.聚类方法(LVQ)</li><li>2.定义图结构来搜索</li></ul><p><strong>Challenge-挑战</strong></p><ul><li>a.原集和profile子集的代表性度量准则的定义</li><li>b.ProfileSet的大小,k的确定</li><li>c.寻找ProfileSet(Representation of Clustering[2])</li><li>d.优化搜索算法</li></ul><h3 id="5-参考文献"><a href="#5-参考文献" class="headerlink" title="5.参考文献"></a>5.参考文献</h3><p>1.Data Profiling-A Tutorial SIGMOD 2017<br>2.Data Clustering: A Review IEEE Computer Society</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-概念&quot;&gt;&lt;a href=&quot;#1-概念&quot; class=&quot;headerlink&quot; title=&quot;1.概念&quot;&gt;&lt;/a&gt;1.概念&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;数据摘要:One of the crucial requirements before comsu
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>同步到腾讯云</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/tencentcloud/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/tencentcloud/</id>
    <published>2019-08-17T08:14:59.453Z</published>
    <updated>2018-08-22T09:08:30.844Z</updated>
    
    <content type="html"><![CDATA[<p>我的博客即将搬运同步至腾讯云+社区，邀请大家一同入驻：<a href="https://cloud.tencent.com/developer/support-plan?invite_code=cibtnefnj6my" target="_blank" rel="external">https://cloud.tencent.com/developer/support-plan?invite_code=cibtnefnj6my</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我的博客即将搬运同步至腾讯云+社区，邀请大家一同入驻：&lt;a href=&quot;https://cloud.tencent.com/developer/support-plan?invite_code=cibtnefnj6my&quot; target=&quot;_blank&quot; rel=&quot;exter
      
    
    </summary>
    
      <category term="Life" scheme="https://github.com/DuncanZhou//categories/Life/"/>
    
    
  </entry>
  
  <entry>
    <title>支持向量机(Support Vector Machine)学习</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/SVM-Support%20vector%20Machine/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/SVM-Support vector Machine/</id>
    <published>2019-08-17T08:14:59.447Z</published>
    <updated>2018-01-19T03:43:50.675Z</updated>
    
    <content type="html"><![CDATA[<h1 id="支持向量机-SVM-Support-Vector-Machine-："><a href="#支持向量机-SVM-Support-Vector-Machine-：" class="headerlink" title="支持向量机(SVM-Support Vector Machine)："></a>支持向量机(SVM-Support Vector Machine)：</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>1.SVM是一种分类算法，是一种<em>二类分类模型</em>,用于解决分类和回归问题。通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。</p><blockquote><p><em>i.e.</em><font color="red">给定一个包含正例和反例的样本集合，svm的目的是寻找一个超平面来对样本进行分割，把样本中的正例面和反例面分开，但不是简单的分开，原则是使正例和反例之间的间隔最大，鲁棒性最好。</font></p></blockquote><p>2.<em>基本公式</em>：在样本空间中，划分超平面的线性方程：<img src="https://github.com/DuncanZhou/images/raw/master/1.PNG" alt="线性方程"><br>样本空间中任意点x到超平面（w，b）距离为<img src="https://github.com/DuncanZhou/images/raw/master/2.PNG" alt="距离"><br>假设正确分类，<img src="https://github.com/DuncanZhou/images/raw/master/3.PNG" alt="">“间隔”为<img src="https://github.com/DuncanZhou/images/raw/master/4.PNG" alt=""><br>所以，现在的目标是求得<em>“最大间隔”</em><img src="https://github.com/DuncanZhou/images/raw/master/5.PNG" alt=""><br>这就是SVM的基本型。</p><p>3.求“最大间隔”过程中的问题转化（转换成对偶问题）</p><blockquote><p>最大 -&gt; 最小 -&gt; 凸二次规划|<font color="red">拉格朗日乘子法</font></p></blockquote><h2 id="线性划分-gt-非线性划分"><a href="#线性划分-gt-非线性划分" class="headerlink" title="线性划分 -&gt; 非线性划分"></a>线性划分 -&gt; 非线性划分</h2><h3 id="1-问题"><a href="#1-问题" class="headerlink" title="1.问题"></a>1.问题</h3><p><img src="https://github.com/DuncanZhou/images/raw/master/6.PNG" alt=""><br>之前的讨论是假设样本是线性可分的，然而现实生活任务中，原始样本空间也许并不存在一个能正确划分两类样本的超平面。(如“异或问题”)，<font color="red">对于这样的问题，可以将原始样本空间映射到一个更高维的特征空间。</font>（Fortunately,如果原始空间是有限集，那么一定存在一个高维特征空间是样本可分。）</p><h3 id="2-解决方案"><a href="#2-解决方案" class="headerlink" title="2.解决方案"></a>2.解决方案</h3><p>映射后求解“最大间隔”的解<br><img src="https://github.com/DuncanZhou/images/raw/master/7.PNG" alt=""></p><h3 id="3-涉及到的问题"><a href="#3-涉及到的问题" class="headerlink" title="3.涉及到的问题"></a>3.涉及到的问题</h3><p>在求解过程中涉及计算样本Xi与Xi映射到特征空间之后的内积。由于特征空间维数可能很高，甚至可能是无穷维，因此直接计算内积通常是困难的，为了避开这个问题，设想这样一个函数-核函数<img src="https://github.com/DuncanZhou/images/raw/master/8.PNG" alt=""><br>求解后得到<img src="https://github.com/DuncanZhou/images/raw/master/9.PNG" alt=""></p><h3 id="4-常用的核函数"><a href="#4-常用的核函数" class="headerlink" title="4.常用的核函数"></a>4.常用的核函数</h3><p><img src="https://github.com/DuncanZhou/images/raw/master/10.PNG" alt=""></p><h2 id="软间隔与正则化"><a href="#软间隔与正则化" class="headerlink" title="软间隔与正则化"></a>软间隔与正则化</h2><blockquote><p><em>软间隔</em>：现实任务中往往很难确定合适的核函数使训练集在特征空间中线性可分，即使恰好找到了某个核函数使训练集在特征空间中线性可分，也很难判定这个貌似线性可分的结果不是由于过拟合所造成的。</p></blockquote><p><em>解决该问题的一个方法是允许svm在一些样本上出错</em>。如<img src="https://github.com/DuncanZhou/images/raw/master/11.PNG" alt=""></p><p>也就是在求解最大化间隔时，同时使不满足约束的样本尽可能少。<img src="https://github.com/DuncanZhou/images/raw/master/15.PNG" alt=""></p><p>三种常用的替代损失函数：<img src="https://github.com/DuncanZhou/images/raw/master/12.PNG" alt=""></p><p>共性：<img src="https://github.com/DuncanZhou/images/raw/master/13.PNG" alt=""></p><h2 id="支持向量回归（Support-Vector-Regression）"><a href="#支持向量回归（Support-Vector-Regression）" class="headerlink" title="支持向量回归（Support Vector Regression）"></a>支持向量回归（Support Vector Regression）</h2><p>给定样本D={(x1,y1),(x2,y2),…},希望学得一个回归模型，使得f(x)与y尽可能接近，w和b是待确定参数。<br>传统回归模型通常直接基于模型输出f(x)与真实输出y之间的差别来计算损失，当且仅当f(x)与y完全相同时，损失才为0.与次不同，SVR假设我们能容忍f(x)与y之间最多有e的偏差，小于等于e的都算0误差。SVR问题形式化为<img src="https://github.com/DuncanZhou/images/raw/master/14.PNG" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;支持向量机-SVM-Support-Vector-Machine-：&quot;&gt;&lt;a href=&quot;#支持向量机-SVM-Support-Vector-Machine-：&quot; class=&quot;headerlink&quot; title=&quot;支持向量机(SVM-Support Vector
      
    
    </summary>
    
      <category term="Seminar" scheme="https://github.com/DuncanZhou//categories/Seminar/"/>
    
    
  </entry>
  
  <entry>
    <title>支持向量机(Support Vector Machine)学习（补充）</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/SVM(Extension)/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/SVM(Extension)/</id>
    <published>2019-08-17T08:14:59.441Z</published>
    <updated>2018-01-19T03:43:42.063Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SMO算法-Sequential-Minimal-Optimization"><a href="#SMO算法-Sequential-Minimal-Optimization" class="headerlink" title="SMO算法(Sequential Minimal Optimization)"></a>SMO算法(Sequential Minimal Optimization)</h1><h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><blockquote><p>SMO算法用于训练SVM，将大优化问题分解为多个小优化问题。这些小优化问题往往很容易求解，并且对它们进行顺序求解的结构与将它们作为整体来求解的结果是完全一致。</p></blockquote><h2 id="2-目标及原理"><a href="#2-目标及原理" class="headerlink" title="2.目标及原理"></a>2.目标及原理</h2><blockquote><p>SMO算法的工作目标是求出一系列alpha和b,一旦求出了这些alpha，就能求出权重向量w。</p><p>每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减少另一个。这里所谓的“合适”就是指两个alpha必须要符合一定的条件，条件之一就是这两个alpha必须在间隔边界之外，而其第二个条件则是这两个alpha还没有进行过区间化处理或者不在边界上。</p></blockquote><h2 id="3-调参"><a href="#3-调参" class="headerlink" title="3.调参"></a>3.调参</h2><blockquote><p>SVM中有两个参数<font color="red">C</font>和<font color="red">K1</font>，其中C是惩罚系数，即对误差的宽容度。C越高，说明越不能容忍出误差，容易过拟合。C越小，容易欠拟合。</p><p>k1是参数是RBF函数作为核函数后，该函数自带的一个参数，隐含的决定了数据映射到新的特征空间后的分布，k1越大，支持向量越少，k1越小，支持向量越多。支持向量的个数影响训练与预测的速度。</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SMO算法-Sequential-Minimal-Optimization&quot;&gt;&lt;a href=&quot;#SMO算法-Sequential-Minimal-Optimization&quot; class=&quot;headerlink&quot; title=&quot;SMO算法(Sequential M
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
  </entry>
  
  <entry>
    <title>超参的搜索方法整理</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/SuperParas/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/SuperParas/</id>
    <published>2019-08-17T08:14:59.436Z</published>
    <updated>2018-08-20T03:03:04.691Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-网格搜索"><a href="#1-网格搜索" class="headerlink" title="1.网格搜索"></a>1.网格搜索</h3><p>网格搜索通过查找搜索范围内的所有的点，来确定最优值。它返回目标函数的最大值或损失函数的最小值。给出较大的搜索范围，以及较小的步长，网格搜索是一定可以找到全局最大值或最小值的。 </p><p>当人们实际使用网格搜索来找到最佳超参数集的时候，一般会先使用较广的搜索范围，以及较大的步长，来找到全局最大值或者最小值可能的位置。然后，人们会缩小搜索范围和步长，来达到更精确的最值。 </p><h3 id="2-随机搜索"><a href="#2-随机搜索" class="headerlink" title="2.随机搜索"></a>2.随机搜索</h3><p>随机搜索的思想和网格搜索比较相似，只是不再测试上界和下界之间的所有值，只是在搜索范围中随机取样本点。它的理论依据是，如果随即样本点集足够大，那么也可以找到全局的最大或最小值，或它们的近似值。</p><p>通过对搜索范围的随机取样，随机搜索一般会比网格搜索要快一些。但是和网格搜索的快速版（非自动版）相似，结果也是没法保证的。 </p><h3 id="3-基于梯度的优化"><a href="#3-基于梯度的优化" class="headerlink" title="3.基于梯度的优化"></a>3.基于梯度的优化</h3><h3 id="4-贝叶斯优化"><a href="#4-贝叶斯优化" class="headerlink" title="4.贝叶斯优化"></a>4.贝叶斯优化</h3><p>贝叶斯优化寻找使全局达到最值的参数时，使用了和网格搜索、随机搜索完全不同的方法。网格搜索和随机搜索在测试一个新的点时，会忽略前一个点的信息。而贝叶斯优化充分利用了这个信息。贝叶斯优化的工作方式是通过对目标函数形状的学习，找到使结果向全局最大值提升的参数。它学习目标函数形状的方法是，根据先验分布，假设一个搜集函数。在每一次使用新的采样点来测试目标函数时，它使用这个信息来更新目标函数的先验分布。然后，算法测试由后验分布给出的，全局最值最可能出现的位置的点。 </p><p>补充:</p><p><img src="https://raw.githubusercontent.com/DuncanZhou/images/master/alipsipng.png" alt="PSI"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-网格搜索&quot;&gt;&lt;a href=&quot;#1-网格搜索&quot; class=&quot;headerlink&quot; title=&quot;1.网格搜索&quot;&gt;&lt;/a&gt;1.网格搜索&lt;/h3&gt;&lt;p&gt;网格搜索通过查找搜索范围内的所有的点，来确定最优值。它返回目标函数的最大值或损失函数的最小值。给出较大的搜索
      
    
    </summary>
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//categories/MachineLearning/"/>
    
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>Hive SQL 学习</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/SQL_Learning/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/SQL_Learning/</id>
    <published>2019-08-17T08:14:59.419Z</published>
    <updated>2018-08-17T07:20:10.019Z</updated>
    
    <content type="html"><![CDATA[<h3 id="partition-by"><a href="#partition-by" class="headerlink" title="partition by"></a>partition by</h3><blockquote><p>partition by关键字是分析性函数的一部分，它和聚合函数不同的地方在于它能返回一个分组中的多条记录，而聚合函数一般只有一条反映统计值的记录，partition by用于给结果集分组，如果没有指定那么它把整个结果集作为一个分组</p></blockquote><p>example: 一个班有学生id，成绩，班级，现在将学生根据班级按照成绩排名。(partition by)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">select *,row_number() over(partition by Grade order by Score desc) as Sequence from Student</div></pre></td></tr></table></figure><h3 id="lateral-view"><a href="#lateral-view" class="headerlink" title="lateral view"></a>lateral view</h3><h3 id="explode-posexplode"><a href="#explode-posexplode" class="headerlink" title="explode / posexplode"></a>explode / posexplode</h3><blockquote><p>explode 拆分一行称多行，而posexplode是根据多行匹配行号进行拆分多行。</p></blockquote><h3 id="窗口函数"><a href="#窗口函数" class="headerlink" title="窗口函数"></a>窗口函数</h3><h4 id="a-first-value"><a href="#a-first-value" class="headerlink" title="a. first_value"></a>a. first_value</h4><p>​    取分组内排序后，截止到当前行，第一个值</p><h4 id="b-last-value"><a href="#b-last-value" class="headerlink" title="b.last_value"></a>b.last_value</h4><p>​    取分组内排序后，截止到当前行，最后一个值  </p><h4 id="c-lead-col-n-default"><a href="#c-lead-col-n-default" class="headerlink" title="c.lead(col,n,default)"></a>c.lead(col,n,default)</h4><p>​    用于统计窗口内往下第n行值。第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL） </p><h4 id="d-lag-col-n-default"><a href="#d-lag-col-n-default" class="headerlink" title="d.lag(col,n,default)"></a>d.lag(col,n,default)</h4><p>​    与lead相反，用于统计窗口内往上第n行值。第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL） </p><h4 id="c-聚集函数-over-partition-by-col1-order-by-col-rows-range-between-UNBOUNDED-num-preceding-and-num-FOLLOWING-current-ROW"><a href="#c-聚集函数-over-partition-by-col1-order-by-col-rows-range-between-UNBOUNDED-num-preceding-and-num-FOLLOWING-current-ROW" class="headerlink" title="c.聚集函数 + over + (partition by col1 [order by col (rows | range) between (UNBOUNDED | [num]) preceding and (num FOLLOWING | current ROW))"></a>c.聚集函数 + over + (partition by col1 [order by col (rows | range) between (UNBOUNDED | [num]) preceding and (num FOLLOWING | current ROW))</h4><h4 id="d-ROW-NUMBER"><a href="#d-ROW-NUMBER" class="headerlink" title="d.ROW_NUMBER()"></a>d.ROW_NUMBER()</h4><p>​    从1开始，按照顺序，生成分组内记录的序列 </p><h4 id="e-RANK"><a href="#e-RANK" class="headerlink" title="e.RANK()"></a>e.RANK()</h4><p>​    生成数据项在分组中的排名，排名相等会在名次中留下空位 </p><h4 id="f-DENSE-RANK"><a href="#f-DENSE-RANK" class="headerlink" title="f.DENSE_RANK()"></a>f.DENSE_RANK()</h4><p>​    生成数据项在分组中的排名，排名相等会在名次中不会留下空位  </p><h4 id="g-CUME-DIST"><a href="#g-CUME-DIST" class="headerlink" title="g.CUME_DIST()"></a>g.CUME_DIST()</h4><p>​    小于等于当前值的行数/分组内总行数 </p><h4 id="h-PERCENT-RANK"><a href="#h-PERCENT-RANK" class="headerlink" title="h.PERCENT_RANK ()"></a>h.PERCENT_RANK ()</h4><p>​    分组内当前行的RANK值-1/分组内总行数-1  </p><h4 id="i-NTILE-n"><a href="#i-NTILE-n" class="headerlink" title="i.NTILE(n)"></a>i.NTILE(n)</h4><p>​    用于将分组数据按照顺序切分成n片，返回当前切片值，如果切片不均匀，默认增加第一个切片的分布 </p><p>Note:</p><ul><li>From子句：执行顺序自上而下，从左到右，从后往前，所以<strong>数据量少的表尽量放后</strong></li><li>where子句：执行顺序自下而上，从右到左，<strong>可以过滤掉大量记录的条件写在where子句的末尾</strong></li><li>group by子句：通过将不需要的记录在group by之前过滤掉，<strong>避免使用having来过滤</strong></li><li>having子句：尽量少用</li><li>select子句：尽量少用*，取字段名称</li><li>order by子句：执行顺序为从左到右排序</li><li>join：<strong>尽量把数据量大的表放在最右边来进行关联</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;partition-by&quot;&gt;&lt;a href=&quot;#partition-by&quot; class=&quot;headerlink&quot; title=&quot;partition by&quot;&gt;&lt;/a&gt;partition by&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;partition by关键字是分
      
    
    </summary>
    
      <category term="SQL" scheme="https://github.com/DuncanZhou//categories/SQL/"/>
    
    
      <category term="SQL" scheme="https://github.com/DuncanZhou//tags/SQL/"/>
    
  </entry>
  
  <entry>
    <title>pyspark记录</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/SparkDataFrameLearning/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/SparkDataFrameLearning/</id>
    <published>2019-08-17T08:14:59.393Z</published>
    <updated>2018-08-10T09:58:26.702Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Spark-DataFrame学习"><a href="#Spark-DataFrame学习" class="headerlink" title="Spark DataFrame学习"></a>Spark DataFrame学习</h2><h3 id="1-文件的读取"><a href="#1-文件的读取" class="headerlink" title="1. 文件的读取"></a>1. 文件的读取</h3><p>1.1 spark.read.json() / spark.read.parquet() 或者 spark.read.load(path,format=”parquet/json”)</p><p>1.2 和数据库的交互 spark.sql(“”)</p><h3 id="2-函数使用"><a href="#2-函数使用" class="headerlink" title="2.函数使用"></a>2.函数使用</h3><ul><li><p>2.1 printSchema() - 显示表结构</p></li><li><p>2.2 df.select(col) - 查找某一列的值</p></li><li><p>2.3 df.show([int n])  - 显示[某几行的]的值</p></li><li><p>2.4 df.filter(condition) - 过滤出符合条件的行</p></li><li><p>2.5 df.groupby(col).count() </p><p>df.groupby(col).agg(col,func.min(),func.max(),func.sum()) - 聚合函数</p></li><li><p>2.6 spark.createDataFrame([(),(),(),()…,()],(col1,col2,col3,…,coln))</p></li><li><p>2.7 自定义udf函数</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="meta">@pandas_udf("col1 type,col2 type,...,coln type",PandasUDFType.GROUPD_MAP)</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(pdf)</span>:</span></div><div class="line"><span class="keyword">pass</span></div></pre></td></tr></table></figure><p>df.groupby(col).apply(f).show()</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Spark-DataFrame学习&quot;&gt;&lt;a href=&quot;#Spark-DataFrame学习&quot; class=&quot;headerlink&quot; title=&quot;Spark DataFrame学习&quot;&gt;&lt;/a&gt;Spark DataFrame学习&lt;/h2&gt;&lt;h3 id=&quot;1-文件的
      
    
    </summary>
    
      <category term="Learning" scheme="https://github.com/DuncanZhou//categories/Learning/"/>
    
    
      <category term="Spark" scheme="https://github.com/DuncanZhou//tags/Spark/"/>
    
  </entry>
  
  <entry>
    <title>模型记录</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/SomeModels/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/SomeModels/</id>
    <published>2019-08-17T08:14:59.388Z</published>
    <updated>2018-08-10T09:58:14.448Z</updated>
    
    <content type="html"><![CDATA[<h2 id="实战模型记录"><a href="#实战模型记录" class="headerlink" title="实战模型记录"></a>实战模型记录</h2><h3 id="1-GBDT（Gradient-Boosting-Decision-Tree）"><a href="#1-GBDT（Gradient-Boosting-Decision-Tree）" class="headerlink" title="1.GBDT（Gradient Boosting Decision Tree）"></a>1.GBDT（Gradient Boosting Decision Tree）</h3><ul><li>GBDT中的树是<strong>回归树（不是分类树）</strong>，GBDT用来做回归预测，调整后也可以用来分类。</li><li>回归树：回归树总体流程类似于分类树，<strong>区别在于，回归树的每一个节点都会得到一个预测值</strong>，以年龄为例，该预测值等于属于这个节点的所有人年龄的平均值。分枝时穷举每个feature的每个阈值找最好的分割点，<strong>但衡量标准不再是最大熵，而是最小平方误差</strong>。<strong>分枝终止条件为属性值唯一或者预设的终止条件（叶子个数上限）</strong></li><li>提升树算法：提升树是迭代多棵回归树来共同决策。<strong>当采用平方误差损失函数时</strong>，<strong>每一个棵回归树学习的是之前所有树的结论和残差</strong>，拟合得到一个当前的残差回归树。</li><li><strong>梯度提升决策树：</strong>当损失函数是平方损失和指数损失函数时，每一步的优化很简单，如平方损失函数学习残差回归树。但<strong>对于一般的损失函数，往往每一步优化没那么容易</strong>（如绝对值损失函数和Huber损失函数），所以有梯度下降方法。</li></ul><h3 id="2-XGBoost（eXtreme-Gradient-Boosting）"><a href="#2-XGBoost（eXtreme-Gradient-Boosting）" class="headerlink" title="2.XGBoost（eXtreme Gradient Boosting）"></a>2.XGBoost（eXtreme Gradient Boosting）</h3><p>和gbdt对比：</p><ul><li>1.GBDT以CART作为基分类器，xgboost还<strong>支持线性分类器</strong>。</li><li>2.GBDT在优化函数中只用到一阶导数信息，<strong>xgboost则对代价函数进行了二阶泰勒展开</strong>，同时用到了一阶和二阶导数。</li><li>3.xgboost在代价函数中<strong>加入了正则项</strong>，控制了模型的复杂度。正则项包含两部分：叶子节点数和叶子结点输出分数。</li><li>4.划分点的查找:<strong>贪心算法和近似算法</strong></li><li>5.支持并行，<strong>在特征粒度上并行</strong>，预先对数据进行排序，保存为block结构，在节点分裂时计算每个特征的信息增益，<strong>各个特征的信息增益就是多个线程进行</strong>。</li></ul><h3 id="3-LightGBM"><a href="#3-LightGBM" class="headerlink" title="3.LightGBM"></a>3.LightGBM</h3><p>优化点</p><ul><li>1.Histogram算法：先把连续的浮点特征值离散化成k个整数，同事构造一个宽度为k的直方图。遍历数据时，根据离散化后的值作为索引在直方图中累计统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。</li><li>2.带深度限制的Leaf-wise的叶子生长策略：每次从当前所有叶子中，<strong>找到分裂增益最大的一个叶子，然后分裂，如此循环</strong>。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。 </li></ul><h3 id="4-RandomForest"><a href="#4-RandomForest" class="headerlink" title="4.RandomForest"></a>4.RandomForest</h3><p>用<strong>bootstrap自助法生成m个训练集</strong>，<strong>对每个训练集构造一颗决策树</strong>，在节点找特征进行分裂的时候，并不是对所有特征找到使得指标（如信息增益）最大的，而是<strong>在特征中随机抽取一部分特征</strong>，在抽取到的特征中找到最优解，进行分裂。模型预测阶段就是bagging策略，分类投票，回归取均值。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;实战模型记录&quot;&gt;&lt;a href=&quot;#实战模型记录&quot; class=&quot;headerlink&quot; title=&quot;实战模型记录&quot;&gt;&lt;/a&gt;实战模型记录&lt;/h2&gt;&lt;h3 id=&quot;1-GBDT（Gradient-Boosting-Decision-Tree）&quot;&gt;&lt;a href=
      
    
    </summary>
    
      <category term="Data Mining" scheme="https://github.com/DuncanZhou//categories/Data-Mining/"/>
    
    
      <category term="MachineLearning" scheme="https://github.com/DuncanZhou//tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>天池-半导体质量预测</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/Semiconduction/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/Semiconduction/</id>
    <published>2019-08-17T08:14:59.381Z</published>
    <updated>2018-01-19T03:43:06.296Z</updated>
    
    <content type="html"><![CDATA[<h2 id="天池-半导体质量预测"><a href="#天池-半导体质量预测" class="headerlink" title="天池-半导体质量预测"></a>天池-半导体质量预测</h2><p>最近跟着做天池的比赛,将比赛过程中遇到的问题记录如下:</p><h3 id="1-特征的选择"><a href="#1-特征的选择" class="headerlink" title="1.特征的选择?"></a>1.特征的选择?</h3><blockquote><p><strong>特征选择的方法</strong>: 1) 嵌入式 2) 过滤式 3) 封装式</p></blockquote><p><strong>1)数据清洗:</strong></p><ul><li>1.筛选掉重复的列</li><li>2.对于类别类型特征,利用sklearn编码(one-hot, Label Encoder等)</li><li>3.使用平均值填充完后再去除冗余列(方差为0,列重复)</li></ul><p>清洗过后,特征从原来的8000多维降到了3400多维.</p><ul><li>4.特征中存在全为NaN值的,也去掉这些列</li></ul><blockquote><p>总结:数据清洗过后,总的特征维数维3342维;随机森林MSE为:0.03612</p></blockquote><p><strong>2)特征选择:</strong></p><ul><li>嵌入式: 根据模型来分析特征的重要性,最常见的方式为<strong>正则化</strong>来做特征选择.</li><li>过滤式: 评估单个特征和结果之间的相关程度,排序留下Top相关的特征部分.(缺点:没有考虑到特征之间的关联作用,可能把有用的关联特征误踢掉)</li><li>封装式: 把特征选择看作一个特征子集搜索问题,筛选各种特征子集,用模型评估效果.</li></ul><blockquote><p>处理方法:</p><ul><li>过滤式:使用单个随机森林得到的feature<em>importances</em>排序后保留了44个特征,为<br>[‘310X207’, ‘210X158’, ‘311X7’, ‘330X1132’, ‘220X13’, ‘310X149’, ‘750X883’, ‘210X207’, ‘312X144’, ‘210X192’, ‘312X61’, ‘312X66’, ‘440AX77’, ‘220X197’, ‘310X153’, ‘330X1190’, ‘344X252’, ‘310X33’, ‘210X174’, ‘440AX95’, ‘312X777’, ‘330X102’, ‘440AX187’, ‘340X161’, ‘312X55’, ‘330X590’, ‘210X89’, ‘330X1129’, ‘210X164’, ‘210X188’, ‘330X1146’, ‘310X119’, ‘360X1049’, ‘440AX182’, ‘750X640’, ‘440AX65’, ‘312X789’, ‘311X154’, ‘310X43’, ‘312X782’, ‘312X555’, ‘420X4’, ‘312X785’, ‘210X229’]</li></ul></blockquote><ul><li>包裹式:利用随机森林的性能作为评价指标筛选出200个特征<br>set([‘440AX98’, ‘310X207’, ‘210X158’, ‘261X641’, ‘261X269’, ‘312X61’, ‘312X66’, ‘220X197’, ‘520X317’, ‘400X151’, ‘400X150’, ‘400X153’, ‘330X594’, ‘330X590’, ‘210X164’, ‘210X8’, ‘420X4’, ‘330X1223’, ‘310X117’, ‘261X524’, ‘310X119’, ‘261X607’, ‘750X640’, ‘210X126’, ‘311X154’, ‘312X555’, ‘261X689’, ‘520X245’, ‘261X477’, ‘750X883’, ‘330X589’, ‘261X590’, ‘300X8’, ‘261X591’, ‘261X468’, ‘440AX77’, ‘300X3’, ‘220X179’, ‘330X1190’, ‘220X177’, ‘220X176’, ‘220X175’, ‘220X174’, ‘220X173’, ‘220X172’, ‘220X171’, ‘220X170’, ‘261X464’, ‘TOOL (#2)’, ‘330X351’, ‘330X102’, ‘330X355’, ‘330X354’, ‘330X1049’, ‘330X1042’, ‘312X57’, ‘312X55’, ‘210X89’, ‘330X1040’, ‘330X1043’, ‘330X1129’, ‘261X460’, ‘330X1044’, ‘261X462’, ‘330X1046’, ‘210X188’, ‘330X353’, ‘360X1049’, ‘440AX66’, ‘440AX67’, ‘440AX64’, ‘440AX65’, ‘330X135’, ‘330X134’, ‘312X144’, ‘330X133’, ‘330X132’, ‘330X139’, ‘312X782’, ‘210X174’, ‘312X785’, ‘312X789’, ‘261X608’, ‘261X609’, ‘520X312’, ‘520X313’, ‘520X314’, ‘330X1228’, ‘420X33’, ‘330X1132’, ‘261X600’, ‘261X601’, ‘330X641’, ‘330X1226’, ‘330X1221’, ‘330X1220’, ‘210X206’, ‘210X207’, ‘261X598’, ‘261X599’, ‘340X105’, ‘340X107’, ‘210X190’, ‘210X191’, ‘210X192’, ‘261X593’, ‘261X594’, ‘261X596’, ‘261X597’, ‘220X557’, ‘220X551’, ‘310X37’, ‘310X36’, ‘310X34’, ‘310X33’, ‘261X268’, ‘310X31’, ‘310X30’, ‘440AX95’, ‘210X3’, ‘210X4’, ‘210X5’, ‘210X6’, ‘210X7’, ‘312X777’, ‘210X9’, ‘261X260’, ‘261X261’, ‘261X266’, ‘261X267’, ‘261X264’, ‘261X265’, ‘520X246’, ‘520X247’, ‘261X736’, ‘261X737’, ‘520X242’, ‘520X243’, ‘310X153’, ‘344X252’, ‘440AX90’, ‘261X262’, ‘330X1146’, ‘440AX182’, ‘440AX187’, ‘261X687’, ‘261X688’, ‘310X43’, ‘330X157’, ‘330X404’, ‘261X512’, ‘261X513’, ‘330X401’, ‘520X55’, ‘330X403’, ‘261X517’, ‘261X518’, ‘261X519’, ‘311X7’, ‘330X409’, ‘330X159’, ‘330X158’, ‘330X461’, ‘520X333’, ‘220X13’, ‘310X149’, ‘520X244’, ‘261X338’, ‘330X1249’, ‘330X1248’, ‘300X7’, ‘261X330’, ‘261X331’, ‘340X161’, ‘261X333’, ‘330X1247’, ‘344X121’, ‘261X336’, ‘330X1244’, ‘520X240’, ‘330X1230’, ‘520X241’, ‘330X1241’, ‘261X335’, ‘220X535’, ‘210X129’, ‘210X128’, ‘220X531’, ‘220X530’, ‘210X125’, ‘210X124’, ‘210X127’, ‘261X526’, ‘210X121’, ‘210X120’, ‘210X123’, ‘261X230’, ‘261X592’, ‘440AX123’, ‘261X742’, ‘440AX99’, ‘311X83’, ‘220X178’, ‘330X535’, ‘210X229’]</li></ul><p>1) 提取特征后,xgboost的mse为0.0325341683406<br>2) 单个随机森林的5折交叉验证的平均mse为0.0288353227614<br>(max_depth=None,n_estimators=160,min_samples_leaf=2,max_features=n_features)</p><p>使用模型的features<em>importances</em>选择的特征和rfe做交集得到的特征为:<br>[‘210X158’, ‘330X1228’, ‘330X1132’, ‘220X13’, ‘310X149’, ‘750X883’, ‘330X589’, ‘210X207’, ‘440AX77’, ‘312X66’, ‘210X192’, ‘330X1190’, ‘310X33’, ‘312X555’, ‘310X31’, ‘310X30’, ‘440AX95’, ‘210X6’, ‘210X8’, ‘330X102’, ‘340X161’, ‘312X57’, ‘310X153’, ‘330X590’, ‘210X89’, ‘330X1129’, ‘210X164’, ‘312X777’, ‘210X188’, ‘330X1146’, ‘310X119’, ‘750X640’, ‘311X7’, ‘312X144’, ‘310X43’, ‘312X782’, ‘210X174’, ‘420X4’, ‘210X229’, ‘312X785’, ‘312X789’]</p><h3 id="2-缺失值的处理"><a href="#2-缺失值的处理" class="headerlink" title="2.缺失值的处理?"></a>2.缺失值的处理?</h3><ul><li>使用任意数值填充</li><li>使用平均值填充</li></ul><h3 id="3-维数降维"><a href="#3-维数降维" class="headerlink" title="3.维数降维?"></a>3.维数降维?</h3><h3 id="4-模型的选择"><a href="#4-模型的选择" class="headerlink" title="4.模型的选择?"></a>4.模型的选择?</h3><ol><li>Random Forest</li><li>GBDT(Gradient Boosting Decision Tree)<blockquote><p>这里记录下GBDT的发展过程: Regression Decision Tree -&gt; Boosting Decision Tree -&gt; Gradient Boosting Decision Tree,GBDT利用加法模型和前向分步法实现学习的优化过程.GBDT是一个基于迭代累加的决策树算法，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。 缺点:1) 计算复杂度高 2) 不适合高维稀疏特征</p></blockquote></li></ol><p>3.Xgboost</p><blockquote><p>xgboost是boosting Tree的一个很牛的实现:</p><ul><li>显示地把树模型复杂度作为正则项加到优化目标中</li><li>公式推导中用到了二阶导数,用了二阶泰勒展开</li><li>实现了分裂点寻找近似算法</li><li>利用了特征的稀疏性</li><li>并行计算</li></ul></blockquote><p>xgboost的训练速度远远快于传统的GBDT,10倍量级.</p><blockquote><p>总结:重新选用xgboost模型,参数如下,mse为0.0320532717482<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">params=&#123;&apos;booster&apos;:&apos;gbtree&apos;,</div><div class="line">    &apos;objective&apos;: &apos;reg:linear&apos;,</div><div class="line">    &apos;eval_metric&apos;: &apos;rmse&apos;,</div><div class="line">    &apos;max_depth&apos;:4,</div><div class="line">    &apos;lambda&apos;:6,</div><div class="line">    &apos;subsample&apos;:0.75,</div><div class="line">    &apos;colsample_bytree&apos;:1,</div><div class="line">    &apos;min_child_weight&apos;:1,</div><div class="line">    &apos;eta&apos;: 0.04,</div><div class="line">    &apos;seed&apos;:0,</div><div class="line">    &apos;nthread&apos;:8,</div><div class="line">     &apos;silent&apos;:0&#125;</div></pre></td></tr></table></figure></p></blockquote><h3 id="5-实践过程"><a href="#5-实践过程" class="headerlink" title="5.实践过程"></a>5.实践过程</h3><p>1.特征选择过程:去除全为Nan的列,去除Nan值个数大于200的列,去除object列,去除重复的列,选择Pearson相关系数&gt;0.2的列,最后共得到5600多维特征.<br>这一步很粗糙,改进: </p><ul><li>1) 加入object的列</li><li>2)特征维数继续筛减:可以试一下PCA降维</li><li>3)时间列属性的加入</li></ul><p>2.模型的选择:单模型线性回归线下mse:0.0388左右,而线上为0.0446.之前用随机森林回归预测,线下0.0297,而线上0.0493.从这个现象结合线下数据只有500条是否可以得出线下和线上数据并不是分布相同,或者说差异较大,而且线下训练可能存在过拟合.2017.12.24将三种回归模型加权平均融合提交结果.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;天池-半导体质量预测&quot;&gt;&lt;a href=&quot;#天池-半导体质量预测&quot; class=&quot;headerlink&quot; title=&quot;天池-半导体质量预测&quot;&gt;&lt;/a&gt;天池-半导体质量预测&lt;/h2&gt;&lt;p&gt;最近跟着做天池的比赛,将比赛过程中遇到的问题记录如下:&lt;/p&gt;
&lt;h3 id
      
    
    </summary>
    
      <category term="Competition" scheme="https://github.com/DuncanZhou//categories/Competition/"/>
    
    
      <category term="Competition" scheme="https://github.com/DuncanZhou//tags/Competition/"/>
    
  </entry>
  
  <entry>
    <title>社交网络中抽取有代表性的用户</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/Representatives/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/Representatives/</id>
    <published>2019-08-17T08:14:59.329Z</published>
    <updated>2018-01-19T03:42:40.139Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-为什么要做这个问题"><a href="#1-为什么要做这个问题" class="headerlink" title="1.为什么要做这个问题"></a>1.为什么要做这个问题</h3><h4 id="1-1-从社会应用角度"><a href="#1-1-从社会应用角度" class="headerlink" title="1.1 从社会应用角度"></a>1.1 从社会应用角度</h4><ul><li>在HCI(人机交互)中,实施调查和去获得用户的反馈都是主要针对有代表性的用户.</li><li>代表性人物的行为习惯和关注点可以折射出整体用户的兴趣偏向和关注点,对于广告投放,物品推荐是有助的. </li><li>对于目前日益增长的社交网络用户,从大量的社交网络用户中抽取一个具有代表性的子集才是Human-readable的,有益于数据分析,相当于一个数据摘要.</li></ul><h4 id="1-2-从科研方法的角度"><a href="#1-2-从科研方法的角度" class="headerlink" title="1.2 从科研方法的角度"></a>1.2 从科研方法的角度</h4><ul><li>从大量模型或数据点中抽取一个保留了原数据集的特征是机器学习/计算机视觉领域数据分析和推荐系统领域都是一个重要的问题.</li><li>机器学习领域,找原型子集来辅助分类算法.</li></ul><h3 id="2-怎样定义代表性"><a href="#2-怎样定义代表性" class="headerlink" title="2.怎样定义代表性"></a>2.怎样定义代表性</h3><blockquote><p>Note:和在社交网络中寻找影响力最大化的问题不同,找出具有代表性的用户的目的是抽取一些”平均”的用户,他们能够在统计上代表原来所有用户的特征.</p></blockquote><h4 id="2-1-代表性用户具备的条件"><a href="#2-1-代表性用户具备的条件" class="headerlink" title="2.1 代表性用户具备的条件:"></a>2.1 代表性用户具备的条件:</h4><p><font color="blue">版本一.</font></p><ul><li>1.从<font color="green">属性特征角度</font>上,他们很好的代表了原数据集用户的属性特征(行为习惯/性格特征/领域情况等等),即,与原数据集用户具有<strong><font color="red">较少的特征损耗</font></strong></li><li>2.从<font color="green">分布特征角度</font>,代表性子集应尽可能拟合原数据集的样本分布,即,与原数据集具有<strong><font color="red">较少的分布损耗</font></strong>(类似于原数据集中每个领域的人物分布,代表性子集能够拟合原数据集每个领域的人物分布)</li><li>3.从<font color="green">差异性角度</font>上,代表性子集需要能够作为每个领域的<font color="red"><strong>典型</strong></font>人物,所以代表性子集内部各领域之间的人物需要保持一定的差异性,即,代表性子集内部需要<strong><font color="red">较大的差异性或较小的相似性</font></strong></li></ul><p><font color="blue">版本二.</font></p><ul><li>1.从<font color="green">特征角度</font>上,他们很好的代表了原数据集用户的属性特征(行为习惯/性格特征/领域情况等等),即,与原数据集用户具有<strong><font color="red">较少的特征损耗</font></strong></li><li>2.从<font color="green">分布角度</font>,代表性子集在满足(1)条件下应尽可能的分散或稀疏,使得子集可以尽可能地还原原数据集的分布,即,P具有<strong><font color="red">具有稀疏性</font></strong>;<br>-note:如果仅仅要求<strong>特征损耗最小</strong>,可能会导致代表性子集都聚集在人数较多较相似的团体中,以致于原数据集的分布丢失.</li></ul><p>目前倾向于版本一.</p><h4 id="2-2-问题定义"><a href="#2-2-问题定义" class="headerlink" title="2.2 问题定义:"></a>2.2 问题定义:</h4><p>在原数据集人物集合中寻找这样的代表性子集P</p><ul><li>a)P能够满足以上代表性的定义</li><li>b)P是数量最小的那个代表性集合</li></ul><h4 id="2-3-Novel之处或者contibution"><a href="#2-3-Novel之处或者contibution" class="headerlink" title="2.3 Novel之处或者contibution:"></a>2.3 Novel之处或者contibution:</h4><ul><li>1.代表性人物包含了两种情况的综合考虑,之前论文中大多考虑单一方面</li><li>2.代表性人物的大小不需要先验设定.</li></ul><p>将用户以各个属性构建向量,以向量之间的距离来定义人物之间的代表性.<br>以Twitter社交拓扑为例,当A用户关注了B用户,将会有A指向B的一条有向边,</p><h3 id="3-如何具体评价子集的代表性"><a href="#3-如何具体评价子集的代表性" class="headerlink" title="3.如何具体评价子集的代表性"></a>3.如何具体评价子集的代表性</h3><h3 id="4-方法"><a href="#4-方法" class="headerlink" title="4.方法"></a>4.方法</h3>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-为什么要做这个问题&quot;&gt;&lt;a href=&quot;#1-为什么要做这个问题&quot; class=&quot;headerlink&quot; title=&quot;1.为什么要做这个问题&quot;&gt;&lt;/a&gt;1.为什么要做这个问题&lt;/h3&gt;&lt;h4 id=&quot;1-1-从社会应用角度&quot;&gt;&lt;a href=&quot;#1-1-从社
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>推荐算法</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/RecommendationNotes/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/RecommendationNotes/</id>
    <published>2019-08-17T08:14:59.318Z</published>
    <updated>2018-08-20T02:55:15.439Z</updated>
    
    <content type="html"><![CDATA[<h3 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h3><h3 id="1-基于内容-用户的推荐"><a href="#1-基于内容-用户的推荐" class="headerlink" title="1.基于内容 / 用户的推荐"></a>1.基于内容 / 用户的推荐</h3><p>更多依赖相似性计算然后推荐</p><ul><li>基于<strong>用户信息</strong>进行推荐</li><li>基于<strong>内容 、物品的信息</strong>进行推荐</li></ul><h3 id="2-协同过滤"><a href="#2-协同过滤" class="headerlink" title="2.协同过滤"></a>2.协同过滤</h3><p>需要通过用户行为来计算用户或物品见的相关性</p><ul><li><p>基于<strong>用户的协同推荐</strong>: 以人为本</p><p>| 小张 | 产品经理、Google、增长   |<br>| —— | ———————————— |<br>| 小明 | 产品经理、Google、比特币 |<br>| 小吴 | 比特币、区块链、以太币   |</p><p><strong>这是一个用户关注内容的列表，显然在这个列表中，小张和小明关注的内容更为相似，那么可以给小张推荐比特币。</strong></p></li><li><p>基于<strong>物品的系统推荐</strong></p><p>以物为本建立各商品的相似度矩阵</p><p>| 产品经理 | 小张、小明 |<br>| ———— | ————— |<br>| Google   | 小张、小明 |<br>| 比特币   | 小明、小吴 |</p><p>小张和小明都不约而同地看了产品经理和Google，这可以说明产品经理和Google有相似，那么之后<strong>有看了Google相关内容的用户就可以给推荐产品经理</strong>的相关内容。     </p></li></ul><h3 id="3-基于知识的推荐"><a href="#3-基于知识的推荐" class="headerlink" title="3.基于知识的推荐"></a>3.基于知识的推荐</h3><p>某一领域的一整套规则和路线进行推荐。参照可汗学院知识树。</p><p>补充：（图片来源知乎shawn1943，感谢）</p><p><img src="https://pic1.zhimg.com/80/v2-9f88f829b59ddb4f1e0571c46c158d1c_hd.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;算法分类&quot;&gt;&lt;a href=&quot;#算法分类&quot; class=&quot;headerlink&quot; title=&quot;算法分类&quot;&gt;&lt;/a&gt;算法分类&lt;/h3&gt;&lt;h3 id=&quot;1-基于内容-用户的推荐&quot;&gt;&lt;a href=&quot;#1-基于内容-用户的推荐&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
      <category term="Recommendation" scheme="https://github.com/DuncanZhou//categories/Recommendation/"/>
    
    
      <category term="Recommendation" scheme="https://github.com/DuncanZhou//tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>Recommendation方向学习</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/RecommendationLearning/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/RecommendationLearning/</id>
    <published>2019-08-17T08:14:59.311Z</published>
    <updated>2018-01-19T03:42:32.528Z</updated>
    
    <content type="html"><![CDATA[<h3 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h3><p>目前推荐上研究的方向有这样几个方向:<br>1.Temporal Context-Aware Recommendation<br>2.Spatial Recommendation for Out-of-Town Users<br>3.Location-based and Real-time Recommendation<br>4.Efficiency of Online Recommendation</p><p>补充学习:</p><blockquote><p>online learning强调的是学习是实时的，流式的，每次训练不用使用全部样本，而是以之前训练好的模型为基础，每来一个样本就更新一次模型，这种方法叫做OGD（online gradient descent）。</p><p>batch learning或者叫offline learning强调的是每次训练都需要使用全量的样本，因而可能会面临数据量过大的问题。</p></blockquote><p>传统的推荐系统广泛都使用了<font color="blue">协同过滤</font>和<font color="blue">基于内容过滤技术</font></p><p>协同过滤分为</p><blockquote><p>基于内存的推荐和基于模型的推荐(矩阵分解)</p></blockquote><p>Context-Aware Recommender Systems(CARS)包含三种范例:contextual pre-filtering,contextual post-filtering and contextual modeling.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;综述&quot;&gt;&lt;a href=&quot;#综述&quot; class=&quot;headerlink&quot; title=&quot;综述&quot;&gt;&lt;/a&gt;综述&lt;/h3&gt;&lt;p&gt;目前推荐上研究的方向有这样几个方向:&lt;br&gt;1.Temporal Context-Aware Recommendation&lt;br&gt;2.Spa
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>python-MPI安装命令</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/Python-MPI/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/Python-MPI/</id>
    <published>2019-08-17T08:14:59.306Z</published>
    <updated>2018-01-19T03:42:23.594Z</updated>
    
    <content type="html"><![CDATA[<h3 id="在Ubuntu下安装MPI环境-python环境"><a href="#在Ubuntu下安装MPI环境-python环境" class="headerlink" title="在Ubuntu下安装MPI环境(python环境)"></a>在Ubuntu下安装MPI环境(python环境)</h3><p><font color="red">Step1:</font>安装python环境&lt;/br&gt;</p><p><font color="red">Step2:</font>sudo apt-get install openmpi-bin&lt;/br&gt;</p><p><font color="red">Step3:</font>sudo apt-get install libopenmpi-dev&lt;/br&gt;</p><p><font color="red">Step4:</font>sudo apt-get install python-mpi4py&lt;/br&gt;</p><p><strong>(第三步不要忽略)</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;在Ubuntu下安装MPI环境-python环境&quot;&gt;&lt;a href=&quot;#在Ubuntu下安装MPI环境-python环境&quot; class=&quot;headerlink&quot; title=&quot;在Ubuntu下安装MPI环境(python环境)&quot;&gt;&lt;/a&gt;在Ubuntu下安装MPI
      
    
    </summary>
    
      <category term="Note" scheme="https://github.com/DuncanZhou//categories/Note/"/>
    
    
  </entry>
  
  <entry>
    <title>python构建小顶堆</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/python-minHeap/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/python-minHeap/</id>
    <published>2019-08-17T08:14:59.300Z</published>
    <updated>2018-01-19T03:42:14.983Z</updated>
    
    <content type="html"><![CDATA[<p>近日实验中需要用到小顶堆,记录下来,便于日后参考.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">import heapq</div><div class="line"># 定义一个小顶堆</div><div class="line">class MinHeap(object):</div><div class="line"></div><div class="line">    # 允许传入tuple,按照第二个元素比较</div><div class="line">    def __init__(self, initial=None, key=lambda x:x[1]):</div><div class="line">        self.key = key</div><div class="line">        if initial:</div><div class="line">            self._data = [(key(item), item) for item in initial]</div><div class="line">            heapq.heapify(self._data)</div><div class="line">        else:</div><div class="line">            self._data = []</div><div class="line"></div><div class="line">    def push(self, item):</div><div class="line">        heapq.heappush(self._data, (self.key(item), item))</div><div class="line"></div><div class="line">    def pop(self):</div><div class="line">        return heapq.heappop(self._data)[1]</div><div class="line"></div><div class="line">    def size(self):</div><div class="line">        return len(self._data)</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日实验中需要用到小顶堆,记录下来,便于日后参考.&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line
      
    
    </summary>
    
      <category term="Note" scheme="https://github.com/DuncanZhou//categories/Note/"/>
    
    
  </entry>
  
  <entry>
    <title>python与neo-4j交互(对py2neo包做的笔记)</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/py2neo/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/py2neo/</id>
    <published>2019-08-17T08:14:59.291Z</published>
    <updated>2018-01-19T03:42:06.693Z</updated>
    
    <content type="html"><![CDATA[<p>1.连接数据库(三种方式相等)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">graph_1 = Graph()</div><div class="line">graph_2 = Graph(host=&quot;localhost&quot;)</div><div class="line">graph_3 = Graph(&quot;http://localhost:7474/db/data&quot;)</div></pre></td></tr></table></figure></p><p>2.事务操作<br>a)直接返回结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">graph.data(&quot;MATCH (a:Person) RETURN a.name, a.born LIMIT 4&quot;)</div></pre></td></tr></table></figure></p><p>b)以pandas格式返回结果<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DataFrame(graph.data(&quot;MATCH (a:Person) RETURN a.name, a.born LIMIT 4&quot;))</div></pre></td></tr></table></figure></p><p>事务操作样例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">from py2neo import Graph, Node, Relationship</div><div class="line">g = Graph()</div><div class="line">tx = g.begin()</div><div class="line">a = Node(&quot;Person&quot;, name=&quot;Alice&quot;)</div><div class="line">tx.create(a)</div><div class="line">b = Node(&quot;Person&quot;, name=&quot;Bob&quot;)</div><div class="line">ab = Relationship(a, &quot;KNOWS&quot;, b)</div><div class="line">tx.create(ab)</div><div class="line">tx.commit()</div><div class="line">g.exists(ab)</div></pre></td></tr></table></figure></p><p>3.匹配关系<br>查找alice的所有朋友<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">for rel in graph.match(start_node=alice,rel_type=&quot;FRIEND&quot;):</div><div class="line">print(rel.end_node()[&apos;name&apos;])</div></pre></td></tr></table></figure></p><p>4.带参数查询<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">from py2neo import Graph</div><div class="line">g = Graph()</div><div class="line"># evaluate()返回结果的第一个值</div><div class="line">g.run(&quot;&quot;MATCH (a) WHERE a.email=&#123;x&#125; RETURN a.name&quot;,x=&quot;bob@acme.com&quot;).evaluate()</div><div class="line">g.run(&quot;&quot;MATCH (a) WHERE a.email=&#123;x&#125; RETURN a.name&quot;,x=&quot;bob@acme.com&quot;).data()</div></pre></td></tr></table></figure></p><p>5.NodeSelector使用,可以使用Cypher语言的where部分<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">from py2neo import Graph,NodeSelector</div><div class="line">graph = Graph()</div><div class="line">selector = NodeSelector(graph)</div><div class="line">slected = selector.select(&quot;Person&quot;,name=&quot;Keanu Reeves&quot;)</div><div class="line">list(selected)</div><div class="line">selected = selector.select(&quot;Person&quot;).where(&quot;_.name =~ &apos;J.*&apos;&quot;,&quot;1960 &lt;= _.born &lt; 1970&quot;)</div><div class="line">list(selected)</div></pre></td></tr></table></figure></p><p>6.删除操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"># 删除所有的</div><div class="line">graph.delete_all()</div></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1.连接数据库(三种方式相等)&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;
      
    
    </summary>
    
      <category term="Note" scheme="https://github.com/DuncanZhou//categories/Note/"/>
    
    
  </entry>
  
  <entry>
    <title>pip安装包更换pypi源</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/pip_changeSource/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/pip_changeSource/</id>
    <published>2019-08-17T08:14:59.279Z</published>
    <updated>2018-01-19T03:41:59.082Z</updated>
    
    <content type="html"><![CDATA[<p>使用豆瓣源</p><blockquote><p>sudo pip install #package -i <a href="http://pypi.douban.com.simple" target="_blank" rel="external">http://pypi.douban.com.simple</a> —trusted-host pypi.douban.com</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用豆瓣源&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;sudo pip install #package -i &lt;a href=&quot;http://pypi.douban.com.simple&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;http://pypi
      
    
    </summary>
    
      <category term="Note" scheme="https://github.com/DuncanZhou//categories/Note/"/>
    
    
  </entry>
  
  <entry>
    <title>Personalized Search泛读记录</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/Personalized%20Search%E6%B3%9B%E8%AF%BB%E8%AE%B0%E5%BD%95/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/Personalized Search泛读记录/</id>
    <published>2019-08-17T08:14:59.267Z</published>
    <updated>2018-01-19T03:41:52.793Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Backgournd"><a href="#Backgournd" class="headerlink" title="Backgournd"></a>Backgournd</h3><p>搜索在20年前就已出现在互联网，而如今搜索已经无处不在。传统的搜索像这样，用户给出Query，Query中包含1个或多个关键词，搜索引擎通过关键词去检索返回查询结果。然而，在互联网上存在的资源早已是亿万级，所以仅仅用传统的搜索方法去返回给用户查询结果势必会存在大量用户不需要的结果，根据2007年”Tag recommendations in folksonomies”一文中提出不考虑用户偏好返回的搜索结果中仅有20%-45%是用户想要的，另外,用户所想查找的内容也可能远远不在结果的前列，所以，这类问题的解决需要在传统的搜索方法上考虑context-上下文，即，<br>简要概括:1.用户搜索返回的结果大量是其所不需要的;2.不同的用户提出同一个关键词,搜索引擎返回的结果都是同样的,而不同的用户使用同一个关键词所想搜索的意图其实可能是不同的.<br>将(1)用户的行为、习惯、兴趣/历史搜索结果等等;(2)资源上下文(3)任务上下文等因素考虑进去。</p><p>网络个性化用于四类:predicting web navigation, assisting personalization information, personalizing content, and personalizing search results<br>有两类方法:协同过滤和user profiles<br>协同过滤的缺点:这种方法能根据大多数人的兴趣推测什么是流行的,但不能预测某一个用户是否对新的页面是否感兴趣.</p><p><strong>1.User Models</strong><br>1.1 可以用来构建用户特征的有<br>a)内容方面:查询的关键词、网页的内容、桌面索引等等<br>b)行为方面:浏览的网页、tag 活动/直接或间接的反馈等<br>c)上下文方面:性别、年龄、地理位置、时间等</p><p>1.2 时间上分短期和长期兴趣(将两者结合起来,按时间分配权重)<br>1.2.1长期兴趣偏好建立方式:<br>    行为:具体的查询和URLs<br>    内容:语言模型/主题模型</p><p>1.2.2短期兴趣偏好建立方式:<br>    搜索session的queries</p><p>1.3 用户分个体还是某类群体</p><p><strong><font color="red">Note:</font></strong><br><strong><font size="4">a</font></strong>.在用户model中还需考虑的有长期和短期的兴趣,仅仅根据用户长期的兴趣来推测用户现在想要的搜索结果会有偏差.e.g.一个人之前搜索的”java”都是关于编程语言,但不排除他下一次搜索”java”是要找”java咖啡或者java 岛”.用户的兴趣会改变的.<br><strong><font size="4">b</font></strong>.对用户可profile的信息有:Clickthrough Histories/Queries Histories/搜索过网页的Snippets/收藏过的书签<br><strong><font size="4">c</font></strong>.Domain Ontological:所谓的领域本体（domain-specific ontology）就是对学科概念的一种描述，包括学科中的概念、概念的属性、概念间的关系以及属性和关系的约束。由于知识具有显著的领域特性，所以领域本体能够更为合理而有效地进行知识的表示。<br><strong><font size="4">d</font></strong>.Folksonomy中的challenges:(1)用户标注的tags有很多的同样的拼写,不同意思的单词;有很多同义词;(2)怎么根据tags去对用户偏好建模(Tags聚类/VSM-空间向量模型/领域本体论(有一篇文章中将用户的tags映射到ODP(the Open Directionary Project)-Web topic ontology中))<br>—在Folksonomy中,用户标注的资源都是用户所感兴趣的资源,或者说用户所标注的资源都能代表用户的兴趣偏好;也就是说用户不再仅仅是web资源的消费者,同时用户可以通过承担web的一些任务同时方便其他web用户.(Folksnonomy这样一类系统代表性的有:Flickr.com/Delicious.com/Last.fm)<br><strong><font size="4">e</font></strong>.Clustering:聚类用于两个方面:切分和分层(分级).<br><strong><font size="4">f</font></strong>.<strong>Social Context</strong>:有一些预定义好的值,每个上下文的值都有具体的值.<strong>Verbal Context:</strong>历史查询/点击记录等</p><h3 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h3><p>1.可以入手的方面有三类:user model(long-term &amp; short-term)/query的替换或扩充/resource(web resource)/privacy/evaluation/查询效率<br>2.可以选择的对象有两个:Server端和Client-Side端</p><p>用户相关性:<br>1.直接的显示反馈<br>2.隐式地从点击熵获得</p><p>用户潜在的意图:<br>1.上下文的元数据<br>    Location/Time/Device</p><p>2.过去的行为<br>    当前的session活动/长期的活动和兴趣偏好</p><blockquote><p>基本的方法：<br>（1）对Query做扩展或替换<br>比如，用户正在浏览的关于汽车的页面，那么当他搜索“轮廓”时，会将Query的关键词添加“汽车”关键词，使返回的结果是用户想要找的汽车轮廓结果。<br>（2）对结果排序<br>根据user profile，进行相似度匹配对结果重新排序。</p></blockquote><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>1.MovieLens(notations &amp; ratings)<br>2.Delicious.com(bookmarks)<br>3.Flickr(notations)<br>4.DOMZ(ODP)—web search经常使用<br>5.BibSonomy(<a href="http://bibsonomy.org" target="_blank" rel="external">http://bibsonomy.org</a>)<br>6.CiteULike(网页书签数据)</p><h3 id="泛读论文时方法总结"><a href="#泛读论文时方法总结" class="headerlink" title="泛读论文时方法总结"></a>泛读论文时方法总结</h3><p>其他方法：<br>1）2002年CIKM中将用户的Query分类<br>2）2003年”Scaling Personalized Web Search”根据用户兴趣给页面分配权重用图来计算，类似PageRank<br>3）2005年WWW”A Personalized Search Engine Based on Web-Snippet Hierarchical Clustering”对网页的Snippet做了聚类然后对结果再根据User Profile对查询结果再排序<br><strong><font color="red">(Snippet指的是网页的标题和摘要)</font></strong><br>4）2005年WWW-“CubeSVD: A Novel Approach to Personalized Web Search”将用户的点击链接历史记录(who click which web page)作为user profile部分辅助查询.通过从点击链接记录数据中发现用户的兴趣和搜索信息的模式.<br>5)2005年CIKM”Implicit User Modeling for Personalized Search”User Model方法,完成了一个客户端查询代理.针对长期兴趣可能会改变的问题,本文中利用即时的搜索上下文和隐式的反馈来user model.<br>隐式的反馈信息有着两类:1)根据之前的查询寻找合适的term去扩展现在的query;2)利用用户已经看过的文档形成摘要来对没有看过的文档重新排序<br>6)2005年WI”Personalized Serach Based on User Search History”根据用户搜索历史对用户profile,利用User Profiles来对查询或者snippets分类,然后再对搜索结果重排序<br>7)2005年WI”Personaliezed Search Results with User Interest Hierarchies Learnt from Bookmarks”建立分层的user profile来对查询结果重新排序.<br>8)2005年DATAK期刊”Category ranking for personalzied search”在ODP分类基础上根据用户profile重新选择一个子图结构分类来personalize 查询结果<br>9)2006年WWW”Automatic Identification of User Interest For Personalized Search”利用用户历史点击记录构建user profile来对查询结果重新排序.<br><strong><font color="blue">补充:2006年SIGKDD”Mining Long-Term Search History to Improve Search Accuracy”挖掘用户长期的搜索历史提出统计的语言模型</font></strong><br>10)2007年WWW”A Large-scale Evaluation and Analysis of Personalized Search Strategies”评价Personalized Search对于传统搜索是否有提高,并且揭示了Click-Based方法优于profile-based方法<br>11)2007年CIS”Personalized Web Search Using User Profile”在Client-Side构建User Profile,然后对用户的Query根据Profile进行扩充使查询更具体,将Query提交给Search Engine.<br>12)2007WWW”Privacy-Enhancing Personalized Web Search”权衡用户隐私和个性化搜索的需要,将用户无结构的个人数据整合成有结构的User Profile<br>13)2007年SIGIR”Privacy Protection in Personalized Search”在个性化搜索时同时保护用户隐私,在Client-Side保护隐私比在Server-Client要好<br>14)2007年WI”Using Personalized Web Search for Enhancing Common Sense and Folksonomy Based Intelligent Search Systems”对于大众分类标签的这样的系统中,用户检索时对于其他用户标注的内容会检索到不相关的内容,本文利用搜索历史和兴趣分类来建立用户偏好<br>15)2008年CIKM”Matching Task Profiles and User Needs in Personalized Web Search”在Client-Side,同时结合用户之前的<strong><font color="green">历史搜索结果</font></strong>和<strong><font color="green>" 当前的session上下文<="" font=""></font></strong>对用户建立不同粒度的profile.同时结合past search results(search histories)和current session context就弥补了只根据用户长期兴趣造成的缺点.<br>16)2008年WWW”Personalized Search and Exploration with MyTag”一篇DEMO完成了这样一个系统,根据flickr,YouTube和del.icio.us多个系统构造用户profile,在用户查询时完成返回多个平台的个性化资源结果.<br>17)2008年SIGIR”The Impact of History Length on Personalized Search”一篇DEMO基于任务研究web search(Task-based即限制了查询的方向,限制了查询任务),研究了搜索历史的长度对personalized search的影响.<br>18)2009年”Cluster Based Personalized Search”着手了两方面:利用文本聚类方法来Personalized Search和新的evaluation准则<br>19)2009年WSDM”Discovering and Using Groups to Improve Personalized Search”由于利用收集的个人信息来User Profile,但是由于通常User的个人信息通常不足够来构建,所以该篇文章利用其他用户来辅助收集用户个人信息.通过Query的相似性groupize一类用户<br><strong><font color="blue">20)2009年TKDE期刊”Evaluating the Effectiveness of Personalized Web Search”测试了五种personalized search算法,提出了新的评价框架来测试是否personalized search对于不同的用户提出不同的queries在不同的搜索上下文中有没有用.(五种测试方法:2种Click-based和3种Topipcal-interest-based);并且,提出了现有的personalized search的缺点:大多数提出的算法都是运用到所有的用户和查询上(对于有些很明确的查询不需要应用personalized;personalization算法的有效性会根据不同的搜索上下文而不同;现有的论文中测试personalized search的算法是基于少量的参与者积累查询数据集,很少有在真是世界中数据集做测试)</font></strong><br><strong><font color="blue">21)2009年CIKM”Personalized Social Search Based on the User’s Social Network”利用用户的三种社会关系:家庭社会关系/相似社会关系/全部的社会关系来建立user profile</font></strong><br><strong><font color="blue">22)2009年CSE”Social Tagging in Query Expansion:a New Way for Personalized Web Search”对于社交网络和协标注系统利用协标注来对Query进行扩展</font></strong><br><strong><font color="blue">23)2009年Konwl Inf syst”Towards a graph-based user profile modeling for a session-based personalized search”提出了利用图结构来user profile</font></strong><br>24)2010年WWW”Anonymizing User Profiles for Personalized Web Search”关于user profile的隐私保护<br>25)2010年”Applying Taxonomic Knowledge and Semantic Collaborative Filtering to Personalized Search: a Bayesian Belief Network based Approach”对于利用查询关键词匹配得到结果的方法而言,有些结果与查询有关而却与查询的关键字术语没有能匹配的结果往往会漏掉.该篇文章为了找出具有权威性的文本,通过语义协同过滤,用贝叶斯信念网络来代表用户的偏好,查询和相关的文本.<br><strong><font color="green">26)2010年CIKM”CiteData:A new multi-faceted dataset for evaluating personalized search performance”现在的personalized search系统使用了用户各种各样的特征数据如:文本超链接/分类标签等,将各种分类方法和社会标注结合起来,随之有分类/PageRank/协同过滤等算法来处理personalized search,但是对于这些方法的评价一直没有合适的数据集,所以该篇文章提出新的评价方法,利用多种多方面的数据来评价personalized search方法的表现.</font></strong><br><strong><font color="blue">27)2010年AMT”Folksonomy-Based Ontological User Interest Profile Modeling and Its Application in Personalized Search”在大众分类系统中,利用用户标注的tags并运用领域本体论来构建用户兴趣偏好</font></strong><br>28)2010年<strong>ICDE</strong>“Personalized Web Search with Location Preferences”文中将用户偏好概念分为了内容概念和位置概念,本文不仅从搜索结果/点击率来构建内容上的兴趣偏好,还考虑了位置概念.<br>29)2010年WIC”Personalized Search based on a User-centered Recommender Engine”提出了将<strong>推荐系统</strong>和Personalized Search结合起来<br><strong><font color="blue">30)2011年CIKM”A Framework for Personalized and Collaborative Clustering of Search Results”根据search results利用Wiki预料来聚类和协同过滤的方法来优化个性化搜索结果.</font></strong><br>30)2011年WEBIST”A Multi-factor Tag-Based Personalized Search”提出了利用用户的tag activity(浏览过的网页和对网页分配的标签)来建立用户的偏好然后再重新排序搜索结果.<br>31)2011年IS的期刊”A personalized search using a  semantic distance measure in a graph-based ranking model”用图结构(映射到ODP上)来表示文本和user profiles,基于语义距离测量来重新对搜索结果排序.<br>32)2011年UMAP”Leveraging Collaborative Filtering to Tag-Based Personalized Search”利用协同过滤的方法通过其他相似用户计算用户的潜在兴趣偏好,通过相似物品来构建物品的潜在tags.<br><strong><font color="blue">33)2011年CSC”Modeling User’s Preference in Folksonomy for Personalized Search”在大众分类系统利用标签聚类来构建user profile</font></strong><br>34)2011年Canadian AI”Normal Distribution Re-Weighting for Personalized Web Search”根据term的频率建立向量构建profile,但是同时重新对vector建立权重.因为频率大小对profile的影响是不一样的,其中,比较注重的是Mid-frequency.<br>35)2011年WWW”Personalized Search on Flickr based on Searcher’s Preference Prediction”一篇DEMO,基于Flickr系统根据用户的朋友的兴趣偏好和聚类方法来预测该用户所要找的图片.(e.g.用户搜索”长城”,返回118147张照片结果,但是,他/她所需要或想要的是哪一张或那几张需要自己去从中挑选)<br><strong><font color="blue">36)2011年FSKD”User Profile for Personalized Web Search”利用三种机器学习方法(Rocchio/K-Nearest Neighbors/SVM)来构建user profiles</font></strong><br>37)2012年ICCCI”Construction of Semantic User Profile for Personalized Web Search”完成这样一个系统,让用户输入用户名和邮件地址后从网页抓取和邮件地址相关的信息来构建user profile(使用VSM)(依据ODP).<br>38)2012年APWeb”Context-Aware Personalized Search Based on User and Resource Profiles in Folksonomies”指出了之前运用于Folksonomy系统中建立VSM后TF-IDF和BM25方法的不合理之处.<br><strong><font color="blue">39)2012年Information Systems期刊”Folksonomy-based personalized search and ranking in social media services”同时利用面向用户的tags和面向items的tags构建模型,构建user-tag矩阵/user-item矩阵/tag-item矩阵;对于查询的term没有出现在标注中的情况也能够对结果重新排序</font></strong><br>40)2012年”Multilingual User Modeling for Personalized Reranking of Multilingual Web Search Results”用多语言来构建用户模型.<br>41)2012年ADMA”Personalized Diversity Search Based on User’s Social Relationships”针对搜索引擎由于不能领会用户潜在的意图和兴趣偏好,所以不能返回给用户精确/充足,且伴随有累赘的结果.现有的方法有返回多样性的结果来满足大部分用户,并且统一地运用到所有的用户和查询中,返回的结果通常返回的是大部分用户的需求,对于某个用户的具体需求并没有被考虑进去.本文将多样性搜索和个性化搜索结合来使搜索结果对于群体和某个用户来说更加精确.<br><strong><font color="blue">42)2012年SIGMOD”Taagle:Efficient,Personalized Search in Collaborative Tagging Networks”用户带有在社交网络中的权值,items带有用户的关键词标注,用户用某一个tags来搜索返回Top-k个结果</font></strong><br>43)2013年SIGMOD”Efficient Ad-hoc Search for Personalized PageRank”对PPR做了改进<br><strong><font color="blue">44)2013年WWW”Enhancing Personalized Search by Mining and Modeling Task Behavior”提出之前在Personalized Search中都是比较依赖和用户历史查询记录相关信息,对于新的查询可能会无所适从;本文提出了Task-based(基于URLs)的方法,通过在历史搜索日志中挖掘出提出过和当前用户任务相关的用户,利用他们的on-task行为来提升web pages的排序.并将算法和Query-based进行对比</font></strong><br><strong><font color="blue">45)2013年(LiQing)WI-IAT”Finding Dominating Set from Verbal Contextual Graph for Persoanlized Search in Folksonomy”对于去挖掘用户潜在的意图和兴趣偏好,基于上下文的信息是不可或缺的,在社会语言学中上下文中分为Verbal Context(queries历史/点击历史数据)和social context(mood/weather/time).通过对比了social context之后,作者选用了verbal context语言模型,verbal context模型用图结构构造,并将重要的节点区别出来.</font></strong><br><strong><font color="blue">46)2013年SIGKDD一篇DEMO”LAICOS:An Open Source Platform Personalized Social Web Search”1.利用了文本内容来建立social context2.和之前方法一样也用了对query进行扩展的方法.当用户提出一个query,系统会根据用户experience匹配query,同时,系统还会根据其他提出过相似查询的用户来返回相似的文档</font></strong><br><strong><font color="blue">47)2013年CIKM”Personalized Models of Search Satisfaction”这篇文章通过区分不同用户的搜索行为来建立用户的满意度,从而使个性化搜索更为准确.(依赖于点击数据)</font></strong><br>48)2013年SIGIR”Personalized Ranking Model Adaptation for Web Search”针对之前搜索引擎对所有的用户都运用单一的排序模型而提出了新的排序模型(通过一系列的线性转化,缩放或者转变)<br>49)2013年ICCCSA”Personalized Semantic Search Using ODP:A Sutdy Case in Academic Domain”将文本大致分类到相应的实体ODP来完成语义搜索<br>50)2013年SIGIR”SoPRa: A New Social Personalized Ranking Function for Improving Web Search”提出了新的搜索结果等级排序函数<br>51)201年SIGIR”Using Social Annotations to Enhance Document Representation for Personalized Search”基于用户查询过的网页,不仅仅基于该用户对其的标注,而且考虑其他用户的标注.因为如果只考虑该用户的标注存在两个问题:1)忽略了他没有标注的页面2)分配的等级分数不合理<br>52)2014年WSDM”Adapting Deep RankNet for Personalized Search”:RankNet被广泛地应用在web搜索任务中,但是很少有应用在Personalized Search中.本文利用5层深度神经网络来构造RankNet运用于Personalized Search中.<br>53)2014年KDD”Personalized Search Result Diversification via Structured Learning”利用有监督学习来解决搜索结果个性化多样性的问题,既保持结果的多样性,同时结合用户的兴趣偏好<br><strong><font color="blue">54)2015年”Adaptive and Multiple Interest-aware User Profiles for Personalized Search in Folksonomy:A Simple but Effective Graph-based Profiling Model”基于图结构利用社会标注的tags构造自适应的且融合多种用户兴趣偏好的user profiles.</font></strong><br>55)2015年WWW”An Optimization Framework for Weighting Implicit Relevance Labels for Personalized Web Search”提出了之前给web document分配权重的不合理之处,另外重新提出了personalized ranking算法.<br>56)2015年”Real Time Personalized Search on Social Networks”提出了在社交网络平台中两个特点1)频繁的内容更新2)小社区群体;而现有的搜索算法都还不能解决这样两个问题,本文提出了实时的personalized top-k查询(等级排序算法融合了时间/社会相关性/文本相似性).<strong>加入了时间因素</strong><br>57)2016年SIGKDD”How to Get Them a Dream Job”主要针对Job Personalized Search.<br>58)2016年SIGIR一篇DEMO”Learning to Rank Personalized Search Results in Professional Networks”在领英中提出新的结果等级排序算法.<br>59)2016年Neurocomputing”Personalized search for social media via dominating verbal context”<strong>Qing Li</strong>同之前篇<br>60)2016年Knowledge-Based Systems”Preference recmmendation for personalized search”综合之前的user profiles模型,本文指出使用比较广泛有一个CP-nets模型,不仅能简明地表达用户定性的兴趣偏好,而且很好地定义了用户偏好的范围.现在很多基于CP-nets的搜索系统都是假设用户之前已经定义好他们的兴趣偏好范围,但是在生活中这并不现实.本文的工作一方面利用不完全的CP-nets,另一方面利用协同过滤来弥补CP-nets的不足.还有一方面,本文提出偏好推荐模式来弥补CP-nets的不足.<br>61)2016年”Topic Model based Privacy Protection in Personalized Web Search”本文在保持搜索引擎个性化的同时,通过在用户查询日志中加入控制噪声来保护用户隐私.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Backgournd&quot;&gt;&lt;a href=&quot;#Backgournd&quot; class=&quot;headerlink&quot; title=&quot;Backgournd&quot;&gt;&lt;/a&gt;Backgournd&lt;/h3&gt;&lt;p&gt;搜索在20年前就已出现在互联网，而如今搜索已经无处不在。传统的搜索像这样，用
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
  <entry>
    <title>Personalized Search论文阅读笔记</title>
    <link href="https://github.com/DuncanZhou/2019/08/17/Personalized%20Search%20by%20Tag-based%20in%20Collaborative%20Tagging%20Systems/"/>
    <id>https://github.com/DuncanZhou/2019/08/17/Personalized Search by Tag-based in Collaborative Tagging Systems/</id>
    <published>2019-08-17T08:14:59.261Z</published>
    <updated>2018-01-19T03:41:45.182Z</updated>
    
    <content type="html"><![CDATA[<h2 id="论文题目"><a href="#论文题目" class="headerlink" title="论文题目"></a>论文题目</h2><h3 id="Personalized-Search-by-Tag-based-User-Profiling-and-Resource-Profile-in-Collaborative-Tagging-Systems"><a href="#Personalized-Search-by-Tag-based-User-Profiling-and-Resource-Profile-in-Collaborative-Tagging-Systems" class="headerlink" title="Personalized Search by Tag-based User Profiling and Resource Profile in Collaborative Tagging Systems"></a>Personalized Search by Tag-based User Profiling and Resource Profile in Collaborative Tagging Systems</h3><h2 id="Concept"><a href="#Concept" class="headerlink" title="Concept"></a>Concept</h2><blockquote><p><strong>Collaboative Tagging Systems</strong>:协标签系统，类似于YouTube、Flickr这样一类系统，允许用户使用标签对资源进行标注，而得出用户的标签；所以，用户具有标签，资源也有标签，这样称为协同标签。</p><p><strong>folksonomy</strong>:大众分类的。如上所说的一些系统中允许用户去标注资源，即，利用大众的力量去分类资源。</p><p><strong>personalized search</strong>:个性化搜索，在返回和查询相关的结果时不仅考虑和搜索的关键词相关，并且考虑用户的profile(偏好)。</p></blockquote><h2 id="About-this-paper"><a href="#About-this-paper" class="headerlink" title="About this paper"></a>About this paper</h2><p>论文中首先总结了之前的工作中对用户和资源的profile构建的方法，用户和资源的profile的tag的权重计算方法有TF、TF-IDF、BM25，以及用户兴趣和资源相似性的计算方法，但这些方法都存在一定的局限性。<br><strong>TF方法</strong>：对于标注比较频繁或者比较活跃的用户，经常使用某些tag标注。如果使用TF计算tag的权重，那么，对于不经常标注资源的用户，其偏好的标签权重必定比活跃的用户tag小很多。</p><p><font color="green">举个例子:</font></p><blockquote><p>Tom = {(tag1:300;tag2:200;tag3:280)}; Jerry = {(tag1:50;tag2:30;tag3:10)} 对于，Tom和Jerry来说，tag1都是其偏好的，但是如果以TF来计算，tag1对于Tom的偏好程度是大于Jerry的，而其实，tag1对Jerry是更为感兴趣的。同理，对于资源标签权重计算方法也存在相似问题。</p></blockquote><p><strong>TF-IDF方法</strong>：这里为TF-IDF的演变版，TF-IUF和TF-IRF。TF-IDF用来表示一个tag能否表示该实体的程度，同理，TF-IUF和TF-IRF表示一个标签能否表示该用户和资源。但是，这里存在的问题是，标签的权值并不能表示用户对于该标签的偏好程度。</p><p><font color="green">举个例子:</font></p><blockquote><p>Tom的标签使用频率如下:[tag1:500;tag2:400;…;tagn:1]。如果使用TF-IDF方法来计算标签权重，那么，tagn的TF-IDF权重是最大的，但是，tagn并不是用户最感兴趣的。同理，对于资源标签权重计算方法也存在相似问题。</p></blockquote><p><strong>BM25</strong>:BM25方法是以TF和TF-IDF的值作为变量，所以BM25方法也存在以上的局限性。</p><p><strong>用户兴趣与资源相似性计算方法</strong>:一般计算该相似度采用的方法是余弦相似性等相似性计算法方法，这存在一些问题。因为现在用户的标签的权值代表的是用户对该tag的感兴趣程度，而不是与目标资源的相关程度，所以用一般相似性计算方法是不合理的。</p><p><font color="green">举个例子:</font></p><blockquote><p>Resource1 = {(tag1:0.9;tag2:0.95;tag3:0.85)}; Resource2 = {(tag1:0.9;tag2:0.1;tag3:0.01)}; User = {(tag1:0.8;tag2:0.5;…;tag3:0.25)}。如果用一般方法计算相似度，那么R1相似性小于R2，而与用户偏好相似的更多是R1。是因为用户对tag1，tag2，tag3都是偏好的，而R2只是对tag1更符合一些。</p></blockquote><p>所以，基于以上方法的不合理之处，该论文提出了计算user和resource标签权值及计算用户兴趣和资源相关性的方法。</p><h2 id="论文提出的方法"><a href="#论文提出的方法" class="headerlink" title="论文提出的方法"></a>论文提出的方法</h2><h3 id="User-Profiles-Modeling"><a href="#User-Profiles-Modeling" class="headerlink" title="User Profiles Modeling"></a>User Profiles Modeling</h3><blockquote><p>某用户中标签的权值 = 该标签被用来标注的次数 / 该用户标签总标注次数</p></blockquote><h3 id="Resources-Profiles-Modeling"><a href="#Resources-Profiles-Modeling" class="headerlink" title="Resources Profiles Modeling"></a>Resources Profiles Modeling</h3><blockquote><p>某资源中标签的权值 = 使用该标签标注该资源的用户数 / 标注该资源的总用户数</p></blockquote><h3 id="Personalized-Search"><a href="#Personalized-Search" class="headerlink" title="Personalized Search"></a>Personalized Search</h3><p>分为查询相似性计算和用户兴趣与查询相似性计算。相关公式见论文。<br>最后，将两个相似性分数融合得到相似性分数。</p><h2 id="实验结果准确性度量标准"><a href="#实验结果准确性度量标准" class="headerlink" title="实验结果准确性度量标准"></a>实验结果准确性度量标准</h2><p>数据集：MovieLens数据集和该论文自己的一个demo系统。</p><p><font color="red">1.标准1</font>:imp = 1 / rp - 1 / rb;rp为新方法对目标资源的等级，rb为baseline方法对目标资源的等级。</p><p><font color="red">2.标准2</font>:HR：命中率计算方法</p><p><font color="red">3.标准3</font>:MRR：平均倒数评级 </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;论文题目&quot;&gt;&lt;a href=&quot;#论文题目&quot; class=&quot;headerlink&quot; title=&quot;论文题目&quot;&gt;&lt;/a&gt;论文题目&lt;/h2&gt;&lt;h3 id=&quot;Personalized-Search-by-Tag-based-User-Profiling-and-Resou
      
    
    </summary>
    
      <category term="Paper" scheme="https://github.com/DuncanZhou//categories/Paper/"/>
    
    
  </entry>
  
</feed>
