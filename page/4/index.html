<!doctype html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />





  <link rel="alternate" href="/atom.xml" title="Duncan's Blog" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="write something useful">
<meta property="og:type" content="website">
<meta property="og:title" content="Duncan's Blog">
<meta property="og:url" content="https://github.com/DuncanZhou/page/4/index.html">
<meta property="og:site_name" content="Duncan's Blog">
<meta property="og:description" content="write something useful">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Duncan's Blog">
<meta name="twitter:description" content="write something useful">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://github.com/DuncanZhou/page/4/"/>





  <title> Duncan's Blog </title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="">

  










  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Duncan's Blog</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle"></p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-主页">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            menu.主页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-分类">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            menu.分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-关于">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            menu.关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-归档">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            menu.归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-标签">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            menu.标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/08/10/BackTrackingNote/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/08/10/BackTrackingNote/" itemprop="url">
                  回溯法笔记
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-10T17:24:49+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="BackTracking-Algorithm-Notes"><a href="#BackTracking-Algorithm-Notes" class="headerlink" title="BackTracking Algorithm Notes"></a>BackTracking Algorithm Notes</h1><h2 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h2><blockquote>
<p>在那些涉及寻找一组解的问题或者求满足某些约束条件的最优解的问题中，有许多问题可以用回溯法来求解。</p>
</blockquote>
<p>为了应用回溯法，所要求的解必须能表示成一个n-元组（x1,x2,…,xn）,其中xi是取自某个有穷集Si。通常，所求解的问题需要求取一个使某一规范函数P(x1,…,xn)取极大值（或取极小值或满足该规范函数条件）的向量。有时还要找出满足规范函数P的所有向量。</p>
<h2 id="2-基本思想"><a href="#2-基本思想" class="headerlink" title="2.基本思想"></a>2.基本思想</h2><blockquote>
<p>不断地用修改过的规范函数（或限界函数）Pi(x1,…,xn)去测试正在构造中的n-元组的部分向量(x1,…,xn)，看其是否可能导致最优解，如果不能，那么将可能要测试的其余向量略去，就是剪枝。</p>
</blockquote>
<p>约束条件分为两类：显式约束和隐式约束。显示约束是限定每个x只从一个给定的集合上取值。隐式约束描述了xi必须彼此相关的情况，规定解空间中那些实际上满足规范函数的元组。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">Algorithm backTracking</div><div class="line">procedure PBACKTRACK(k)</div><div class="line">//此算法是对回溯法抽象地递归描述。进入算法时，解向量X(1:n)的前k-1个分量X(1),...,X(k-1)已赋值</div><div class="line">	global n,X(1:n)</div><div class="line">	for 满足下式的每个X(k)</div><div class="line">		X(k) 属于 T(X(1),...,X(k-1)) and B(X(1),...,X(k)) = True do</div><div class="line">		if (X(1),...,X(k)) 是一条已抵达答案结点的路径 then</div><div class="line">			print (X(1),...,X(k))</div><div class="line">		endif</div><div class="line">	call PBACKTRACK(k + 1)</div><div class="line">	repeat</div><div class="line">end PBACKTRACK</div></pre></td></tr></table></figure></p>
<h2 id="3-代表性的问题"><a href="#3-代表性的问题" class="headerlink" title="3.代表性的问题"></a>3.代表性的问题</h2><h3 id="a、n-queen问题"><a href="#a、n-queen问题" class="headerlink" title="a、n-queen问题"></a>a、n-queen问题</h3><p>有关n后问题的定义自行查阅，这里不给出解释。</p>
<p><font color="green" style="font-weight: bold;"><em>解决思路</em></font><br>假定皇后i将放在行i上，因此，8皇后问题可以表示出8-元组(x1,x2,…,x8)，其中xi是放置皇后i所在的列号。隐式约束条件：1）没有两个xi可以相同；2）没有两个皇后可以在同一条对角线<br>假设两个皇后在(i,j)和(k,l)位置，则在同在对角线公式为abs(j-l) = abs(i-k)。即，在PLACE函数中可用此公式来判断。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">procedure NQ(k)</div><div class="line">//X(1),...,X(k-1)已赋值，求X(k),...,X(n)</div><div class="line">	global X(1:n)</div><div class="line">	X(k) = 1</div><div class="line">	while X(k) &lt;= n do</div><div class="line">	//PLACE为检测第k个皇后是否满足约束条件</div><div class="line">		if PLACE(k) then</div><div class="line">			if k = n then </div><div class="line">				print (X(1),...,X(n))</div><div class="line">			else</div><div class="line">			//继续生成</div><div class="line">				NQ(k + 1)</div><div class="line">			endif</div><div class="line">		endif</div><div class="line">		X(k) = X(k) + 1</div><div class="line">	repeat</div><div class="line">end NQ</div></pre></td></tr></table></figure>
<h3 id="b、子集和数问题"><a href="#b、子集和数问题" class="headerlink" title="b、子集和数问题"></a>b、子集和数问题</h3><blockquote>
<p>定义：已知n+1个正数：Wi,1&lt;=i&lt;=n和M。要求找出Wi的和数为M的所有子集。例如，若n=4，(W1,W2,W3,W4)=(11,13,24,7),M=31，则满足要求的子集是(11,13,7)和(24,7)。值得指出的是，通过给出其和数为M的那些Wi的下标来表示解向量比直接用这些Wi表示解向量更为方便。因此这个解向量由(1,2,4)和(3,4)表示。</p>
</blockquote>
<p>显示约束条件要求Xi在1~n之间。隐式约束条件则是要求没有两个Xi是相同的且相应的Wi的和数是M。(为了避免产生同一个子集重复的情况,如(1,4,2)和(1,2,4))，附加另一个隐式约束条件:Xi+1大于Xi,i在1~n之间。</p>
<p><font color="green" style="font-weight: bold;"><em>解决思路</em></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">//current代表当前集合中已有和，k表示当前判断的第k个元素，rest代表集合中剩余元素之和</div><div class="line">public void sumOfsubset(int current,int k,int rest)&#123;</div><div class="line">    //先生成左儿子,w[k]加入</div><div class="line">    flag[k] = 1;</div><div class="line">    if(current + w[k] == M)&#123;</div><div class="line">        System.out.print(&quot;&#123;\t&quot;);</div><div class="line">        //子集找到，输出</div><div class="line">        print(k);</div><div class="line">        System.out.println();</div><div class="line">    &#125;</div><div class="line">    else if(current + w[k] + w[k + 1] &lt;= M)</div><div class="line">        sumOfsubset(current + w[k],k + 1,rest - w[k]);</div><div class="line">    //生成右儿子,w[k]不加入</div><div class="line">    if((current + rest - w[k] &gt;= M) &amp;&amp; (current + w[k + 1] &lt;= M))&#123;</div><div class="line">        flag[k] = 0;</div><div class="line">        sumOfsubset(current,k + 1,rest - w[k]);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h3 id="c、图的着色"><a href="#c、图的着色" class="headerlink" title="c、图的着色"></a>c、图的着色</h3><blockquote>
<p>定义：已知一个图G和m&gt;0种颜色，在只准使用这m种颜色对G的结点着色的情况下，是否能使图中任何相邻的两个结点都具有不同的颜色。这个问题称为m-着色判定问题。</p>
</blockquote>
<p><font color="green" style="font-weight: bold;"><em>解决思路</em></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">   	procedure MCOLORING(k)</div><div class="line">	global integer m,n,X(1:n),boolean Graph(1:n,1:n)</div><div class="line">	integer k</div><div class="line">	loop</div><div class="line">		//给第k个结点赋值颜色</div><div class="line">		call NextValue(k)</div><div class="line">		if X(k) = 0 then</div><div class="line">			exit</div><div class="line">		endif</div><div class="line">		if k = n then</div><div class="line">			print(X)</div><div class="line">		else</div><div class="line">			call MCOLORING(k + 1)</div><div class="line">		endif</div><div class="line">	repeat</div><div class="line">end MCOLORING</div></pre></td></tr></table></figure></p>
<h3 id="d、哈密顿环"><a href="#d、哈密顿环" class="headerlink" title="d、哈密顿环"></a>d、哈密顿环</h3><blockquote>
<p>定义：一个哈密顿环是一条沿着图G的n条边环行的路径，它访问每个结点一次并且返回到它的开始位置。</p>
</blockquote>
<h3 id="e、0-1背包问题"><a href="#e、0-1背包问题" class="headerlink" title="e、0/1背包问题"></a>e、0/1背包问题</h3><p>定义不解释，这个问题解决的方案很多，可以用动归、贪心算法，这里使用回溯法求解。<br>代码如下：</p>
<p><font color="green" style="font-weight: bold;"><em>解决思路</em></font><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div></pre></td><td class="code"><pre><div class="line">package Chapter8;</div><div class="line">//Created by duncan on 16-12-14.</div><div class="line">public class BPByBackTrack &#123;</div><div class="line">    int bestvalue = 0;</div><div class="line">    int[] lastfalg;</div><div class="line">    public void swap(int[] x,int m,int n)&#123;</div><div class="line">        int temp;</div><div class="line">        temp = x[m];</div><div class="line">        x[m] = x[n];</div><div class="line">        x[n] = temp;</div><div class="line">    &#125;</div><div class="line">    //先将物品效益和重量按照P/W的大小非递减排序,p和w都从1开始存储</div><div class="line">    public void sort(int[] p,int[] w)&#123;</div><div class="line">        int len = p.length -1 ;</div><div class="line">        for(int i = 1; i &lt;= len; i++)&#123;</div><div class="line">            for(int j = 1; j &lt;= len - i; j++)&#123;</div><div class="line">                    if(((float)p[j] / w[j]) &gt; ((float)p[j + 1] / w[j + 1]))&#123;</div><div class="line">                        swap(p,j,j + 1);</div><div class="line">                        swap(w,j,j + 1);</div><div class="line">                    &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    //限界函数,k为上次去掉的物品，M为背包容量,currentp为当前背包中效益值，currentw为背包中当前重量，返回值为当前最大值上限</div><div class="line">    public int Bound(int[] p,int[] w,int k,int M,int currentp,int currentw)&#123;</div><div class="line">        int tempp = currentp,tempw = currentw;</div><div class="line">        int len = p.length - 1;</div><div class="line">        for(int i = k + 1; i &lt;= len; i++)&#123;</div><div class="line">            tempw += w[i];</div><div class="line">            if(tempw &lt; M)</div><div class="line">                tempp += p[i];</div><div class="line">            else</div><div class="line">                return (tempp + (1 - (tempw - M) / w[i] * p[i]));</div><div class="line">        &#125;</div><div class="line">        return tempp;</div><div class="line">    &#125;</div><div class="line">    //回溯法求解背包问题,p,w为效益值和重量数组，M为背包容量，k为当前处理的物品，flag为记录物品放或不放的标志数组,currentp为当前效益，currentw为当前重量</div><div class="line">    public void BKNAPBT(int[] p, int[] w,int M,int k,int[] flag,int currentp,int currentw)&#123;</div><div class="line">        int n = p.length - 1;</div><div class="line">        if(k &gt; n)&#123;</div><div class="line">                if(currentp &gt; bestvalue) &#123;</div><div class="line">                    bestvalue = currentp;</div><div class="line">                    lastfalg = flag.clone();</div><div class="line">                    return;</div><div class="line">                &#125;</div><div class="line">        &#125;</div><div class="line">        else&#123;</div><div class="line">            //进入左子树</div><div class="line">            if(currentw + w[k] &lt;= M)</div><div class="line">            &#123;</div><div class="line">                flag[k] = 1;</div><div class="line">                BKNAPBT(p,w,M,k + 1,flag,currentp + p[k],w[k] + currentw);</div><div class="line">            &#125;</div><div class="line">            //进入右子树前先判断右子树的最大上限是否能够比当前最优值大，如果没有则减去右子树</div><div class="line">            if(Bound(p,w,k,M,currentp,currentw) &gt;= currentp) &#123;</div><div class="line">                flag[k] = 0;</div><div class="line">                BKNAPBT(p, w, M, k + 1, flag, currentp, currentw);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    public void print(int[] p, int[] w)&#123;</div><div class="line">        int len = p.length -1;</div><div class="line">        int m = 0,v = 0;</div><div class="line">        System.out.println(&quot;\n最终放入背包的物品的价值为:&quot;);</div><div class="line">        for(int i = 1; i &lt;= len; i++)&#123;</div><div class="line">            if(lastfalg[i] == 1) &#123;</div><div class="line">                m += w[i];</div><div class="line">                v += p[i];</div><div class="line">                System.out.print(p[i] + &quot; &quot;);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        System.out.println(&quot;\n\n最终重量为：&quot; + m);</div><div class="line">        System.out.println(&quot;\n最优解价值为:&quot; + v);</div><div class="line">    &#125;</div><div class="line">    public static void main(String[] args) &#123;</div><div class="line">        int m = 0,M = 110;</div><div class="line">        BPByBackTrack bp = new BPByBackTrack();</div><div class="line">        int[] w = &#123;0,1,11,21,23,33,43,45,55&#125;;</div><div class="line">        int[] p = &#123;0,11,21,31,33,43,53,55,65&#125;;</div><div class="line">        int[] flag = &#123;0,0,0,0,0,0,0,0,0&#125;;</div><div class="line">        System.out.println(&quot;物品价值为&quot;);</div><div class="line">        for(int i = 1 ; i &lt;= p.length -1 ;i++)&#123;</div><div class="line">            System.out.print(p[i] + &quot;\t&quot;);</div><div class="line">        &#125;</div><div class="line">        System.out.println(&quot;\n物品重量为&quot;);</div><div class="line">        for(int i = 1 ; i &lt;= p.length -1 ;i++)&#123;</div><div class="line">            System.out.print(w[i] + &quot;\t&quot;);</div><div class="line">        &#125;</div><div class="line">        System.out.println();</div><div class="line">        bp.sort(p,w);</div><div class="line">        bp.BKNAPBT(p,w,M,1,flag,0,0);</div><div class="line">        System.out.println(&quot;背包容量为:&quot; + M);</div><div class="line">        bp.print(p,w);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/08/10/Exploring Folksonomy for Personalized Search/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/08/10/Exploring Folksonomy for Personalized Search/" itemprop="url">
                  Personalized Search论文阅读笔记-08年SIGIR
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-08-10T17:24:49+08:00">
                2018-08-10
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper/" itemprop="url" rel="index">
                    <span itemprop="name">Paper</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="论文题目"><a href="#论文题目" class="headerlink" title="论文题目"></a>论文题目</h2><h3 id="Exploring-Folksonomy-for-Personalized-Search"><a href="#Exploring-Folksonomy-for-Personalized-Search" class="headerlink" title="Exploring Folksonomy for Personalized Search"></a>Exploring Folksonomy for Personalized Search</h3><h2 id="概念解释"><a href="#概念解释" class="headerlink" title="概念解释"></a>概念解释</h2><p><strong>Folksonomy:</strong></p>
<blockquote>
<p>该单词由folk和taxonomy组成，folk是口语中伙伴的意思，taxonomy是分类方法的意思，该词用来表示现在存在的大众分类的一种现象。目前许多应用允许用户上传资源时选择标签标注该资源。现有的应用如flickr和dogear。</p>
</blockquote>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>对于这样允许大众分类的应用，如何满足用户在搜索时尽可能准确地返回用户所需要的资源是一个有意思的问题。因为如果像传统的搜索方法仅通过查询关键词去匹配搜索结果，返回的结果可能会不满足用户的初衷。而且，不同的用户在搜索不同的资源时有可能会使用同样的关键词，比如，爱好运动和爱好喝咖啡的用户在搜索杯子的时候使用的关键词都可能是“杯子”，而返回的结果对于爱好运动的用户来说应该尽可能是运动型杯子，对于爱好喝咖啡的用户来说应该尽可能是咖啡杯子。所以，这里的问题都归结于Personalized Search。</p>
<h2 id="Solution"><a href="#Solution" class="headerlink" title="Solution"></a>Solution</h2><p>根据之前的工作，解决这样问题的方法有两类:<font color="red">Query Refinement</font>和<font color="red">Result Processing</font> </p>
<blockquote>
<p>1)对于查询术语进行替换和扩展，替换成其他的术语或者用其他的术语来填充<br>2)对查询返回的结果再排序或者结果聚类等方法<br>使用较为普遍的是Result Processing，该篇论文中使用的也是第二种方法。</p>
</blockquote>
<p>本文中提出的方法分为三个步骤：<br>1)Query中的terms先求结果排序得到Ranklist<br>2)通过User的Interest Vector和资源的Topic Vector求相似性得到Ranklist<br>3)聚合两个list得到最终的Ranklist<br>(具体在实验过程中，为了提高效率，第二步求Ranklist只对第一步中的前N个再去求排序)</p>
<h2 id="Details-Of-Methods"><a href="#Details-Of-Methods" class="headerlink" title="Details-Of-Methods"></a>Details-Of-Methods</h2><p><font color="red"><strong>Step1：</strong></font><br>对于Query中的terms去求资源排序，本文中采用<strong>BM25</strong>和<strong>Language Model for IR(LMIR)</strong>这两种文本检索模型。<br>先通过基本的文本检索去得到第一步的Ranklist。</p>
<p><font color="red"><strong>Step2：</strong></font><br>第二步是本文的核心。关键在于建立Topic Space,因为建立了Topic Space后，才能对用户建立兴趣向量，才能对资源建立主题向量，然后再去计算两者之间的相似性。</p>
<p>1.本文中的主题空间使用了Folksonomy的方法，以标注的tag作为向量的每一维，每个维的值的计算方法可以通过tfidf或者BM25来计算，从而构成用户和资源的兴趣和topic向量。<br>此外，本文中以ODP分好类的web pages的topic space作为baseline方法。</p>
<p>2.当分别构建好用户和资源的向量后，本文通过类PageRank算法来迭代形成最终的用户和资源的矩阵。R矩阵行代表用户，列代表兴趣。T矩阵行代表资源，列代表topic。</p>
<p><font color="red"><strong>Step3：</strong></font><br>本文中聚合两个list的方法是通过简单的WBF方法去得到最终的Ranklist</p>
<h2 id="Evaluation-Methods"><a href="#Evaluation-Methods" class="headerlink" title="Evaluation-Methods"></a>Evaluation-Methods</h2><p>本文基于一个假设：用户收藏过或标注过的资源即为用户相关资源，以此来通过检索评价标准来评价Personalized Search。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/05/23/ccx/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/05/23/ccx/" itemprop="url">
                  ccx
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-05-23T11:22:39+08:00">
                2018-05-23
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Competition/" itemprop="url" rel="index">
                    <span itemprop="name">Competition</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="金融建模比赛记录"><a href="#金融建模比赛记录" class="headerlink" title="金融建模比赛记录"></a>金融建模比赛记录</h2><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>对于A训练集（有标签）:</p>
<ul>
<li>1.数据分散在四个文件内,train_behavior,train_ccx,train_consumer,train_target,各个数据文件的解释大赛excel表格中已有.</li>
<li>2.需要根据ccx_id将每个文件中的数据进行聚合, 聚合之前可以先在每个文件中提取特征.</li>
<li>对于每个文件内</li>
</ul>
<ul>
<li>train_behavior（基础信息+行为数据）:一共2270维特征，对其中(1)唯一值列去除—共去除23列;（2）对于缺失90%值的列进行去除;(3)对于包含空值且只有两种值的列进行去除;(类别值的列:’var3’, u’var4’, u’var5’, u’var6’, u’var11’, u’var12’, u’var13’, u’var14’, u’var15’, u’var18’, u’var19)。<strong>最终得到336维特征</strong>(3)去除时间列(2列)</li>
<li>train_consumer(消费数据): 用户的消费记录,提取了\<categorical列的次数,消费次数,消费总额,平均每次消费金额,消费最大金额,消费最小金额,最大金额与最小金额差\></categorical列的次数,消费次数,消费总额,平均每次消费金额,消费最大金额,消费最小金额,最大金额与最小金额差\></li>
<li>train_ccx(查询记录)：用户的查询记录,提取了”查询次数”以及categorical列的次数</li>
</ul>
<p>初步结果(模型都是默认参数)</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">features/models(AUC)</th>
<th style="text-align:center">lightgbm</th>
<th style="text-align:center">lr</th>
<th style="text-align:center">gbdt</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">behavior</td>
<td style="text-align:center">0.588</td>
<td style="text-align:center">0.566</td>
<td style="text-align:center">0.578</td>
</tr>
<tr>
<td style="text-align:center">behavior+consuming</td>
<td style="text-align:center">0.626</td>
<td style="text-align:center">0.563</td>
<td style="text-align:center">0.581</td>
</tr>
<tr>
<td style="text-align:center">behavior+consuming+ccx</td>
<td style="text-align:center">0.639</td>
<td style="text-align:center">0.592</td>
<td style="text-align:center">0.603</td>
</tr>
</tbody>
</table>
</div>
<p>对于B训练集(无标签)</p>
<blockquote>
<p>该问题属于半监督学习,半监督学习分为<strong>纯半监督学习</strong>和<strong>直推学习</strong>.</p>
</blockquote>
<ul>
<li><strong>纯半监督学习</strong>:是将<strong>未标记数据和有标记数据都作为训练集来训练</strong>,得到模型,来预测<strong>待测数据</strong></li>
<li><strong>直推学习</strong>:是将<strong>未标记数据作为需要预测的对象</strong>,通过有标记数据进行训练,来预测.</li>
</ul>
<p>解决思路:</p>
<ul>
<li>1.<strong>聚类</strong>将A和B合并聚为两类,用该聚类簇中A标签投票标记B(否决)</li>
<li>2.<strong>自训练方法</strong>,先训练A得到一个分类模型,然后通过分类模型分类B,将置信度高的进行标记,然后加入训练集,训练-&gt;标记置信度高的,迭代.(尝试)</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/04/25/Crawler/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/25/Crawler/" itemprop="url">
                  Crawler
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-25T10:06:44+08:00">
                2018-04-25
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Crawler/" itemprop="url" rel="index">
                    <span itemprop="name">Crawler</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <blockquote>
<p>由于论文需要补充数据集,现抓取微博上演员,歌手,导演,运动员和普通用户共1w个.包括他们的基本信息和粉丝和朋友关系.</p>
</blockquote>
<hr>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><p>(不考虑多线程)</p>
<ul>
<li><p>1.安装依赖的库: <strong>requests,selenium,BeautifulSoup</strong></p>
</li>
<li><p>2.分析页面,从微博搜索框输入相应领域,获得分页的结果页面,从结果页面提取用户的id.</p>
</li>
<li><p>3.由于返回的结果页面是异步加载,通过<strong>selenium</strong>模拟浏览器访问,抓取返回的结果页面上的id.(需要对selenium<strong>添加请求头信息</strong>)</p>
</li>
<li><p>4.抓取到用户id后,可通过weibo API抓取其基本信息和关系信息.</p>
</li>
</ul>
<p>(在抓取用户的关注时,使用多线程)</p>
<ul>
<li>5.python多线程模块threading,因为是I/O密集型,所以用多线程</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/04/17/ProbabilityTheory/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/04/17/ProbabilityTheory/" itemprop="url">
                  ProbabilityTheory
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-04-17T10:03:51+08:00">
                2018-04-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Data-Mining/" itemprop="url" rel="index">
                    <span itemprop="name">Data Mining</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="概率论相关公式整理如下"><a href="#概率论相关公式整理如下" class="headerlink" title="概率论相关公式整理如下:"></a>概率论相关公式整理如下:</h1><hr>
<h3 id="第二章-基本概念"><a href="#第二章-基本概念" class="headerlink" title="第二章 基本概念"></a>第二章 基本概念</h3><ul>
<li>交换律:A + B = B + A,AB=BA</li>
<li>结合律:(A+B)+C=A+(B+C)=A+B+C,(AB)C=A(BC)=ABC</li>
<li>分配律:(A+B)C=AC+BC,AB+C = (A+C)(B+C)</li>
<li>德摩根律: $\overline{A+B}=\bar{A}\bar{B}$,$\overline{AB}=\bar{A}+\bar{B}$</li>
<li>P(A-B) = P(A)-P(AB)</li>
<li>P(A+B) = P(A) + P(B) - P(AB)</li>
<li>乘法概率公式: 若P(B)&gt;0,$P(AB)=P(B)P(A|B)$.若P(A)&gt;0,$P(AB)=P(A)P(B|A)$.<br>一般地,$P(A_1A_2…A_{n-1})&gt;0$,则$P(A_1A_2…A_n)=P(A_1)P(A_2|A_1)P(A_3|A_2A_1)…P(A_n|A_1A_2…A_{n-1})$</li>
<li>全概率公式: $P(B)=\sum_{i=1}^{n}P(A_i)P(B|A_i)$</li>
<li>贝叶斯概率公式: $P(B|A) = \frac{P(B)P(A|B)}{P(A)}$</li>
</ul>
<h3 id="第三章-分布"><a href="#第三章-分布" class="headerlink" title="第三章 分布"></a>第三章 分布</h3><p>1.<strong>离散型分布</strong></p>
<ul>
<li><p>1.0-1分布 $X\sim B(1,p)$</p>
<script type="math/tex; mode=display">P(X=k)=p^k(1-p)^{1-k}(0<p<1,k=0,1)</script></li>
<li><p>2.二项分布 $X \sim B(n,p)$</p>
<script type="math/tex; mode=display">P(X=k)=C\_n^kp^kq^{n-k}(k=0,1,2,...,n)(0<p<1,q=1-p)</script></li>
<li><p>3.泊松分布 $X \sim P(\lambda)$</p>
<script type="math/tex; mode=display">P(X=k)=\frac{\lambda^ke^{-\lambda}}{k!}(k=0,1,2,...)</script></li>
</ul>
<p>2.<strong>连续型分布</strong></p>
<ul>
<li><p>1.均匀分布 $X \sim U[a,b]$</p>
<script type="math/tex; mode=display">f(x)=\begin{cases}
\frac{1}{b-a} & a\leq x\leq b \\ 
0 & others
\end{cases}</script></li>
<li><p>2.指数分布 $X \sim E(\lambda)$</p>
<script type="math/tex; mode=display">f(x)=\begin{cases}
\lambda e^{-\lambda x} & x > 0 \\ 
0 & x \leq 0
\end{cases}</script><p>$\lambda&gt;0$</p>
</li>
<li><p>3.正态分布 $X\sim N(\mu,\sigma^2)$</p>
<script type="math/tex; mode=display">f(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-\mu)^2}{2\sigma^2}},-\infty < x < +\infty</script></li>
</ul>
<h3 id="第四章-随机变量的特征"><a href="#第四章-随机变量的特征" class="headerlink" title="第四章 随机变量的特征"></a>第四章 随机变量的特征</h3><p>1.<strong>期望概念</strong></p>
<ul>
<li>离散型: $E(X)=\sum_{i=1}^{\infty}x_ip_i$</li>
<li>连续型: 设连续型随机变量X的概率密度函数为f(x),若积分$\int_{-\infty}^{+\infty}|x|f(x)dx&lt;+\infty$存在,并称积分$\int_{-\infty}^{+\infty}xf(x)dx$为X的数学期望,记为E(X),即$E(X)=\int_{-\infty}^{+\infty}xf(x)dx$</li>
</ul>
<p>2.<strong>期望性质</strong></p>
<ul>
<li>E(c) = c, 其中c为常数</li>
<li>E(cX) = cE(X), 其中c为常数</li>
<li>E(X+Y) = E(X) + E(Y)</li>
<li>若X,Y相互独立,E(XY) = E(X)E(Y)</li>
</ul>
<p>3.<strong>方差概念</strong><br>$D(X)=E(X^2)-[E(X)]^2$</p>
<p>4.<strong>方差性质</strong></p>
<ul>
<li>D(c) = 0, 其中c为常数</li>
<li>$D(cX) = c^2D(X)$, 其中c为常数</li>
<li>若X,Y相互独立, D(X+Y) = D(X) + D(Y)</li>
</ul>
<p>5.<strong>协防差</strong><br>$Cov(X,Y) = E{[X-E(X)][Y-E(Y)]}$</p>
<p>6.<strong>相关系数</strong><br>$\rho_{XY}=\frac{Cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}$</p>
<p>7.<strong>协防差和相关系数性质</strong></p>
<ul>
<li>Cov(X,Y) = Cov(Y,X)</li>
<li>Cov(aX,bY) = abCov(X,Y), a,b为常数</li>
<li>$Cov(X_1+X_2,Y) = Cov(X_1,Y) + Cov(X_2,Y)$</li>
<li>D(X+Y) = D(X) + D(Y) + 2Cov(X,Y)</li>
<li>Cov(X,Y) = E(XY) - E(X)E(Y)</li>
<li>$|\rho_{XY}| \leq 1$</li>
<li>若X,Y相互独立,则$\rho_{XY}=0$</li>
<li>$\rho_{XY}=\pm$的充要条件是存在两个常数a,b,且$a\neq0$,使得$P{Y=aX+b}=1$.</li>
</ul>
<h3 id="第五章-大数定律和中心极限定理"><a href="#第五章-大数定律和中心极限定理" class="headerlink" title="第五章 大数定律和中心极限定理"></a>第五章 大数定律和中心极限定理</h3><p>1.<strong>契比雪夫不等式</strong>: 设随机变量X的数学期望为E(X)=a,方差为D(X),则对于给定的数$\epsilon&gt;0$,有</p>
<script type="math/tex; mode=display">P\{|X-a|\geq \epsilon\}\leq \frac{D(X)}{\epsilon^2}</script><p>2.<strong>大数定律</strong>: 设{X<sub>n</sub>}为一随机变量序列,a为一个常数,如果对任何给定的正数$\epsilon$,有$\lim_{n \to \infty}P{|X_n-a|\geq \epsilon}=0$,则称随机变量序列{X<sub>n</sub>}依概率收敛于a,记为<script type="math/tex">X\_n \overset{P}{\rightarrow}a(n \to \infty)</script>.</p>
<p>3.<strong>契比雪夫大数定律</strong>: 设{X<sub>n</sub>}为一随机变量序列,若对于所有的自然数n,数学期望E(X<sub>n</sub>)及方差D(X<sub>n</sub>)均存在,且存在某常数M&gt;0,使得D(X<sub>n</sub>)$\leq M$,则有<script type="math/tex">\frac{1}{n}\sum\_{i=1}^{n}[X\_i-E(X\_i)]\overset{P}{\rightarrow}0</script>.</p>
<p>4.<strong>贝努里大数定律</strong>: 在n次重复独立试验中,设Y<sub>n</sub>为事件A发生的次数,每次试验事件A发生的概率为P,则<script type="math/tex">\frac{Y\_n}{n} \overset{P}{\rightarrow}P(n \to \infty)</script>.</p>
<p>5.<strong>辛钦大数定律</strong>: 设{X<sub>n</sub>}为独立同分布的随机变量序列,且具有数学期望E(X<sub>i</sub>)=$\mu,i=1,2,…$,则<script type="math/tex">\frac{1}{n}\sum\_{i=1}^{n}X\_i\overset{P}{\rightarrow}\mu(n \to \infty)</script>.</p>
<p>6.<strong>中心极限定理</strong>: 设{X<sub>n</sub>}为独立同分布的随机变量序列,且E(X<sub>i</sub>)=$\mu$,D(X<sub>i</sub>)=$\sigma^2\neq0,i=1,2,…$,则当n充分大时,$\frac{\sum_{i=1}^{n}X_i-E(\sum_{i=1}^{n}X_i)}{\sqrt{D(\sum_{k=1}^{n}X_k)}}$近似地服从标准正态分布,记作<script type="math/tex">\frac{\sum\_{i=1}^{n}X\_i-E(\sum\_{i=1}^{n}X\_i)}{\sqrt{D(\sum\_{k=1}^{n}X\_k)}}=\frac{\sum\_{i=1}^{n}X\_k-n\mu}{\sqrt{n}\sigma}\sim N(0,1)</script>.</p>
<h3 id="第六章-数理统计概念"><a href="#第六章-数理统计概念" class="headerlink" title="第六章 数理统计概念"></a>第六章 数理统计概念</h3><p>1.<strong>统计量</strong></p>
<ul>
<li>样本均值: $\bar{X}=\frac{1}{n}\sum_{i=1}^{n}X_i$</li>
<li>样本方差: $S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2=\frac{1}{n-1}[\sum_{i=1}^{n}X_i^2-n(\bar{X})^2]$ </li>
<li>样本标准差: $S=\sqrt{S^2}$</li>
<li>样本k阶原点矩 $A_k=\frac{1}{n}\sum_{i=1}^{n}X_i^k(k=1,2,..)$ </li>
<li>样本k阶中心矩 $B_k=\frac{1}{n}\sum_{i=1}^{n}(X_i-\bar{X})^k$</li>
</ul>
<p>2.<strong>抽样分布</strong><br>卡方分布,F分布,正态分布</p>
<h3 id="第七章-参数估计"><a href="#第七章-参数估计" class="headerlink" title="第七章 参数估计"></a>第七章 参数估计</h3><p>1.<strong>矩估计</strong>: 概括来讲就是用样本矩估计总体矩(原点矩).<br>2.<strong>极大似然估计法</strong></p>
<ul>
<li>离散型:概率连乘求极大</li>
<li>连续型:概率密度函数连乘求偏导</li>
</ul>
<p>3.<strong>估计量的评价标准</strong>:待完善</p>
<p>4.<strong>区间估计</strong>:待完善</p>
<h3 id="第八章-假设检验"><a href="#第八章-假设检验" class="headerlink" title="第八章 假设检验"></a>第八章 假设检验</h3><ul>
<li>1.建立原假设H<sub>0</sub>(备选假设H<sub>1</sub>)</li>
<li>2.根据检验对象,构造适当的统计量g(X<sub>1</sub>,X<sub>2</sub>,…,X<sub>n</sub>)</li>
<li>3.在H<sub>0</sub>成立的条件下,确定统计量g(X<sub>1</sub>,X<sub>2</sub>,…,X<sub>n</sub>)的分布</li>
<li>4.由显著性水平$\alpha$确定临界值,从而得到拒绝域或接受域</li>
<li>5.根据样本值计算统计量的观测值,由此作出接受原假设或拒绝原假设的结论</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/03/29/Models/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/29/Models/" itemprop="url">
                  记录几个经典模型
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-29T09:35:21+08:00">
                2018-03-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h3 id="参考网络博客和个人理解记录如下"><a href="#参考网络博客和个人理解记录如下" class="headerlink" title="参考网络博客和个人理解记录如下:"></a>参考网络博客和个人理解记录如下:</h3><hr>
<h3 id="1-GBDT-Gradient-Boosting-Decision-Tree"><a href="#1-GBDT-Gradient-Boosting-Decision-Tree" class="headerlink" title="1.GBDT(Gradient Boosting Decision Tree)"></a><font color="red">1.GBDT(Gradient Boosting Decision Tree)</font></h3><p><strong>1.优势</strong></p>
<ul>
<li>效果还不错</li>
<li>既可用于分类也可用于回归</li>
<li>可以筛选特征</li>
</ul>
<p><strong>2.关键点</strong></p>
<p><strong>2.1 gbdt 的算法的流程？</strong><br>gbdt通过多轮迭代,每轮迭代生成一个弱分类器,每个分类器在上一轮分类器的残差基础上进行训练.(<strong>弱分类器一般会选择CART TREE - 分类回归树</strong>)</p>
<p><strong>最终的总分类器是将每轮训练得到的弱分类器加权求和得到. - 加法模型</strong></p>
<p>模型最终可描述为:$F_M(x)=\sum_{m=1}^{M}T(x;\theta_{m})$<br>模型一共训练M轮,每轮产生一个弱分类器$T(x;\theta_m)$,弱分类器的损失函数<script type="math/tex">\hat{\theta}\_m=argmin\_{\theta\_m}\sum\_{i=1}^{N}L\{y\_i,F\_{m-1}(x\_i)+T\_m(x\_i;\theta\_m)\}</script></p>
<p>gbdt在每轮迭代的时候,都去拟合损失函数在当前模型下的负梯度.<br><strong>2.2 gbdt 如何选择特征 ？</strong><br>原始的gbdt做法非常暴力,首先<strong>遍历每个特征</strong>,然后<strong>对每个特征遍历它所有可能的切分点</strong>,找到最优特征m的最优切分点j.</p>
<p><strong>2.3 gbdt 如何构建特征 ？</strong><br>工业界做法是和<strong>逻辑回归结合</strong>,得到组合特征.</p>
<p><strong>2.4 gbdt 如何用于分类？</strong><br><img src="https://raw.githubusercontent.com/DuncanZhou/images/master/gbdt-multiclassifier.png" alt="gbdt多分类"></p>
<p>对于多分类任务,GBDT的做法采用<strong>一对多</strong>的策略.一共有K个类别,训练M轮,每一轮都训练K个树,训练完成后一共有M*K个树.<strong>损失函数log loss</strong></p>
<p><strong>2.5 gbdt 通过什么方式减少误差 ？</strong><br>拟合残差,梯度下降</p>
<p><strong>2.6 gbdt的效果相比于传统的LR，SVM效果为什么好一些 ？</strong></p>
<ul>
<li>1.结合了多个弱分类器,是集成学习,所以泛化能力和准确率更高</li>
<li>2.SVM对于训练集不同的维度,数据量的大小,核函数的选择直接决定了模型的训练效果.gbdt相较于SVM和LR更不容易过拟合,因为它的超参学习能力较好,gbdt的泛化能力更多取决于数据集.</li>
</ul>
<p><strong>2.7 gbdt的参数有哪些，如何调参 ？</strong><br><strong>1.框架参数</strong></p>
<ul>
<li>步长 - 选择一个较大的步长</li>
<li>迭代次数或者说学习器的个数 - 100左右</li>
<li>学习率$\eta$</li>
<li>损失函数 - 分类问题和回归问题不一样(分类问题有对数似然和指数似然函数;回归模型有均方误差,绝对损失,Huber损失和分位数损失)</li>
</ul>
<p><strong>2.弱学习器参数</strong></p>
<ul>
<li>树的深度 - 10-100</li>
<li>最大特征数 - 划分时考虑的最大特征数</li>
<li>最小叶子结点样本数</li>
<li>最大叶子结点个数 - 限制最大叶子结点数,防止过拟合</li>
</ul>
<p><strong>2.8 gbdt的优缺点 ？</strong><br><strong>1.优点</strong></p>
<ul>
<li>泛化能力强,不容易过拟合</li>
<li>不需要复杂的特征工程</li>
</ul>
<p><strong>2.缺点</strong></p>
<ul>
<li>难以实行并行化</li>
<li>模型复杂度较高,深入分析和调优有一定难度</li>
</ul>
<h3 id="2-XgBoost-Extreme-Gradient-Boosting"><a href="#2-XgBoost-Extreme-Gradient-Boosting" class="headerlink" title="2.XgBoost(Extreme Gradient Boosting)"></a><font color="red">2.XgBoost(Extreme Gradient Boosting)</font></h3><p><strong>1.xgboost和GBDT区别</strong></p>
<ul>
<li>传统GBDT以CART作为基分类器,<strong>xgboost还支持线性分类器.</strong></li>
<li>传统GBDT在优化时只用到<strong>一阶导数信息</strong>,而<strong>xgboost进行了二阶泰勒展开</strong></li>
<li>xgboost在代价函数中<strong>加入了正则项</strong></li>
<li>对于<strong>缺失值的处理</strong>,xgboost可以自动学习出它的分裂方向</li>
<li>xgboost支持并行,<strong>并行过程是在确定最佳分割点时</strong>,每一轮的训练还是前向分步法,这个过程不能并行.选择最佳分割点时使用近似直方图算法</li>
</ul>
<h3 id="3-SVM-Support-Vector-Machine"><a href="#3-SVM-Support-Vector-Machine" class="headerlink" title="3.SVM(Support Vector Machine)"></a><font color="red">3.SVM(Support Vector Machine)</font></h3><p>参考该篇博客: <a href="https://blog.csdn.net/szlcw1/article/details/52259668" target="_blank" rel="external">https://blog.csdn.net/szlcw1/article/details/52259668</a> (谢谢作者整理)</p>
<h3 id="4-CNN-Convolutional-Neural-Network"><a href="#4-CNN-Convolutional-Neural-Network" class="headerlink" title="4.CNN(Convolutional Neural Network)"></a><font color="red">4.CNN(Convolutional Neural Network)</font></h3>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/03/20/Leetcode/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/20/Leetcode/" itemprop="url">
                  Leetcode
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-20T23:26:03+08:00">
                2018-03-20
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>刷题leetcode题解: <a href="https://github.com/DuncanZhou/LeetCodePractice">https://github.com/DuncanZhou/LeetCodePractice</a><br>大约有300多道,如有错误,欢迎指教,邮箱链接: ymzhou@stu.suda.edu.cn</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/03/17/StatisticLearning/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/17/StatisticLearning/" itemprop="url">
                  StatisticLearning
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-17T15:49:11+08:00">
                2018-03-17
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="lt-统计学习方法-gt-李航"><a href="#lt-统计学习方法-gt-李航" class="headerlink" title="&lt;统计学习方法&gt; - 李航"></a>&lt;统计学习方法&gt; - 李航</h1><hr>
<p><strong>重在推导过程,简单记录一些细节</strong></p>
<h3 id="第一章-统计学习方法概论"><a href="#第一章-统计学习方法概论" class="headerlink" title="第一章 统计学习方法概论"></a>第一章 统计学习方法概论</h3><p>1.泛化误差/期望损失(风险函数):是理论模型f(X)<strong>关于联合分布P(X,Y)</strong>的平均意义下的损失.</p>
<p>2.训练误差(经验风险/经验损失):是模型f(X)关于训练数据集的平均损失</p>
<p>3.根据大数定律,<strong>当样本容量N趋于无穷时,经验风险趋于期望风险</strong>,所以一般用经验风险估计期望风险.但现实中训练样本数目有限,所以对经验风险要进行一定的矫正.<strong>经验风险最小化和结构风险最小化(正则化)</strong></p>
<p>4.过拟合解决方案:</p>
<ul>
<li>正则化</li>
<li>交叉验证<ul>
<li>简单交叉验证</li>
<li>K-Fold交叉验证</li>
<li>留一交叉验证</li>
</ul>
</li>
</ul>
<p>5.生成方法和判别方法比较</p>
<ul>
<li><strong>生成方法</strong>:由数据<strong>学习联合概率分布P(X,Y),然后求出条件概率分布P(Y|X)</strong>作为预测模型,即生成模型$P(Y|X)=\frac{P(X,Y)}{P(X)}$.</li>
<li><strong>判别方法</strong>:由数据<strong>直接学习决策函数f(X)或者条件概率分布P(Y|X)</strong>作为预测的模型.</li>
<li>两者区别:<ul>
<li>生成方法可以还原出联合概率分布,而判别方法不能;生成方法的学习收敛速度更快.</li>
<li>判别方法直接学习的式条件概率或决策函数,直接面对预测,往往学习的准确率更高.可以对数据进行各种程度上的抽象,定义特征并使用特征,简化学习问题.</li>
</ul>
</li>
</ul>
<p>6.回归问题按照输入变量的个数分为<strong>一元回归和多元回归</strong>;按照输入变量和输出变量之间关系的类型即模型的类型,分为<strong>线性回归和非线性回归</strong>.</p>
<p>7.回归学习最常用的损失函数是<strong>平方损失函数</strong> - <strong>最小二乘法求解</strong>.</p>
<h3 id="第二章-感知机"><a href="#第二章-感知机" class="headerlink" title="第二章 感知机"></a>第二章 感知机</h3><p>1.模型:$f(x)=sign(w\cdot{x}+b)$,找一个可以划分正负样例的超平面,属于判别模型</p>
<p>2.学习策略:损失函数定义为误分类点到超平面的总距离</p>
<p>3.学习算法:随机梯度下降</p>
<h3 id="第三章-k近邻法"><a href="#第三章-k近邻法" class="headerlink" title="第三章 k近邻法"></a>第三章 k近邻法</h3><p>kd tree的划分方法和搜索方法参考网上资料</p>
<h3 id="第四章-朴素贝叶斯"><a href="#第四章-朴素贝叶斯" class="headerlink" title="第四章 朴素贝叶斯"></a>第四章 朴素贝叶斯</h3><p>1.基于属性独立的强假设</p>
<p>2.朴素贝叶斯 -&gt; 贝叶斯估计(防止有属性概率为0存在)</p>
<p>略</p>
<h3 id="第五章-决策树"><a href="#第五章-决策树" class="headerlink" title="第五章 决策树"></a>第五章 决策树</h3><p>1.决策树模型呈树形结构,在分类问题中,表示基于特征对实例进行分类的过程.可以认为<strong>是if-then规则的集合</strong>,也可以认为是<strong>定义在特征空间与类空间上的条件概率分布</strong>.</p>
<p>2.决策树学习过程包含三个步骤:<strong>特征选择,决策树的生成和决策树模型的修剪</strong></p>
<p>3.决策树的<strong>损失函数通常是正则化的极大似然函数</strong>,决策树学习的<strong>策略是以损失函数为目标函数的最小化</strong>,决策树的学习算法通常采用启发式方法,因为从所有可能的决策树中选取最优决策树是NP完全问题.</p>
<p>4.<strong>特征选择</strong></p>
<p>4.1 特征选择的准则通常是选择信息增益或信息增益率(基尼系数)</p>
<p>4.2 熵:$H(p)=-\sum_{i=1}^{n}p_ilogp_i$,熵越大,不确定性越大</p>
<p>4.3 条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性.$H(Y|X)=\sum_{i=1}^{n}p_iH(Y|X=x_i)$</p>
<p>4.4 <strong>信息增益</strong>:特征A对训练集D的信息增益g(D,A),定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差,即$g(D,A)=H(D)-H(D|A)$</p>
<p>4.5 <strong>信息增益比</strong>:特征A对训练集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练集D的经验熵H(D)之比为:$g_R(D,A)=\frac{g(D,A)}{H(D)}$</p>
<p>5.<strong>ID3算法/C4.5算法</strong>参考&lt;西瓜书&gt;,西瓜书上讲得略微好一点</p>
<p>6.<strong>CART算法</strong>:<strong>最小二乘法生成回归树</strong>,<strong>基于基尼系数生成回归树</strong></p>
<p>7.剪枝策略:预剪枝和后剪枝 (参考西瓜书上) 将数据集分为训练集和验证集,用验证集来进行剪枝操作.</p>
<h3 id="第六章-Logistic回归和最大熵模型"><a href="#第六章-Logistic回归和最大熵模型" class="headerlink" title="第六章 Logistic回归和最大熵模型"></a>第六章 Logistic回归和最大熵模型</h3><p>1.X服从Logistic分布是指X具有以下分布函数和密度函数:</p>
<script type="math/tex; mode=display">F(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma }}</script><script type="math/tex; mode=display">f(x)=F'(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma (1+e^{-(x-\mu)/\gamma})^2}</script><p>式中,$\mu$为位置参数,$\gamma&gt;0$为形状参数.</p>
<p>2.logistic回归策略:<strong>构造极大似然函数</strong>,使用<strong>梯度下降方法或拟牛顿法</strong>求解优化.</p>
<p>3.最大熵模型(待完善)</p>
<h3 id="第七章-SVM"><a href="#第七章-SVM" class="headerlink" title="第七章 SVM"></a>第七章 SVM</h3><p>其他略,已经复习过</p>
<p><strong>补充</strong>:SMO(序列最小最优化算法):<br><strong>1.总体思路</strong></p>
<ul>
<li>选取一对需要更新的变量$\alpha_i$,$\alpha_j$</li>
<li>固定$\alpha_i$,$\alpha_j$以外的参数,求解对偶问题</li>
</ul>
<p><strong>2.具体细节</strong></p>
<ul>
<li>First,SMO算法先选取<strong>违背KKT条件程度最大的变量</strong></li>
<li>Second,第二个变量理应选择一个使目标函数值减小最快的变量,但由于比较各变量所对应的目标函数值减幅的复杂度过高,<strong>因此SMO采用了一个启发式:使选取的两变量所对应样本之间的间隔最大.</strong></li>
</ul>
<h3 id="第八章-提升方法"><a href="#第八章-提升方法" class="headerlink" title="第八章 提升方法"></a>第八章 提升方法</h3><p>1.概念:对提升方法来说,有两个问题需要回答</p>
<ul>
<li>在每一轮如何改变训练数据的权值或概率分布 - <strong>AdaBoost提高那些前一轮弱分类器错误分类样本的权值,而降低那些被正确分类样本的权值</strong></li>
<li>如何将弱分类器组合成一个强分类器 - AdaBoost采取加权多数表决的方法,具体地,<strong>加大分类误差率较小的弱分类器的权值</strong>,使其表决中起较大的作用,<strong>减小分类误差率较大的弱分类器的权值</strong>,使其再表决中其较小的作用.</li>
</ul>
<p>2.<strong>AdaBoost</strong><br>学习样本权重$D_m$,学习分类器权重$\alpha_m$</p>
<ul>
<li>$D_m={w_{m1},w_{m2},…,w_{mN}}$,样本权重和上一次的分类器分类结果有关</li>
<li>$\alpha_m=\frac{1}{2}log\frac{1-e_m}{e_m}$,$e_m$为分类误差错误率(算错误率时乘上样本权重)</li>
</ul>
<p>3.<strong>提升树</strong><br>前向分步法+拟合残差,在拟合残差时,如果损失函数是平方差函数或指数损失函数时,每一步优化很简单.如果是一般损失函数,则可以使用梯度提升算法.</p>
<p>4.<strong>Bagging</strong>和<strong>Stacking</strong>见&lt;西瓜书&gt;</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/03/15/MachineLearningNotes/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/15/MachineLearningNotes/" itemprop="url">
                  西瓜书阅读
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-15T18:58:32+08:00">
                2018-03-15
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Learning/" itemprop="url" rel="index">
                    <span itemprop="name">Learning</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h2 id="西瓜书阅读记录-2-0"><a href="#西瓜书阅读记录-2-0" class="headerlink" title="西瓜书阅读记录(2.0)"></a>西瓜书阅读记录(2.0)</h2><p>2018年1月19日提交1.0<br>2018年3月1日重新持续更新2.0<br>2018年3月15日完成1-11章的阅读,下面开始阅读&lt;统计学习方法&gt;</p>
<p>=============================================</p>
<h3 id="第一章-绪论"><a href="#第一章-绪论" class="headerlink" title="第一章 绪论"></a>第一章 绪论</h3><p>1.归纳偏好</p>
<ul>
<li><strong>奥卡姆剃刀:</strong>若有多个假设与观察一致,则选择最简单的那个.</li>
</ul>
<p>2.NEL定理(No Free Lunch):脱离具体问题,空泛的谈论”什么学习算法更好”毫无意义.</p>
<h3 id="第二章-模型评估与选择"><a href="#第二章-模型评估与选择" class="headerlink" title="第二章.模型评估与选择"></a>第二章.模型评估与选择</h3><p>1.<strong>过拟合</strong>:当学习器把训练样本学得”太好了”的时候,很可能已经把训练样本本身的一些特点当作了所有潜在样本都会具有的一般性质.</p>
<p>2.<strong>欠拟合</strong>:学习能力低下造成的,解决办法:在决策树学习中扩展分支/在神经网络学习中增加训练轮数等.</p>
<p><strong>3.评估方法</strong>:</p>
<p>3.1 测试集应该尽可能与训练集互斥,即测试样本尽量不再训练集中出现,未在训练过程中使用过.</p>
<p>3.2 划分训练集和测试集的方法: a)<strong>留出法</strong>,直接将数据集划分为互斥的两个集合;b)<strong>交叉验证法(k-fold validation)</strong>,先将数据集D划分为k个大小相似的互斥子集,每个子集都尽可能保持数据分布的一致性.然后,每次用k-1个子集的并集作为训练集,余下的那个子集作为测试集,进行k次训练和测试,最终返回这k个测试结果的均值.(k的通常取值为10,并且通常对k-fold validation做多次,一般为10次10折交叉验证法).c)<strong>自助法(bootstrapping)</strong>,给定包含m个样本的数据集D,对它进行采样产生数据集D’:每次随即从D中挑选一个样本,将其拷贝放入D’,然后再将该样本放回初始数据集D中,使得该样本在下次采样时仍有可能被采到;这个过程重复执行m次后,我们就得到了包含m个样本的数据集D’.</p>
<p>3.3 调参参数类型:<strong>算法参数(超参)</strong>和<strong>模型参数</strong>.</p>
<ul>
<li>模型参数是学习得到的,作为模型的一部分保存</li>
<li>算法参数是算法中的参数,是模型外部的配置,如:神经网络中的学习速率,支持向量机中的C和sigma参数.</li>
</ul>
<p><strong>4.性能度量</strong>:</p>
<p>4.1 回归任务最常用的性能度量是”均方误差”: </p>
<script type="math/tex; mode=display">E(f;D)=\frac{1}{m}\sum\_{i=1}^{m}(f(x\_i)-y\_i)^2$$.

4.2  评价标准: 错误率与精度,查全率和查准率.错误率和精度指多少样本被判错,多少样本被判错;查全率和查准率指模型判断为正例中有多少比例是真正的正例,模型判断为反例中有多少为真正的反例.(两种评价标准对应的需求不一样)

|   真实情况  | 预测结果正例  |   预测结果反例  |
|   :--:  |  :--:   |   :--:    |
|   正例  |   TP(真正例) |   FN(反正例) |
|   反例  |   FP(假正例) |   TN(真正例) |
$$P(查准率) = TP / (TP  + FP)</script><script type="math/tex; mode=display">R(查全率) = TP / (TP + FN)</script><p>4.3 P-R图:以查准率为纵坐标,以查全率为横坐标.在进行比较时,若一个学习器的P-R曲线被另一个学习器的曲线完全”包住”,则可断言后者的性能优于前者. “平衡点”(BEP):当查准率 = 查全率时的取值,即为平衡点.当两个曲线有交点时,可通过比较平衡点的取值.</p>
<p>4.4 F1-measure:</p>
<script type="math/tex; mode=display">F1 = 2 * TP / (样例总数 + TP - TN)</script><p>(<strong>补充</strong>):<script type="math/tex">F_\beta = \frac{1+\beta^2\times{P}\times{R}}{\beta^2\times{P}+R}</script>,当$\beta=1$时,退化为标准的F1;$\beta&gt;1$时查全率有更大影响;$\beta$&amp;lt1时查准率有更大影响.</p>
<p>4.5 查准率和查全率的应用目的区别:例如在商品推荐系统中,为了尽可能少打扰用户,更希望推荐内容的确是用户感兴趣的,此时查准率更重要;而在逃犯信息检索系统中,更希望尽可能少漏掉逃犯,此时查全率更重要.</p>
<p>4.6 对于多分类考察查准率和查全率,基于两种方式:a)先在各个混淆矩阵上计算(P<sub>1</sub>,R<sub>1</sub>),(P<sub>2</sub>,R<sub>2</sub>),…,(P<sub>n</sub>,R<sub>n</sub>),然后再计算平均值得到”宏查准率”和”宏查全率”.b)先将各混淆矩阵上的对应元素计算平均,再基于这些平均值计算出”微查准率”和”微查全率”.</p>
<p>4.7 ROC和AUC: <strong>ROC体现了综合考虑学习器在不同任务下的”期望泛化性能”的好坏,或者说,”一般情况下”泛化性能的好坏</strong>.ROC曲线的纵轴是”真正例率(TPR)”,横轴是”假正例率(FPR)”,两者分别定义为TPR=TP / (TP + FN), FPR=FP / (TN + FP). 和P-R图相似,若一个学习器的ROC曲线被另一个学习器的曲线完全”包住”,则可断言后者性能优于前者.若两个学习器的ROC曲线发生交叉,则难以一般性地断言两者孰优孰劣,此时如果一定要进行比较,则较为合理的判据是比较ROC曲线下的面积,即AUC.</p>
<script type="math/tex; mode=display">AUC = \frac{1}{2}\sum\_{i=1}^{m-1}(x\_{i+1} - x\_i) \cdot(y\_i + y\_{i+1})$$.

**5.比较检验(待丰富)**:假设检验/交叉验证t检验/McNemar检验/Friedman检验与Nemenyi后续检验

**6.偏差与方差**:

6.1 泛化误差可分为偏差/方差与噪声之和.**偏差**度量了学习算法的期望预测与真实结果的偏离程度,即刻画了学习算法本身的拟合能力;**方差**度量了同样大小的训练集的变动所导致的学习性能的变化,即刻画了数据扰动所造成的影响;**噪声**则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界,即刻画了学习问题本身的难度.

**(2.0补充)**
7.训练误差(经验误差):学习器在训练集上的误差

8.泛化误差:学习器在新样本上的误差

***
### 第三章.线性模型
1.线性模型:给定由d个属性描述的示例X=(x<sub>1</sub>;x<sub>2</sub>;...;x<sub>d</sub>),其中x<sub>i</sub>是X在第i个属性上的取值,线性模型试图**学得一个通过属性的线性组合来进行预测的函数**,即
$$f(\textbf{x})=w\_1x\_1+w\_2x\_2+...+w\_dx\_d+b</script><p>写成向量形式:</p>
<script type="math/tex; mode=display">f(\textbf{x})=\textbf{w}^T+b</script><p>其中,<strong>w</strong>=(w<sub>1</sub>;w<sub>2</sub>;…;w<sub>d</sub>).<strong>w</strong>和b学得之后,模型就得以确定.</p>
<p><strong>2.线性回归</strong></p>
<p>2.1 概念:线性回归试图学得一个线性模型以尽可能准确地预测实值输出标记.</p>
<p>2.2 <strong>均方误差</strong>是回归任务中最常用的性能度量.基于均方误差来求解模型的方法成为<strong>最小二乘法</strong>.</p>
<p>2.3 对于多元线性回归,可以利用最小二乘法来对<strong>w</strong>和b进行估计.</p>
<p>2.4 对数线性回归: 认为示例所对应的输出标记是在指数尺度上变化.</p>
<script type="math/tex; mode=display">lny=\textbf{w}^T+b</script><p>实际上是试图让<script type="math/tex">e^{w^Tx}+b</script>逼近y.</p>
<p>2.5 广义线性模型: <script type="math/tex">y=g^{-1}(\textbf{w}^T+b)</script>(将输入空间上的真实值到输出空间上预测值的非线性函数映射)</p>
<p><strong>3.对数几率回归</strong></p>
<p>3.1 对数几率回归是一种”Sigmoid函数”.进而将回归问题转化为分类问题.</p>
<p><strong>(补充)</strong>:优化方法:极大似然估计;先构造极大似然函数,再利用梯度下降或牛顿法进行优化函数.</p>
<p><strong>4.线性判别分析(待温故)</strong></p>
<p>4.1 线性判别分析(Linear Discriminant Analysis),简称LDA,是一种经典的线性学习方法.LDA:给定训练样例集,设法将样例集投影到一条直线上,使得同类样例的投影点尽可能近/异类样例的投影点尽可能远;在对新样本进行分类时,将其投影到同样的这条直线上,再根据投影点的位置来确定新样本的类别.即,欲使同类样例的投影点尽可能接近,可以让同类样例投影点的协方差尽可能小;而欲使异类样例的投影点尽可能远离,可以让类中心之间的距离尽可能大.</p>
<p>4.2 奇异值: 特征值分解是提取矩阵特针很不错的方法,但是它只是针对方针而言的,对于非方阵矩阵,使用奇异值分解能适用于任何形式的矩阵.分解形式为:</p>
<script type="math/tex; mode=display">A\_{m\*n}=U\_{m\*m}\Sigma\_{m\*n}{V\_{n\*n}}^T(\Sigma\_{m\*n}为对角矩阵)</script><p><strong>5.多分类学习</strong></p>
<p>5.1 多分类学习的基本思路是”拆解法”,即将多分类任务拆分为若干个二分类任务求解.最经典的拆分策略有三种:”一对一(One vs. One OvO)”,”一对其余(One vs. Rest,OvR)”和”多对多(Many vs. Many,简称MvM)”.</p>
<p><strong>(补充)</strong><br>5.2 类别不平衡问题:指的是分类任务中不同类别的训练样例数目差别很大的情况.<br>基本策略:</p>
<script type="math/tex; mode=display">\frac{y^{'}}{1-y^{'}}=\frac{y}{1-y}\times\frac{m^{-}}{m^{+}}</script><p>解决方案:</p>
<ul>
<li>1.直接对训练集里的反例样例进行”欠采样”(下采样),即去除一些反例使得正/反例数目接近,然后进行学习</li>
<li>2.对训练集里的正类样例进行”过采样”(上采样),即增加一些正例使得正/反例数目接近,然后再进行学习</li>
<li>3.直接基于原始训练集进行学习,但在用训练好的分类器进行预测时,将基本策略公式嵌入到决策过程中,称为”阈值移动”</li>
</ul>
<hr>
<h3 id="第四章-决策树"><a href="#第四章-决策树" class="headerlink" title="第四章 决策树"></a>第四章 决策树</h3><p>4.1 <strong>信息熵</strong>是度量样本集合纯度最常用的一种指标.假定当前样本集合D中第k类样本所占的比例为p<sub>k</sub>(k=1,2,…,|Y|),则D的信息熵为</p>
<script type="math/tex; mode=display">Ent(D)=-\sum\_{k=1}^{|Y|}p\_klog\_2p\_k</script><p>Ent(D)的值越小,则D的纯度越高.</p>
<p>4.2 假定离散属性a有V个可能的取值{a<sup>1</sup>,a<sup>2</sup>,…,a<sup>V</sup>},若使用a对样本集D进行划分,则会产生V个分支结点,其中第v个分支结点包含了D中所有在属性a上取值为a<sup>v</sup>,记为D<sup>v</sup>.于是可以计算出用属性a对样本集D进行划分所获得的<strong>信息增益</strong></p>
<script type="math/tex; mode=display">Gain(D,a)=Ent(D)-\sum\_{v=1}^{V}\frac{|D|^v}{|D|}Ent(D^v)$$.
一般而言,信息增益越大,则意味着使用属性a来进行划分所获得的"纯度提升"越大.因此,可利用信息增益来进行决策树的划分属性选择.

4.3 **ID3**决策树学习算法就是以**信息增益**为准则来选择划分属性.(信息增益准则对可取值数目较多的属性有所偏好)

4.4 **C4.5**决策树算法不直接使用信息增益,而是使用"**增益率**"来选择最优划分属性.增益率定义为:
$$GainRatio(D,a)=\frac{Gain(D,a)}{IV(a)}$$,其中,
$$IV(a)=-\sum\_{v=1}^{V}\frac{|D|^v}{|D|}log\_2\frac{|D|^v}{|D|}$$,IV(a)称为属性a的"固有值".增益率对属性数目偏少的属性有所偏好.

(**补充**):C4.5算法并不是直接选择增益率最大的候选划分属性,而是使用了启发式算法:**先从候选划分属性中找出信息增益高于平均水平的属性,然后再从中选择增益率最高的.**

4.5 **CART决策树**使用"基尼指数"来选择划分属性.

**4.6 剪枝处理**

4.6.1 剪枝是决策树学习算法对付"过拟合"的一个重要手段.

4.6.2 剪枝策略包括:**预剪枝**和**后剪枝**.

4.6.3 **预剪枝**是在决策树生成过程中,对每个结点在划分前先进行估计,若当前结点的划分不能带来决策树泛化性能的提升,则停止划分并将当前结点标记为叶结点;**后剪枝**则是先从训练集生成一棵完整的决策树,然后自底向上地对非叶结点进行考察,若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升,则将该子树替换为叶结点.

4.6.3 后剪枝决策树通常比预剪枝决策树保留了更多的分支.一般情形下,后剪枝决策树的欠拟合风险很小,泛化性能往往优于预剪枝决策树.但其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多.

**(如何判断决策树泛化性能能否提升?)**:采用留出法,即预留一部分数据用作"验证集"以进行性能评估.

**4.7 连续与缺失值**

4.7.1 连续值处理: 二分法.(也是基于信息增益来选择划分点).二分法切分出n-1个划分点,然后从这些划分点中选择信息增益最大的划分点.

4.7.2 缺失值处理: 简单来讲,通过样本中无缺失值样本来估计同一个有属性值缺失的样本被划入不同子结点的概率.
**(补充)**:解决两个问题:
* 1)如何在属性值缺失的情况下进行划分属性选择?
* 2)给定划分属性,若样本在该属性上的值缺失,如何对样本进行划分?

对于第一个问题,还是沿用信息增益来进行划分,借助无缺失值的样本.
$$Gain(D,a)=\rho\*Gain(\tilde{D},a)=\rho\*(Ent(\tilde{D}-\sum\_{v=1}^{V}\tilde{r\_{v}}Ent(\tilde{D}^v)))</script><p>其中,<script type="math/tex">Ent(\tilde{D})=-\sum\_{k=1}^{|Y|}\tilde{p}\_{k}log\_2\tilde{p}\_k</script>.(参考西瓜书Page86)</p>
<p>对于第二个问题,若样本x在划分属性a上的取值已知,则将x划入与其取值对应的子结点,且样本权值在子结点中保持为$W_x$.若样本x在划分属性a上的取值未知,则将x同时划入所有子结点,且样本权值在与属性值$a^v$对应的子结点调整为$\tilde{r_v}\cdot{w_x}$.</p>
<p>4.8 多变量决策树: 实现斜划分甚至更复杂的决策树.<strong>在多变量决策树的学习过程中,不是为每个非叶结点寻找一个最优划分属性,而是试图建立一个合适线性分类器.</strong></p>
<hr>
<h3 id="第五章-神经网络"><a href="#第五章-神经网络" class="headerlink" title="第五章 神经网络"></a>第五章 神经网络</h3><p>1 神经网络的学习过程就是根据训练数据来调整神经元之间的<strong>连接权</strong>以及每个功能神经元的<strong>阈值</strong>.</p>
<p>2 感知机: 由两层神经元组成,输入层接受外界输入信号后传递给输出层,输出层是M-P神经元,亦称”阈值逻辑单元”. 对于非线性问题,需要考虑使用多层功能神经元.</p>
<p>3 误逆差传播算法(亦称反向传播算法,BP算法):BP算法是基于梯度下降策略,以目标的负梯度方向对参数进行调整.</p>
<p>4 累积BP算法的目标是最小化训练集D上的累积误差<script type="math/tex">E=\frac{1}{m}\sum\_{k=1}^{m}E\_k</script>.标准BP算法每次更新只针对单个样例,参数更新得非常频繁,而对不同样例进行更新的效果可能出现”抵消”现象.因此为了达到同样的累积误差极小点,标准BP算法往往需要进行更多次数的迭代.累积BP算法直接针对累积误差最小化,它在读取整个训练集D一遍后才对参数进行更新,其参数更新的频率低得多,但在很多任务中,累积误差下降到一定程度后,进一步下降会非常缓慢,这时标准BP往往会更快获得较好的解,尤其是在训练集D非常大时更明显.</p>
<p>5 BP神经网络经常遭遇过拟合,两种策略解决: <strong>a)早停</strong>,将数据分成训练集和验证集,训练集用来计算梯度/更新连接权和阈值,验证集用来估计误差,若训练集误差降低但验证集误差升高,则停止训练,同时返回具有最小验证集误差的连接权和阈值. <strong>b)正则化</strong>,其基本思想是在误差目标函数中增加一个用于描述网络负责度的部分.</p>
<p>6 神经网络采用一下策略”跳出”局部极小:</p>
<ul>
<li><strong>以多组不同参数值初始化多个神经网络</strong>,按标准方法训练后,<strong>取其中误差最小的解作为最终参数</strong>.</li>
<li>使用”<strong>模拟退火</strong>“,即以一定概率接受比当前解更差的结果.</li>
<li>使用<strong>随机梯度下降</strong>.与标准梯度下降精确计算梯度不同,随即梯度下降法在计算梯度时加入了随即因素,于是,即使陷入局部极小点,它计算出的梯度仍可能不为0,这样有机会跳出局部极小点继续搜索.</li>
</ul>
<p><strong>(补充)</strong>:梯度下降:</p>
<ul>
<li>1)批量梯度下降:每次使用全量的训练集样本来更新模型参数</li>
<li>2)随机梯度下降:每次从训练集中随机选择一个样本来进行学习</li>
<li>3)小批量梯度下降:每次更新速度与更新次数中间取得一个平衡，其每次更新从训练集中随机选择 m (m小于n) 个样本进行学习</li>
</ul>
<p>7 其他常见神经网络</p>
<ul>
<li>RBF网络(使用径向基函数作为隐层神经元激活函数,而输出层是对隐层神经元输出的线性组合.)</li>
<li>ART网络(竞争型学习是神经网络中一种常用的无监督学习策略,在使用该策略时,网络的输出神经元相互竞争,每一时刻仅有一个竞争获胜的神经元被激活,其他的神经元的状态被抑制.ART网络有比较层/识别层/识别阈值和重置模块构成.比较层负责接收输入样本,并将其传递给识别层神经元.识别层每个神经元对应一个模式类,神经元数目可在训练过程中动态增长以增加新的模式类)</li>
<li>SOM网络(一种竞争学习型的无监督神经网络,它能将高维输入数据映射到低维空间,同时保持输入数据在高维空间的拓扑结构.)</li>
<li>级联相关网络</li>
<li>Elman网络</li>
<li>Boltzmann机</li>
</ul>
<p>8 深度学习:一般地，CNN的基本结构包括两层，其一为特征提取层，每个神经元的输入与前一层的局部接受域相连，并提取该局部的特征。一旦该局部特征被提取后，它与其它特征间的位置关系也随之确定下来；其二是特征映射层，网络的每个计算层由多个特征映射组成，每个特征映射是一个平面，平面上所有神经元的权值相等。特征映射结构采用影响函数核小的sigmoid函数作为卷积网络的激活函数，使得特征映射具有位移不变性。此外，由于一个映射面上的神经元共享权值，因而减少了网络自由参数的个数。卷积神经网络中的每一个卷积层都紧跟着一个用来求局部平均与二次提取的计算层，这种特有的两次特征提取结构减小了特征分辨率。CNN主要用来识别位移、缩放及其他形式扭曲不变性的二维图形.</p>
<p>8.1 卷积: 说白了,卷积操作就是一种加权求和.在卷积层中,通常包含若干个特征平面,每个特征平面由一些矩形排列的神经元组成,同一特征平面的神经共享单元共享权值,共享的权值就是卷积核.卷积核带来的直接好处减少网络各层之间的连接,同时降低了过拟合的风险.</p>
<p>8.2 池化: 也叫子采样,降维处理,减少了模型的参数.</p>
<p><strong>(补充)</strong>:神经网络的误差反向传播算法的推导需要重新看.</p>
<hr>
<h3 id="第六章-支持向量机"><a href="#第六章-支持向量机" class="headerlink" title="第六章 支持向量机"></a>第六章 支持向量机</h3><h4 id="第六章-西瓜书"><a href="#第六章-西瓜书" class="headerlink" title="第六章 西瓜书"></a>第六章 西瓜书</h4><p>1.划分超平面:在样本空间中,划分超平面可通过如下线性方程来描述:</p>
<script type="math/tex; mode=display">w^Tx+b=0$$,
其中,$$w=(w\_1;w\_2;...;w\_d)$$为法向量;b为位移项,决定了超平面与原点之间的距离.将超平面记为(**w**,b),样本空间中任意点x到超平面(**w**,b)的距离为$$r=\frac{|w^T+b|}{||w||}$$.

2.**支持向量**:假设超平面(**w**,b)能将训练样本正确分类,即对于$$(x\_i,y\_i)\in{D}$$,若y<sub>i</sub>=+1,则有$$w^T+b>0$$;若y<sub>i</sub>=-1,则有$w^T+b<0$.令$w^T+b\geq{+1},y\_i=1$;$w^T+b\leq{+1},y\_i=-1$.距离超平面最近的这几个训练样本点使上述等号成立,它们被称为"支持向量".两个异类支持向量到超平面的距离之和为$$\gamma=\frac{2}{||w||}$$.它们被称为"间隔".

3.**支持向量机**:$min\_{w,b}\frac{1}{2}||w||^2,s.t. y\_i(w^Tx\_i+b)\geq{1},i=1,2,3,...,m.$.

**(补充:)**SMO算法:
* 选取一对需要更新的变量$\alpha\_{i}$和$\alpha\_{j}$
* 固定$\alpha\_{i}$和$\alpha\_{j}$以外的参数,求解拉格朗日函数后更新$\alpha\_{i}$和$\alpha\_{j}$
SMO算法先选取违背KKT条件程度最大的变量,第二个变量本应选择一个使目标函数值减小最快的变量,但由于比较各变量所对应的目标函数值减幅的复杂度过高,<font color='red'>**因此SMO采用了一个启发式:使选取的两变量所对应样本之间的间隔最大.**</font>

4.正定矩阵:实对称矩阵

5.二次规划问题:给定一个目标函数,找到n维的向量x,使得
$$minimize \frac{1}{2}x^TQx+c^Tx,subject to Ax\leq{b}$$.如果Q为半正定矩阵,那么该问题就是**凸二次规划问题**.凸二次规划问题,如果至少一个向量满足约束并且在可行域有下界,则凸二次规划问题就有一个全局最小值.如果Q是正定的,则这类二次规划为严格的凸二次规划问题,那么全局最小值就是唯一的.

6.对于凸二次规划问题解法:拉格朗日方法/Lemke方法,内点法,有效集法,椭球法等.

7.对偶问题:任何一个求极大化的线性规划问题都有一个求极小化的线性规划问题与之对应,反之亦然.如果我们把其中一个叫原问题,则另一个就叫做它的对偶问题,并称这一对互相联系的两个问题为一对**对偶问题**.

8.核函数:当样本在原始样本空间中线性不可分时,可以将样本映射到更高维的特征空间中,使得样本在这个特征空间内线性可分.如果原始空间是有限维,那么一定存在一个高维特征空间使样本可分.即x<sub>i</sub>与x<sub>j</sub>在特征空间的内积等于它们在原始样本空间中通过函数*k(.,.)*计算的结果,这里的*k(.,.)*就是**核函数**.有了这样的函数,就不必计算高维甚至无穷维特征空间中的内积.

9.核函数类型:
* 线性核
* 多项式核
* 高斯核(RBF核)
* 拉普拉斯核
* Sigmoid核

**(补充)**:核函数的组合形式($k\_1(x)$为核函数):
* 1.核函数的线性组合还是核函数
* 2.核函数的直积还是核函数$k\_1\times{k\_2}(x,z)=k\_1(x,z)k\_2(z)$仍是核函数
* 3.对任意函数$g(x)$,$k(x,z)=g(x)k\_1(x,z)g(z)$仍是核函数

**10.软间隔和正则化**

10.1 软间隔:在现实任务中往往很难确定合适的核函数使得训练样本在特征空间中线性可分;退一步说,即便恰好找到了某个核函数使训练集在特征空间中可分,也很难判定这个"线性可分"是不是由过拟合造成的.缓解该问题的一个方法是允许支持向量机在一些样本上出错.支持向量机形式要求所有样本均满足约束,即所有样本都必须划分正确,这称为**"硬间隔"**.而**软间隔**允许某些样本不满足约束.

**补充**:软间隔线性支持向量机优化目标为:
1.$$min\_{w,b}\frac{1}{2}{||w||}^2+C\sum\_{i=1}^{m}l\_{0/1}(y\_i(w^T+b)-1)</script><p>2.C为惩罚参数,当C无穷大时,则迫使所有样本都满足约束.当C取有限值时,则允许有一些样本不满足约束.<br>3.$l_{0/1}$为0/1损失函数.<br>4.硬间隔和软间隔区别在于:前者是$0\leq{\alpha_i}\leq{C}$,后者是$0\leq{\alpha_i}$.<br>5.支持向量机模型都由两项构成:结构风险和经验风险.结构风险用于描述模型的某些性质,经验风险用于描述模型与训练数据的契合程度.为了降低模型复杂度和防止过拟合,通过$L_p$范数来正则化结构风险.</p>
<p>11.损失函数:</p>
<ul>
<li>hinge损失</li>
<li>指数损失</li>
<li>对率损失</li>
</ul>
<p><strong>补充</strong>:SVR-支持向量回归</p>
<ul>
<li>1)容忍$f(x)$与真实输出$y$之间有$\epsilon$的误差,通过这种方式来最大限度的包容尽可能多的点在内</li>
<li>2)目标函数的优化,依然用拉格朗日乘子法.</li>
</ul>
<hr>
<h4 id="第六章-统计学习方法"><a href="#第六章-统计学习方法" class="headerlink" title="第六章 统计学习方法"></a>第六章 统计学习方法</h4><p>1.支持向量机学习方法包含构建由简至繁的模型:线性可分支持向量机,线性支持向量机及非线性支持向量机.当<strong>训练数据线性可分时</strong>,通过<strong>硬间隔最大化</strong>学习一个线性的分类器,即<strong>线性可分支持向量机</strong>,又称为硬间隔支持向量机;当<strong>训练数据近似线性可分时</strong>,通过<strong>软间隔最大化</strong>,也学习一个线性的分类器,即<strong>线性支持向量机</strong>,又称为软间隔支持向量机;当<strong>训练数据线性不可分时</strong>,通过<strong>核技巧及软间隔最大化</strong>,学习<strong>非线性支持向量机</strong>.</p>
<p><strong>2.空间概念</strong></p>
<p>2.1 线性空间(向量空间)</p>
<blockquote>
<p>线性空间又称作向量空间,对于一个线性空间,知道”基”(相当于三维空间中的坐标系)便可确定空间中元素的坐标(即位置).<strong>线性空间之定义了加法和数乘元算</strong>.</p>
</blockquote>
<p>2.2 赋范线性空间</p>
<blockquote>
<p>定义了范数的线性空间(为了了解<strong>向量的长度</strong>)</p>
</blockquote>
<p>2.3 内积空间</p>
<blockquote>
<p>定义了内积的线性空间(为了了解<strong>向量的夹角</strong>)</p>
</blockquote>
<p>2.4 欧式空间</p>
<blockquote>
<p>定义了内积的实线性空间V为实内积空间或欧几里德空间.</p>
</blockquote>
<p>2.5 Banach空间</p>
<blockquote>
<p>完备的赋范线性空间</p>
</blockquote>
<p>2.6 希尔伯特空间</p>
<blockquote>
<p>希尔伯特空间是欧几里德空间的一个推广,其不再局限于有限维的情形.与欧几里德空间相仿,希尔伯特空间也是内积空间,其上有距离和角的概念,此外,希尔伯特空间还是一个完备的空间,其上所有的柯西序列等价于收敛序列,从而微积分中的大部分概念都可以无障碍地推广到希尔伯特空间中.<br><img src="https://raw.githubusercontent.com/DuncanZhou/images/master/ML-01.jpg" alt="空间的一些数学概念"></p>
</blockquote>
<hr>
<h3 id="第七章-提升方法-boosting"><a href="#第七章-提升方法-boosting" class="headerlink" title="第七章 提升方法(boosting)"></a>第七章 提升方法(boosting)</h3><ol>
<li><p>提升方法是一种常用的统计学习方法,在分类问题中,它通过<strong>改变训练样本的权重</strong>,学习多个分类器,并将这些<strong>分类器进行线性组合</strong>,提升分类的性能.\</p>
</li>
<li><p><strong>提升树</strong>是以<strong>分类树</strong>或<strong>回归树</strong>为基本分类器的提升方法. 以决策树为基函数的提升方法称为提升树,对分类问题决策树是二叉分类树,对回归问题决策树是二叉回归树.</p>
</li>
<li><p>提升树算法采用前向分步算法.</p>
</li>
<li><p>提升树利用加法模型与前向分步算法实现学习的优化过程,当损失函数是平方损失和指数损失函数时,每一步的优化是简单的.但对一般的损失函数而言,往往每一步优化并不容易,这里可以使用梯度提升算法. <strong>其关键是利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值</strong>,拟合一个回归树.</p>
</li>
</ol>
<hr>
<h3 id="第八章-贝叶斯分类器"><a href="#第八章-贝叶斯分类器" class="headerlink" title="第八章 贝叶斯分类器"></a>第八章 贝叶斯分类器</h3><ol>
<li><p>对分类任务来说,在所有相关概率都已知的理想情形下,贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记.</p>
</li>
<li><p>贝叶斯判定准则: 为了最小化总体风险,只需在每个样本上选择那个是条件风险最小的类别标记.(条件风险=期望损失).</p>
</li>
<li><p>极大似然估算后验概率,两种策略: 1) 给定样本x,可通过直接建模P(c|x)来预测c(从为x的类别标记),这样得到的是”判别式模型”; 2) 也可以先对联合概率分布P(x,c)建模,然后由此获得P(c|x),这样得到的是”生成式模型”;</p>
</li>
</ol>
<p>4.求解贝叶斯分类器:朴素贝叶斯分类器.基于一个假设:所有属性之间相互独立</p>
<ul>
<li>对于离散性属性:$P(x_i|c)=\frac{D_{c,x_i}}{D_c}$</li>
<li>对于连续性属性:$p(x_i|c)=\frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{({x_i-\mu_{c,i}})^2}{2{\sigma_{c,i}}^2})$</li>
</ul>
<p>5.为了避免其他属性携带的信息被训练集中未出现的属性值”抹去”,在估计概率值时通常要进行”平滑”,<strong>常用”拉普拉斯修正”</strong>.令N表示训练集D中可能的类别数,$N_i$表示第i个属性可能的取值数,则修正为:</p>
<ul>
<li>$P(c)=\frac{|D_c|+1}{|D|+N}$</li>
<li>$P(x_i|c)=\frac{|D_{c,x_i}|+1}{|D_c|+N_i}$</li>
</ul>
<p>6.如果任务对预测速度要求较高,则针对训练集将朴素贝叶斯分类器涉及的所有概率估值事先计算好存储起来.如果任务数据更替频繁,则可事先不进行任何训练,待收到预测请求时再根据当前数据集进行概率估值.如果数据不断增加,则可在现有估值的基础上,仅对新增样本的属性值所涉及的概率估值进行计数修正即可实现增量学习.</p>
<blockquote>
<p>判别式模型常见的主要有：<br>Logistic Regression<br>SVM<br>Traditional Neural Networks<br>Nearest Neighbor<br>CRF<br>Linear Discriminant Analysis<br>Boosting<br>Linear Regression</p>
<p>产生式模型常见的主要有：<br>Gaussians<br>Naive Bayes<br>Mixtures of Multinomials<br>Mixtures of Gaussians<br>Mixtures of Experts<br>HMMs<br>Sigmoidal Belief Networks, Bayesian Networks<br>Markov Random Fields<br>Latent Dirichlet Allocation</p>
</blockquote>
<p>(判别式模型和生成式模型:<a href="http://www.cnblogs.com/fanyabo/p/4067295.html" target="_blank" rel="external">http://www.cnblogs.com/fanyabo/p/4067295.html</a>)</p>
<p><strong>补充:</strong>:<br>1.贝叶斯最优分类器为:$h^*=argmax_{c\in{y}}P(c|x)$.要用贝叶斯判定准则来最小化决策风险,首先要获得后验概率$P(c|x)$,而这在现实生活中是难以直接获得的,机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率.有两种策略:判别式模型和生成式模型.</p>
<p>2.判别式模型和生成式模型比较:<br>定义单个测试数据为$(c_0,x_0)$,$c_0$为测试数据的label,$x_0$为测试数据的feature</p>
<ul>
<li>判别式模型(注重条件概率):它是训练完毕后,输入测试数据,判别模型直接给出的是$P(c|x_0)$.实际上是我们看了训练过的数据之后,学习到了对数据分步的后验知识,然后根据这个认识和测试样本的feature来决策.判别模型求解的思路是：条件分布———&gt;模型参数后验概率最大———-&gt;（似然函数\cdot 参数先验）最大———-&gt;最大似然</li>
<li>生成式模型(注重联合分布概率):给定输入$x_0$,生成式模型可以给出输入和输出的联合分布$P(x_0,c_0)$.生成模型的求解思路是：联合分布———-&gt;求解类别先验概率和类别条件概率</li>
</ul>
<p>3.<strong>半朴素贝叶斯分类器</strong><br>3.1 目的:为了降低贝叶斯公式中的后验概率$P(c|x)$的困难,朴素贝叶斯分类器采用了属性条件独立的假设,但在现实任务中这个假设很难成立.<br>3.2 做法:适当考虑一部分属性间的相互依赖信息<br>3.3 策略:独依赖估计(One-Dependent Estimator)-ODE,就是<strong>假设每个属性在类别之外最多依赖于一个其他属性</strong>.$P(c|x)\propto{P(c)\prod_{i=1}^{d}P(x_i|c,pa_i))}$.相比朴素贝叶斯分类器,$x_i$多了一个依赖.$pa_i$为属性$x_i$所依赖的属性.<br>3.4 问题的关键就在于:如何确定每个属性的父属性,也就是所依赖的属性.<br>方案:</p>
<ul>
<li>1.<strong>SPODE</strong>-所有的属性都依赖于同一个属性,称为”超父”,然后通过交叉验证等模型选择方法来确定超父属性</li>
<li>2.<strong>TAN</strong>-在最大带权生成树算法的基础上,将属性间依赖关系约简到一种树形结构.<ul>
<li>1.计算任意两个结点的互信息$I(x_i,x_j|y)=\sum_{x_i,x_j;c\in{y}}P(x_i,x_j|c)log\frac{P(x_i,x_j|c)}{P(x_i|c)P(x_j|c)}$</li>
<li>2.以属性为结点构建完全图,任意两个结点之间边的<strong>权重设为$I(x_i,x_j|y)$</strong></li>
<li>3.构建此完全图的<strong>最大带权生成树</strong>,挑选根变量,将边置为有向</li>
<li>4.加入类别结点y,增加从y到每个属性的有向边</li>
</ul>
</li>
<li>3.<strong>AODE</strong>-一种基于集成学习机制,更为强大的独依赖分类器.AODE尝试将每个属性作为超父来构建SPODE,然后将那些具有足够训练数据支撑的<strong>SPODE集成</strong>起来作为最终结果.即$P(c|x)\propto{\sum_{i=1,|D_{x_i}|\geq{m^{‘}}}^{d}P(c,x_i)\prod_{j=1}^{d}P(x_j|c,x_i))}$</li>
</ul>
<p>4.<strong>贝叶斯网</strong><br>4.1 概念:借助有向无环图来刻画属性之间的依赖关系,并使用条件概率表来描述属性的联合概率分布.一个贝叶斯网B由结构G和参数$\theta$两部分构成,$\theta$定量描述变量的依赖关系.<br>4.2 结构:给定父结点集,贝叶斯网假设每个属性与它的非后裔属性独立,于是$B=&lt;G,\theta&gt;$将属性$x_1,x_2,…,x_d$的联合概率分布定义为$P_B(x_1,x_2,…,x_d)=\prod_{i=1}^{d}P_B(x_i|\pi_i)=\prod_{i=1}^{d}\theta_{x_i|\pi_i}$</p>
<hr>
<h3 id="第九章-集成学习-提升方法"><a href="#第九章-集成学习-提升方法" class="headerlink" title="第九章 集成学习(提升方法)"></a>第九章 集成学习(提升方法)</h3><p><strong>1.概念介绍</strong></p>
<ol>
<li>1 集成学习方法大致分为两类: 1) 个体学习器之间存在强依赖关系,必须串行化生成的序列化方法; 2) 个体学习器间不存在强依赖关系,可同时生成的并行化方法. <strong>1)的代表是Boosting</strong>;<strong>2)的代表是Bagging和”随机森林”</strong>;</li>
</ol>
<p>1.2 Bagging是并行集成学习方法最著名的代表,训练基于<strong>自助采样法</strong>.</p>
<p>1.3 Bagging通常对分类任务使用简单投票法,对回归任务使用简单平均法.</p>
<p>1.4 随机森林(Random Forest)是Bagging的一个扩展变体,RF在以决策树为基学习器构建Bagging集成的基础上,进一步在决策树的训练过程中引入了随机属性选择.</p>
<p><strong>补充:</strong><br>1.5 集成中只包含同种类型的个体学习器称为”同质的”.同质集成中的学习器亦称”基学习器”,相应的学习算法称为”基学习算法”.集成也可包含不同类型的个体学习器,这样的集成是”异质的”.相应的个体学习器一般不称为基学习器,常成为组件学习器.</p>
<p>1.6 <strong>Important:</strong>要获得好的集成,个体学习器应<strong>“好而不同”</strong>,<strong>即个体学习器要有一定的”准确性”</strong>,<strong>即学习器不能太坏,并且要有”多样性”,即学习器间具有差异</strong>.</p>
<p>1.7 <strong>Boosting</strong>:</p>
<p>1.7.1 概念:Boosting是一族可将弱学习器提升为强学习器的算法.</p>
<p>1.7.1 工作机制:先从初始训练集训练出一个基学习器,再根据基学习器的表现对训练样本分布进行调整,使得先前基学习器做错的训练样本在后续受到更多关注,然后基于调整后的样本分布来训练下一个基学习器;如此重复,直至基学习器数目达到事先指定的值T,最终将这T个基学习器进行加权结合.</p>
<p>1.7.2 代表算法AdaBoost</p>
<ul>
<li>推导:基于”加性模型”,即学习器的线性组合,$H(x)=\sum_{t=1}^{T}\alpha_th_t(x)$.训练T个基分类器,对上一轮分类错误的样本分配更多的权重.</li>
</ul>
<p>1.8 <strong>Bagging和随机森林</strong></p>
<p>1.8.1 概念:Bagging是并行式集成学习方法最著名的代表.直接<strong>基于自主采样法(bootstrap sampling),有放回的采样</strong>.</p>
<p>1.8.2 操作:Bagging对分类任务使用简单投票法,对回归任务使用简单平均法.</p>
<p>1.8.3 随机森林:是Bagging的一个扩展变体.<strong>RF在以决策树构建Bagging集成的基础上,进一步再决策树的训练过程中引入了随机属性选择.</strong>具体来说,传统决策树在选择划分属性时是在当前结点的属性集合中选择一个属性;而在RF中,对基决策树的每个结点,先从该结点的属性集合中<strong>随机选择一个包含k个属性的子集</strong>,然后再从这个子集中<strong>选择一个最优属性用于划分</strong>.</p>
<p><strong>2.组合策略</strong></p>
<p>2.1 平均法<br>包括简单平均法和加权平均法.加权平均法的权重一般是从训练数据中学习而得,但是加权平均法未必一定优于简单平均法.<strong>一般而言,在个体学习器性能相差较大时宜使用加权平均法,而在个体学习器性能相近是宜使用简单平均法.</strong></p>
<p>2.2 投票法<br>包括绝对多数投票法,相对多数投票法及加权投票法.</p>
<p>2.3 学习法<br>当训练数据很多时,一种更为强大的结合策略是使用”学习法”,即通过另一个学习器来进行结合.<strong>Stacking是学习法的典型代表</strong>.Stacking先从初始数据集训练出初级学习器,然后”生成”一个新数据集用于训练次级学习器.在这个新数据集中,<strong>初级学习器的输出被当做样例输入特征</strong>,而初始样本的标记仍被当作样例标记.</p>
<hr>
<h3 id="第十章-聚类"><a href="#第十章-聚类" class="headerlink" title="第十章 聚类"></a>第十章 聚类</h3><p><strong>1.性能度量</strong></p>
<p>1.1 聚类性能的度量有两类: 一类是将聚类结果与某个”参考模型”进行比较,称为”外部指标”.另一类是直接考察聚类结果而不利用任何参考模型,称为”内部指标”.</p>
<p><strong>1.1 外部指标</strong></p>
<p>1.2 a = |SS|,b=|SD|,c=|DS|,d=|DD|(关于SS,SD,DS和DD的解释参考书Page198),常用的三种性能度量:</p>
<ul>
<li>Jaccard系数: <script type="math/tex">JC=\frac{a}{a+b+c}</script></li>
<li>FM指数: <script type="math/tex">FMI=\sqrt{\frac{a}{a+b}\frac{a}{a+c}}</script></li>
<li>$RI=\frac{2(a+d)}{m(m-1)}$<br>上述性能度量的结果均在[0,1]区间,值越大越好.</li>
</ul>
<p><strong>1.2内部指标</strong><br><strong>补充:</strong>通过考虑聚类结果的簇之间的距离<br><strong>DBI指数和DI指数</strong>(DBI值越小越好,而DI值越大越好.)</p>
<p><strong>2.聚类算法</strong><br>2.1 <strong>原型聚类</strong>:k-means聚类,学习向量量化(LVQ)-有标记聚类,高斯混合聚类<br>2.2 <strong>密度聚类</strong>:DBSACN:1.找到所有的核心对象;2.从核心对象出发将密度可达点加入生成聚类簇<br>2.3 <strong>层次聚类</strong>:Hierarchical clustering:先将数据集中的每个样本看作一个初始聚类簇,然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并,不断重复,直到达到预设的聚类簇个数.</p>
<p><strong>补充:3距离计算</strong><br>3.1 距离度量函数满足以下性质:</p>
<ul>
<li>非负性</li>
<li>同一性</li>
<li>对称性</li>
<li>直递性</li>
</ul>
<p>3.2 常用的距离度量函数</p>
<ul>
<li>Minkowski distance(闵可夫斯基距离)$dist_{mk}(x_i,x_j)=(\sum_{n}^{u=1}|x_{iu}-x_{ju}|^p)^\frac{1}{p}$</li>
<li>Euclidean distance(欧式距离) 当闵可夫斯基距离中的p=2时,即为欧式距离</li>
<li>Manhattan distance(曼哈顿距离) 当闵可夫斯基距离中的p=1时,即为曼哈顿距离</li>
</ul>
<p>3.3 无序属性的处理</p>
<ul>
<li><p>对<strong>无序属性可采用VDM</strong>(Value Difference Metric).令$m_{u,a}$表示在属性u上取值为a的样本数,$m_{u,a,i}$表示在第i个样本簇中的属性u上取值为a的样本数,k为样本簇数,则属性u上两个离散值a和b之间的VDM距离为$VDM_p(a,b)=\sum_{i=1}^{k}|\frac{m_{u,a,i}}{m_{u,a}}-\frac{m_{u,b,i}}{m_{u,b}}|^p$.</p>
</li>
<li><p>将闵可夫斯基距离和VDM结合即可处理混合属性.假定有$n_c$个有序属性,$n-n_c$个无序属性,则$MinkovDM_p(x_i,x_j)=(\sum_{u=1}^{n_c}|x_{iu}-x_{ju}|^p+\sum_{u=n_c+1}^{n}VDM_p(x_{iu},x_{ju}))^\frac{1}{p}$</p>
</li>
</ul>
<hr>
<h3 id="第十一章-降维与度量学习"><a href="#第十一章-降维与度量学习" class="headerlink" title="第十一章 降维与度量学习"></a>第十一章 降维与度量学习</h3><p><strong>1.降维(维数约简)</strong><br>1.1 为什么要降维?因为在高维情形下出现的<strong>数据样本稀疏,距离计算困难</strong>等问题,是所有机器学习方法共同面临的严重障碍.</p>
<p>1.2 为什么能进行降维?因为在很多时候,人们观测或收集到的数据样本虽然是高维的,但与学习任务密切相关的也许仅仅是某个低维分布,即高维空间中的一个低维”嵌入”.</p>
<p>1.3 降维方法:</p>
<ul>
<li><strong>多维缩放MDS</strong>(最优化问题解法:计算内积矩阵)<br><strong>补充:</strong><ul>
<li>原样本为$R^{m\times{m}}$,降维后为$R^{d’\times{m}}$,使得任意两个样本在$d’$维空间中的欧式距离等于原始空间中的距离,即$||z_i-z_j||=dist_{ij}$.</li>
<li>令$B=Z^TZ$,B为降维后的内积矩阵,$b_{ij}=z_i^Tz_j$,$dist_{ij}^2=||z_i||^2+||z_j||^2-2z_i^Tz_j=b_{ij}+b_{jj}-2b_{ij}$,对Z进行中心化,然后推导求出B;求出B后利用特征值分解,求得Z矩阵</li>
</ul>
</li>
<li><strong>主成分分析PCA</strong>(Principal Component Analysis)(最优化问题解法:计算协方差矩阵),用一个超平面对所有样本进行恰当表达.<ul>
<li><strong>最近重构性</strong>:样本点到这个超平面的距离都足够近</li>
<li><strong>最大可分性</strong>:样本点在这个超平面上的投影点能尽可能分开</li>
<li><strong>思路</strong>:将所有的样本投影到超平面上,然后求投影变换后的新坐标系,正交基向量</li>
</ul>
</li>
<li><strong>核化线性降维(KPCA)</strong></li>
<li><strong>流形学习(Manifold Learning)</strong><ul>
<li>等度量映射(Isometric Mapping)(将多维空间中的测地线距离作为MDS算法的原始空间距离矩阵的输入,其中任意两点之间的最短路径可以用Dijkstra或者Floyd算法求)</li>
<li>局部线性嵌入(Locally Linear Embeeding)</li>
</ul>
</li>
<li><strong>度量学习(Metric Learning)</strong>(通过学习的方式,学到一种转换维度的距离度量的方式)</li>
</ul>
<hr>
<h3 id="第十二章-特征选择与稀疏学习"><a href="#第十二章-特征选择与稀疏学习" class="headerlink" title="第十二章 特征选择与稀疏学习"></a>第十二章 特征选择与稀疏学习</h3><p><strong>1.概念和意义</strong><br>1.1 特征选择：从给定的特征集合中选择出相关特征子集的过程，成为”特征选择”;</p>
<p>1.2 特征选择的原因:</p>
<ul>
<li>在现实任务中经常会遇到<strong>维数灾难</strong>的问题,这是由于属性过多造成的，如果能从中选择出重要的特征，使得后续的学习过程仅需在一部分特征上构建模型,则维数灾难问题会大为减轻。</li>
<li><strong>去除不相关特征</strong>往往会降低学习任务的难度。</li>
</ul>
<p><strong>2.如何特征选择</strong><br>分为两步:</p>
<ul>
<li>“子集搜索”:前向搜索，每次向特征集合中添加，直到结果不再优为止;或者后向搜索，从完整的特征候选集合中减少特征（类似于贪心算法）。</li>
<li>“子集评价”:基于<strong>信息增益</strong>计算属性特征的贡献。对于属性子集A,假定根据其取值将D分成了V个子集{$D^1$,$D^2$,…,$D^V$},每个子集中的样本在A上取值相同,于是我们可计算属性子集A的信息增益.信息增益越大,意味着特征子集A包含的有助于分类的信息越多.基于每个属性子集的信息增益作为评价准则.</li>
</ul>
<p><strong>3.特征选择的方法</strong><br><strong>3.1 过滤式</strong><br>过滤式选择不考虑后续学习器。</p>
<blockquote>
<p>Relief是一种著名的过滤式特征选择方法，该方法设计了一个”相关统计量”来度量特征的重要性。（是为二分类问题设计的。扩展变体Relief-F能处理多分类的问题。）可以设定相关统计量的<strong>阈值</strong>或者设定选择<strong>特征的个数K</strong>.</p>
</blockquote>
<p><strong>3.2 包裹式</strong><br>与过滤式选择不考虑后续学习器不同，包裹式选择<strong>直接把最终将要使用的学习器的性能作为特征子集的评价准则</strong>。In other words，包裹式选择的目的就是为给定学习器选择最有利于其性能的特征子集。包裹式选择方法直接针对给定学习器进行优化。</p>
<blockquote>
<p>LWW(Las Vegas Wrapper)是一个典型的包裹式特征选择方法，它在拉斯维加斯方法框架下使用<strong>随机策略</strong>来进行子集搜索，并<strong>以最终分类器的误差为特征子集评价准则</strong>。-交叉验证</p>
</blockquote>
<p><strong>3.3 嵌入式</strong><br>嵌入式选择是将特征选择过程与学习器训练过程融为一体，两者在同一个优化过程中完成，即在学习器训练过程中自动地进行了特征选择。<br>具体做法：将过拟合中的正则项中的L2范数替换为L1范数，L1范数和L2范数都有助于降低过拟合的风险，但L1范数还会带来一个额外的好处，它比后者更易于获得”稀疏”解。</p>
<p><strong>4.稀疏表示与字典学习</strong><br>4.1 将样本转化为合适的稀疏表示形式，从而使学习任务得以简化，模型复杂度得以降低，通常称为”字典学习”，亦称”稀疏编码”。</p>
<p><strong>5.压缩感知</strong><br>压缩感知关注的是如何利用信号本身所具有的稀疏性，从部分观测样本中恢复原信号。通常认为，压缩感知分为”感知测量”和”重构恢复”这两个阶段。”感知测量”关注如何对原始信号进行处理以获得稀疏样本表示;”重构恢复”关注的使如何基于洗属性从少量观测中恢复原信号，这是压缩感知的精髓。</p>
<hr>
<h3 id="第十三章-半监督学习"><a href="#第十三章-半监督学习" class="headerlink" title="第十三章 半监督学习"></a>第十三章 半监督学习</h3><p><strong>1.概念</strong><br>在只有少量的标注样本,而有大量的未标注样本,让学习器不依赖外界交互,自动地利用未标记样本来提升学习性能,就是半监督学习.</p>
<p><strong>2.方法</strong><br><strong>2.1 假设</strong></p>
<ul>
<li>聚类假设:假设数据存在簇结构,同一个簇的样本属于同一个类别.</li>
<li>流形假设:假设数据分布在同一个流形结构上,邻近的样本拥有相似的输出值.”邻近”程度常用”相似”程度来刻画,因此,流形假设可看作聚类假设的推广,但流形假设对输出值没有限制,因此比聚类假设的使用范围更广.<br>其实,这两个假设本质都是<strong>“相似的样本拥有相似的输出”</strong>.</li>
</ul>
<p><strong>2.2 分类</strong><br>半监督学习可分为纯半监督学习和直推学习.</p>
<ul>
<li>纯半监督学习:假定训练数据中的未标记样本并非待预测的数据.</li>
<li>直推学习:假定学习过程中所考虑的未标记样本恰是待预测数据.</li>
</ul>
<p><strong>2.3 具体方法</strong></p>
<ul>
<li>生成式方法</li>
<li>半监督SVM</li>
<li>图半监督学习</li>
<li>基于分歧的方法(与上述三个不同的是,基于分歧的方法使用多学习器,而学习器之间的”分歧”对未标记数据的利用至关重要.)</li>
<li>半监督聚类</li>
</ul>
<hr>
<h3 id="第十四章-概率图模型"><a href="#第十四章-概率图模型" class="headerlink" title="第十四章 概率图模型"></a>第十四章 概率图模型</h3><p><strong>1.隐马尔可夫模型</strong><br>1.假定所关心的变量集合为Y,可观测变量集合为O,其他变量的集合为R,”生成式”模型考虑联合分布P(Y,R,O),”判别式”模型考虑条件分布P(Y,R|O).给定一组观测变量值,推断就是要由P(Y,R,O)或P(Y,R|O)得到条件概率分布P(Y|O).</p>
<p>2.概率图模型是一类用图来表达变量相关关系的概率模型.它以图为表示工具,最常见的是用一个结点表示一个或一组随即变量,结点之间的边表示变量间的概率相关关系,即”变量关系图”.</p>
<p>3.概率图模型大致分为两类:</p>
<ul>
<li>使用有向无环图表示变量之间的依赖关系,称为<strong>有向图模型或贝叶斯网</strong>.</li>
<li>使用无向图表示变量间的相关关系,称为<strong>无向图模型或马尔可夫网</strong>.</li>
</ul>
<p>4.隐马尔可夫模型是结构最简单的动态贝叶斯网.(主要用于时序数据建模,在语音识别/自然语言处理等领域有广泛应用.)</p>
<p>5.确定一个隐马尔可夫模型需要以下三组参数:</p>
<ul>
<li>状态转移概率(状态转移矩阵)</li>
<li>输出观测概率(输出观测矩阵)</li>
<li>初始状态概率</li>
</ul>
<p><strong>2.马尔可夫随机场(MRF)</strong><br>2.1 全局马尔可夫性:给定两个变量子集的分离集,则这两个变量子集条件独立.</p>
<p>2.2 由全局马尔可夫性得到两个有用的推论:</p>
<ul>
<li><strong>局部马尔可夫性</strong>:给定某变量的邻接变量,则该变量条件独立于其他变量.</li>
<li><strong>成对马尔可夫性</strong>:给定所有其他变量,两个非邻接变量条件独立.</li>
</ul>
<p>2.3 指数函数常被用于定义势函数.</p>
<p><strong>3.条件随机场</strong><br>3.1 条件随机场是一种判别式无向图模型.</p>
<p>3.2 生成式模型是直接对联合分布进行建模,而判别式模型则是对条件分布进行建模.(隐马尔可夫模型和马尔可夫随机场都是生成式模型,条件随机场是判别式模型.)</p>
<hr>
<h3 id="第十五章-规则学习"><a href="#第十五章-规则学习" class="headerlink" title="第十五章 规则学习"></a>第十五章 规则学习</h3><p><strong>1.基本概念</strong><br>1.1 规则分为两类: <strong>“命题规则”</strong>和<strong>“一阶规则”</strong>,前者由是”原子命题”和逻辑连接词”与,或,非”和”蕴含”构成的简单陈述句.后者的基本成分是能描述事物的属性或关系的”原子公式”.</p>
<p><strong>2.方法</strong><br>2.1 序贯覆盖</p>
<p>2.2 剪枝优化(预剪枝和后剪枝)</p>
<p>2.3 一阶规则学习<br>受限于命题逻辑表达能力,命题规则学习难以处理对象之间的”关系”,而关系信息在很多任务中非常重要.例如,我们在现实世界挑选西瓜时,通常很难把水果摊上所有西瓜的特征用属性值描述出来,因为我们很难判断:色泽看起来多深才叫”色泽青绿”?敲起来声音多低才叫”敲声沉闷”?比较现实的做法是将西瓜进行相互比较,例如,”瓜1的颜色比瓜2更深,并且瓜1的根蒂比瓜2更蜷”,因此”瓜1比瓜2更好”.</p>
<hr>
<h3 id="第十六章-强化学习"><a href="#第十六章-强化学习" class="headerlink" title="第十六章 强化学习"></a>第十六章 强化学习</h3><p><strong>1.基本概念</strong><br><strong>1)</strong>强化学习任务通常用<strong>马尔可夫决策过程(MDP-Markov Decision Process)</strong>来描述:机器处于<strong>环境E</strong>中,<strong>状态空间为X</strong>,其中每个状态x是机器感知到的环境的描述.机器能采取的动作构成了<strong>动作空间A</strong>.若某个动作a作用在当前状态x上,则潜在的<strong>转移函数P</strong>将使得环境从当前状态按某种概率转移到另一个状态.在转移到另一个状态的同时,环境会根据潜在的<strong>“奖赏”函数R</strong>反馈给机器一个奖赏.</p>
<p><strong>2)</strong>强化学习任务对应了四元组<strong>E=<x,a,p,r></x,a,p,r></strong>,其中P:X*A*X-&gt;R指定了状态转移概率,R:X*A*X-&gt;R指定了奖赏;在有的应用中,奖赏函数可能仅与状态转移有关,即R:X*X-&gt;R;</p>
<p><strong>3)</strong>机器要做的是通过在环境中不断尝试而学得一个<strong>“策略”Pi</strong>,根据这个策略,在状态x下,就能得知要执行的动作<strong>a=Pi(x)</strong>.</p>
<p>策略有两种方法:</p>
<ul>
<li>一种是将策略表示为函数<strong>Pi:X-&gt;A,确定性策略</strong>常用这种表示.</li>
<li>另一种是概率表示<strong>Pi:X*A-&gt;R,随机性策略</strong>常用这种表示,Pi(x,a)为状态x下选择动作a的概率,动作概率之和为1.</li>
</ul>
<p><strong>总结</strong>:在强化学习任务中,学习的目的就是要找到能使长期累积奖赏最大化的策略.</p>
<ul>
<li>T步累积奖赏</li>
<li>r折扣累积奖赏</li>
</ul>
<p><strong>4)</strong>强化学习和监督学习的差别和联系</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">强化学习</th>
<th style="text-align:center">监督学习</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">状态(x)</td>
<td style="text-align:center">示例(x)</td>
</tr>
<tr>
<td style="text-align:center">动作(a)</td>
<td style="text-align:center">标记(y)</td>
</tr>
<tr>
<td style="text-align:center">策略(Pi)</td>
<td style="text-align:center">分类器或回归器</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>与一般监督学习不同,强化学习任务的最终奖赏是在多步动作之后才能观察到的.</p>
</blockquote>
<p><strong>2.应用</strong><br>2.1 K-摇臂赌博机</p>
<ul>
<li>若仅为获知<strong>每个摇臂的期望奖赏</strong>,则<strong>可采用”仅探索”法</strong>,将所有<strong>尝试机会平均分配</strong>给每个摇臂,最后以每个摇臂各自的平均吐币概率作为其奖赏期望的近似估计.</li>
<li>若仅为执行<strong>奖赏最大的动作</strong>下,则<strong>可采用”仅利用”法</strong>,,按下目前最优的(即到目前为止平均奖赏最大的)摇臂.</li>
</ul>
<p><strong>总结</strong>:”探索”和”利用”两者是矛盾的,因为尝试次数有限,加强了一方则会自然削弱另一方.这就是强化学习所面临的”探索-利用窘境”.显然,欲累积奖赏最大,则必须在探索和利用之间达成较好的折中.</p>
<p>策略:</p>
<ul>
<li>epsilon-贪心:基于一个概率来对探索和利用进行折中,每次尝试时,以epsilon的概率进行探索,以均匀概率选取一个摇臂,以1-epsilon的概率进行利用,即选择当前平均奖赏最高的摇臂.</li>
<li>Softmax:基于当前已知的摇臂平均奖赏来对探索和利用进行折中.若各摇臂的平均奖赏相当,则选取各摇臂的概率也相当;若某些摇臂的平均奖赏明显高于其他摇臂,则它们被选取的概率也明显更高.</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://github.com/DuncanZhou/2018/03/13/Interview/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="duncan">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="Duncan's Blog">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="Duncan's Blog" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2018/03/13/Interview/" itemprop="url">
                  面试细节
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Veröffentlicht am</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-13T00:00:00+08:00">
                2018-03-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">in</span>
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Note/" itemprop="url" rel="index">
                    <span itemprop="name">Note</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          

          
          

          
          
          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>写于2018年3月,刚好在找实习,搜集了一些&lt;剑指offer&gt;上和其他关于面试的建议,记录下来.</p>
<hr>
<h2 id="1-着装及外貌"><a href="#1-着装及外貌" class="headerlink" title="1.着装及外貌"></a>1.着装及外貌</h2><ul>
<li>衣服不用过于正式,整洁干净就可以了.</li>
<li>保持思维敏捷,容光焕发</li>
</ul>
<h2 id="2-自我介绍"><a href="#2-自我介绍" class="headerlink" title="2.自我介绍"></a>2.自我介绍</h2><ul>
<li><strong>时间</strong>: 30s - 1min (面试官手中已有你的简历,因此自我介绍不用过于详细)</li>
<li><strong>内容</strong>: 主要学习,工作经历(没有工作经历就简短说一下做了什么项目)</li>
</ul>
<h2 id="3-项目介绍"><a href="#3-项目介绍" class="headerlink" title="3.项目介绍"></a>3.项目介绍</h2><p>建议使用<strong>STAR</strong>模型描述自己经历过的每一个项目</p>
<ul>
<li><strong>Situation:简短的项目背景</strong>,比如项目的规模,开发的软件的功能,目标用户等.</li>
<li><strong>Task:自己完成的任务</strong>,在用词上注意区分”参与”和”负责”</li>
<li><strong>Action:为了完成任务自己做了哪些工作,怎么做的</strong>.详细介绍</li>
<li><strong>Result:自己的贡献.</strong>如果是参与功能开发,可以说按时完成了多少功能;如果做优化,可以说性能提高的百分比是多少;如果是维护,可以说修改了多少个bug.</li>
</ul>
<p>面试官可能会问的问题:</p>
<ul>
<li>你在该项目中碰到的最大的问题是什么?怎么解决的?</li>
<li>从这个项目中你学到了什么?</li>
<li>什么时候会和其他团队成员有什么样的冲突?怎么解决冲突的?</li>
</ul>
<blockquote>
<p>note:<font color="red">介绍项目时,少讲背景,突出自己的贡献.</font></p>
</blockquote>
<h2 id="4-掌握的技能"><a href="#4-掌握的技能" class="headerlink" title="4.掌握的技能"></a>4.掌握的技能</h2><ul>
<li><strong>了解:</strong>指对某一个技术只是上过课或看过书,但没有做过实际的项目.</li>
<li><strong>熟悉:</strong>如果我们在实际项目中使用某一项技术已经有较长的时间,通过查阅相关的文档可以独立解决大部分问题,我们就熟悉它了.(在简历中我们描述技能的掌握程度大部分应该是”熟悉”).</li>
<li><strong>精通:</strong>如果我们对一项技术使用得得心应手,在实际开发过程中我们都有信心也有能力解决,可以说精通这个技术.</li>
</ul>
<h2 id="5-面试官面试考察interviewee的几个方面"><a href="#5-面试官面试考察interviewee的几个方面" class="headerlink" title="5.面试官面试考察interviewee的几个方面"></a>5.面试官面试考察interviewee的几个方面</h2><ul>
<li><strong>1.扎实的基础知识</strong>:编程语言,数据结构,算法等-<ul>
<li>语言:至少掌握1-2门编程语言</li>
<li>数据结构:熟练掌握<strong>链表,树,栈,队列和哈希表等</strong>数据结构和它们的操作</li>
<li>算法:查找,排序,贪心,动规,dfs等</li>
</ul>
</li>
<li><strong>2.能写高质量的代码</strong>:能写出正确,完整的,鲁棒的高质量代码;面试官会格外关注<strong>边界条件,特殊输入</strong>等看似细枝末节但实质至关重要的地方.</li>
<li><strong>3.分析问题思路清晰</strong>:思路清晰,解决复杂问题</li>
<li><strong>4.能优化时间效率和空间效率</strong>:能从时间,空间复杂度两方面优化算法效率</li>
<li><strong>5.学习和沟通能力</strong>:具备优秀的沟通能力,学习能力,发散思维能力等<ul>
<li>团队合作能力</li>
<li>沟通能力</li>
<li>举一反三能力</li>
</ul>
</li>
</ul>
<h2 id="6-interviewee提问环节"><a href="#6-interviewee提问环节" class="headerlink" title="6.interviewee提问环节"></a>6.interviewee提问环节</h2><font color="red">Don't talk about pay!</font>



          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/3/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="extend next" rel="next" href="/page/5/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://raw.githubusercontent.com/DuncanZhou/images/master/webwxgetmsgimg.jpg"
               alt="duncan" />
          <p class="site-author-name" itemprop="name">duncan</p>
          <p class="site-description motion-element" itemprop="description">write something useful</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/">
              <span class="site-state-item-count">63</span>
              <span class="site-state-item-name">Artikel</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">11</span>
                <span class="site-state-item-name">Kategorien</span>
              
            </div>
          

          
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">13</span>
                <span class="site-state-item-name">Tags</span>
              
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/DuncanZhou" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://instagram.com/callmeduncanzhou" target="_blank" title="instagram">
                  
                    <i class="fa fa-fw fa-instagram"></i>
                  
                  instagram
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://music.163.com/#/user/home?id=317536872" target="_blank" title="music">
                  
                    <i class="fa fa-fw fa-music"></i>
                  
                  music
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.zhihu.com/people/duncanzhou-22/activities" target="_blank" title="zhihu">
                  
                    <i class="fa fa-fw fa-gift"></i>
                  
                  zhihu
                </a>
              </span>
            
          
        </div>

        
        

        
        
        <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=371362&auto=1&height=66"></iframe>

        


      </section>

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">duncan</span>
</div>


<div class="powered-by">
  Erstellt mit  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>
<span id="busuanzi_container_site_pv">
   | Total visited <span id="busuanzi_value_site_pv"></span> times
</span>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	




  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

  

  
</body>
</html>
